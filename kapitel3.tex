\chapter{Related Work}
\label{chap:relatedWork}
This chapter will give a brief outline and evaluation of related work on the topic of performance evaluation of deep learning inference. These related papers deal with the performance evaluation of either edge or cloud inference or both of deployment options in a comparative fashion.

%%Change title to more fitting one
%\section{Performance of Edge vs Cloud Inference}
%\subsection{Speed/accuracy trade-offs for modern convolutional object detectors}
%In \cite{DBLP:journals/corr/HuangRSZKFFWSG016} the authors provide a guide for selecting the right convolutional object detection architecture for the desirable speed/memory/accuracy. To demonstrate this trade-offs the authors test various popular models such as Inception v2, MobileNet or Resnet 101. To create the benchmarks, "a machine with 32GB RAM,
%Intel Xeon E5-1650 v2 processor and an Nvidia GeForce
%GTX Titan X GPU card \cite{DBLP:journals/corr/HuangRSZKFFWSG016} is used.


\section{Cloud-based or On-device:
An Empirical Study of Mobile Deep Inference}
Guo "evaluate[s] the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application" \cite{DBLP:conf/ic2e/Guo18} for both cloud and edge inference. The hardware and frameworks used to conduct the experiments leading to this evaluation can be seen in table \ref{frameworks_hardware_1}. The three tested CNNs are image classification models (AlexNet, NIN and SqueezeNet) with an image input size of $224\times224$.
They divided the experiment into four steps and measured the executing time of each step: 
The model loading time (1.\,Load model), the time needed to resize the image to a $224\times224$ bitmap (2.\,Rescale bitmap), the time needed to upload the bitmap to the server in case of cloud inference (3.\,Upload bitmap) and the inference time (4.\,Compute probability).
Additional to the latencies they measured the CPU usage, battery and memory consumption during these 4 steps using Trepn Profiler.
They used 15 images in total, divided into three different image size groups ($540\times960$, $108\times1920$, $2160\times3840$) with the same 5 scaled images in each group.

After analyzing their measurements, they present the following results:
Cloud inference "exhibits substantial benefits
in terms of inference response time and mobile energy savings
over on-device inference, in this case by two orders of magnitude" \cite{DBLP:conf/ic2e/Guo18}. As the responsible bottleneck for this conclusion, they identify the model loading and inference times during edge inference.
Also, it is more efficient to upload the $540\times960$ images directly to the cloud without resizing them on the edge beforehand.


\begin{table}[H]
\centering
\caption{Hardware and inference frameworks used by \cite{DBLP:conf/ic2e/Guo18}}
\begin{tabular}{@{}lll@{}}
\toprule
 & Frameworks & Hardware \\
 \midrule
Edge & Caffe Android Lib; CNNDroid & Nexus 5 \\
Cloud & Caffe & \begin{tabular}[c]{@{}l@{}}Intel Xeon E5-2670, Nvidia Grid K520,\\ 15GB Memory\end{tabular}\\
\bottomrule

\end{tabular}

\label{frameworks_hardware_1}
\end{table}

Finally, the authors conclude that edge inference is inferior to cloud inference by factor two for both inference time and energy consumption. Thus cloud inference delivers better performance for most cases. However due to improvements in models, hardware and frameworks, the authors "believe it is
very likely that on-device[edge] inference can be done efficiently in the near future" \cite{DBLP:conf/ic2e/Guo18}. 
\section{Latency and Throughput Characterization of Convolutional
Neural Networks for Mobile Computer Vision}
Similar to Guo, Hanhirova et al. \cite{DBLP:conf/mmsys/HanhirovaKSSHY18} study the inference performance of CNNs on both edge devices and cloud backend, but use different hardware devices and frameworks as can be seen in table \ref{frameworks_hardware_2}. 
The authors use image classification (MobileNetV1 and InceptionV2) as well as object detection models (SSD MobilenetV1 COCO, SSD InceptionV2 COCO
and VGG16 FasterRCNN PASCAL VOC) for their experiments, but we only cover the results of the image classification models, since these are more relevant for this thesis.
The authors focus on the inference metrics latency and throughput, in particular with regard to various batch sizes, that they incrementally increase in the course of their experiments.

The authors conduct experiments with multiple edge devices.
In the case of the Android experiments, $480\times640$ \emph{JPG} images get captured. Afterwards, these images get resized, cropped and normalized during the preprocessing step before getting fed to the deep learning models mentioned earlier.




For edge inference, the authors also experiment with running two neural network instances simultaneously in separate threads instead of one. 
These two neural network instances lead to a higher throughput in the case of the Android phone, but to a decrease for the Jetson TX 2. They claim this decrease originates from the "context switch overhead of scheduling
the two processes" \cite{DBLP:conf/mmsys/HanhirovaKSSHY18}.

For both cloud inference frameworks, TensorFlow Serving and TensorRT throughput increase with increasing batch size until a certain saturation point, that depends on factors such as the model, hardware and the configuration.

Like Guo, they also identify the model loading process as a bottleneck on the Android phone, contrary to the inference time, which they call "comparatively fast" \cite{DBLP:conf/mmsys/HanhirovaKSSHY18}.

\begin{table}[H]
\centering
\caption{Used hardware and inference frameworks in \cite{DBLP:conf/mmsys/HanhirovaKSSHY18}}
\begin{tabular}{@{}lll@{}}
\toprule
 & Frameworks & Hardware \\
 \midrule
Edge & Tensorflow Java API; Snapdragon NPE & Nokia 8; Jetson TX 2 \\
Cloud & TensorFlow Serving; TensorRT & \begin{tabular}[c]{@{}l@{}}Intel i7 7700K, Nivida 1080GTX;\\ i7 8700K, Nvidia Titan V\end{tabular}\\
\bottomrule

\end{tabular}

\label{frameworks_hardware_2}
\end{table}

The authors conclude "that CNNs for mobile computer vision have significant latencyâ€“ throughput trade-offs, but the behavior is very complex" \cite{DBLP:conf/mmsys/HanhirovaKSSHY18} caused by a number of different factors affecting the performance. This indicates that performance results cannot be transferred to different model/hardware/framework settings.



\section{MLPerf}
MLPerf is a project that strives to create a "common set of benchmarks that enables the machine learning (ML) field to measure system performance for both training and inference from mobile devices to cloud services" \cite{mlperfWebsite}.
They want to create these benchmarks with fair, useful and replicable  measurements using representative workloads \cite{mlperf}. 

They aim to benchmark a wide variety of model types (image classification, translation, reinforcement learning,\,...), different models for each type (ResNet50, MobileNet,\,...), hardware and frameworks (Caffe2, TensorFlow Lite,\,...).

On December 12th 2018 the first results for model training were published, with results for inference yet to be released.


\section{Other publications}
There are various other publications studying either cloud or edge inference or both, among these are:



\cite{DBLP:journals/corr/abs-1810-01109} provides a study on the current state of model inference on Android smartphones, including available frameworks, deep learning models and hardware accelerators. Additionally, they present performance results for different hardware configuration using nine different tests including image recognition.

\cite{rethinkingDeployment} proposes a new approach for developing and deploying machine learning models optimized for inference on edge devices. Based on this approach they build a prototype to demonstrate the advantages of their proposed solutions.

%Clipper?

\cite{DBLP:journals/corr/HuangRSZKFFWSG016} presents "a guide for selecting a detection architecture that achieves the right
speed/memory/accuracy balance for a given application and platform" \cite{DBLP:journals/corr/HuangRSZKFFWSG016}.



\cite{DBLP:journals/corr/CanzianiPC16} presents an analysis of the impact of popular deep learning models like InceptionV4 on accuracy, memory, inference latency and power consumption for practical applications.


In \cite{jiang2018efficient} Jiang and others present a pipeline for optimization of edge inference performance.
Their approach consists of a two-step process, where they first optimize the computation graph as much as possible e.g. by applying quantization.
The second step is called kernel optimization and proposes a search for the optimal kernel implementation for the operations of the deep learning model.
This search heavily depends on the hardware specification on the edge device, for example, different instruction sets allow different kernel optimizations.

In \cite{201468} Crankshaw et al. present the Clipper framework for deploying machine learning models at the cloud.
The proposed framework, with a focus on low-latency, integrates among other things caching, dynamic batch sizes, delayed batching and online model selection.
Clipper supports multiple machine learning frameworks such as TensorFlow, Caffe and Scikit-Learn.
After a comparative evaluation of both Clipper and TensorFlow Serving, the authors come to the conclusion that both frameworks achieve comparable latency and throughput results, while Clipper additionally enables additional features such as caching and online learning.

\section{Evaluation of the Related Work}
For both the works of Guo \cite{DBLP:conf/ic2e/Guo18} and Hanhirova \cite{DBLP:conf/mmsys/HanhirovaKSSHY18} it can be stated that meanwhile better frameworks, model and edge devices have been developed.
Frameworks like TensorFlow Lite in combination with Android Neural Networks API provide optimized operators/kernels along with accelerator support, which can speed up inference substantially while using less system resources.
Also, more edge devices carrying these accelerators, some even dedicated to AI applications, are being released at a fast pace.
Furthermore, deep learning models developed explicitly for edge deployment like MobileNet have been published. These models can even be further optimized with the help of techniques like quantization, that severely lead to improved speedup inference latencies and model sizes, while only taking little impact on model accuracy.

Another aspect essential for the inference performance, that is not studied in depth by most publications, is the impact of preprocessing on performance.
Although \cite{DBLP:conf/ic2e/Guo18} and \cite{DBLP:conf/mmsys/HanhirovaKSSHY18} measure the latency of the preprocessing process, they do not report other metrics such as memory consumption. The effect of preprocessing on the cloud is also not studied in depth by any of the presented papers.


MLPerf looks to be a promising project, but it is still in an early phase with no inference results presented so far. Hence it is still unclear which metrics besides inference latency/throughput they measure and how they measure them.

Therefore at the moment of writing it can be stated that no other published study covers the end-to-end pipeline of preprocessing and inference at the cloud or the edge in a comparative manner with the selected combination of frameworks, workloads, models and hardware specifications. 
Additionally, none of the analyzed publications proposes a performance or a decision model for the optimal selection of deployment options for deep learning inference.

\vspace{0.5cm}
This chapter gave an outline and evaluation of publication on the topics related to this thesis, which will be useful when we evaluate the results of our own benchmarks, which will be presented in a section \ref{chap:Evaluation}.
In the following chapter, the methodology for performance evaluation of deep learning inference will be introduced.

 \endinput 
