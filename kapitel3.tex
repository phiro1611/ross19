\chapter{Methodology}
%%WHAT Is inference3
\label{chap:methodology}
In this section a performance model, characterising the deployment of deep learning models for inference, is presented. To understand these characteristics, the underlying factors that affect it need to be understood first. Therefore we first describe the problem space before we can map the problem space on the performance model.


\section{Problem Space}
The inference performance of a deep learning model is mainly affected by three factors, the architecture of the model itself and the hardware environment where it gets deployed and the framework performing the operations of the model on the hardware environment.
These three aspect are covered in this section.


\subsection{Deep Learning}
%%Bisschen genauer was deep learning input output...
%Einfluss auf inference latency an memory?
This section gives a brief, but nowhere complete overview of the deep learning field. We will focus on the basic structures and intuition behind deep learning models, as a comprehensive analysis would go beyond the scope of this thesis.

Deep learning are a subclass of neural networks.
The intention of a neural network is to make a prediction $\mathbf{\Hat{Y}}$ given an input $\mathbf{X}$ that is as close as possible to the underlying ground truth $\mathbf{Y}$. 
The difference of neural networks and deep learning networks is the amount of layers and hidden units. Deep learning models are built of many hidden layers and hidden units leading to deep architectures.
These deep architectures with often millions of parameters lead to a significant increase in computational power for both training and inference.
%While for simple linear regression model with a single variable a single computation
% ground through Y and the net predictions Y hat
\begin{figure}[!htb]
%%bild mit simpler architecture? Bild->conv->pool->flatten->dense-> output
    \centering
    \resizebox{.8\linewidth}{!}{\input{Bilder/simple_neural_network.tex}}
    \caption{Simple neural network}
    \label{fig:simpleNN}
\end{figure}

Figure \ref{fig:simpleNN} shows the structure of a simple neural network with input $\mathbf{X}$ and output $\mathbf{\Hat{Y}}$, where $\mathbf{X}$ is a vector of size four, meaning the neural network takes four features as its inputs. 
The output $\mathbf{\Hat{Y}}$ for this example is vector of only size one, hence the network predictions one value.
Between the input and output lay three fully connected hidden layers with five, three and again five hidden units.
Each of these hidden layers has a weight matrix $\mathbf{W\in\mathbb{R}^{m\times n}}$, where $\mathbf{m}$ is the number of hidden units and $\mathbf{n}$ the number of hidden units in the previous layer.
This weight matrices are needed to compute the state of each hidden layer by calculated the weighted sum on all its inputs.
For example the state of the first hidden layer is calculated by multiplying $\mathbf{W_1X}$, where $\mathbf{W_1}$ is the weight matrix of layer one and $\mathbf{X}$ the output of the previous layer, in this case the input layer.
To calculate  $\mathbf{\Hat{Y}}$ in the output layer using $\mathbf{W_O}$ , all previous states need to calculated consecutively. 
This chain of calculations needed to get from input $\mathbf{X}$ to output $\mathbf{\Hat{Y}}$ is also called forward-propagation, because the values of the input vector get passed through all layers by multiplication.
For example \ref{fig:simpleNN} all computations can be seen in equation \ref{eq:1}, with the shapes of all input/output vectors and weight matrices depicted in equation \ref{eq:2}.

\begin{equation} \label{eq:1}
\begin{gathered}
\mathbf{\hat{Y}(X)} = \mathbf{W_O(W_3(W_2(W_1X)))}
\end{gathered}
\end{equation}


\begin{equation} \label{eq:2}
\begin{gathered}
\mathbf{X} = \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           x_{3} \\
           x_{4}
         \end{bmatrix},
\mathbf{W_1} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}& w_{14}\\
           w_{21} & w_{22}& w_{23}& w_{24} \\
           w_{31} & w_{32}& w_{33}& w_{34} \\
           w_{41} & w_{42}& w_{43}& w_{44} \\
           w_{51} & w_{52}& w_{53}& w_{54} 
         \end{bmatrix},
\mathbf{W_2} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}& w_{14}& w_{15}\\
           w_{21} & w_{22}& w_{23}& w_{24}& w_{25} \\
           w_{31} & w_{32}& w_{33}& w_{34}& w_{35} 
         \end{bmatrix},\\ \\
\mathbf{W_3} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}\\
           w_{21} & w_{22}& w_{23} \\
           w_{31} & w_{32}& w_{33} \\
           w_{41} & w_{42}& w_{43} \\
           w_{51} & w_{52}& w_{53} 
         \end{bmatrix},
\mathbf{W_O} = \begin{bmatrix} w_{11} & w_{12}& w_{13}& w_{14}& w_{15} \end{bmatrix},
\mathbf{\hat{Y}} = \begin{bmatrix} y_1 \end{bmatrix}
%\hat{Y} = (1\times5)(5\times3)(3\times5)(5\times4) (4\times1) = (1\times5)(5\times3)(3\times5)(5\time1) = (1\times5)(5\times3)(3\times1) = (1\times5)(5\times1) = 1
\end{gathered}
\end{equation}

For sake of simplicity the example does not use activation functions $\mathbf{\sigma}$ (e.\,g. ReLU) and biases $\mathbf{b}$, which would usually be used in practice in the form of $\mathbf{\sigma(WX+b)}$ instead of $\mathbf{WX}$.
Activation functions are needed to decide if a neuron (hidden unit) fires or not as well as minimising the risk of exploding/vanishing gradients.
%beispielrechnung für das nn


This simple network only contains three hidden layers with at most five hidden layers and thus 45 parameters, but in practice state of the art deep learning models contain often millions of parameters. 
The forward propagation of such models often need more than one GFLOPS, leading to high computational demands for real-time AI applications.

%15+5+15+20 = 45 parameters
%large network >10 000 000 param

There are many different operators and network architectures, all with different computational demands, thus making performance prediction very complex.
Since we will use image classification as a use case for the experimentation, we will now present the basics of Convolutional Neural Networks, which are used for this use case.

\paragraph{Convolutional Neural Network}
This specific class of neural network achieved wide success in the field of image classification and detection in images. Convolutional Neural Networks (CNN) are used to extract features like complex shapes or colour patterns from images to perform image classification. 
%Abschnitt über convolutions und pooling?

Image classification network classify the contents of a image by outputting a class for the image with a respective confidence level, for example how likely an animal on a picture represents a cat.
A simple architecture for this classification task can be seen in figure \ref{fig:simpleCNN}.
The network expects input images as a three dimensional tensor $h\times w\times c$, where $h$ and $w$ are the fixed height and width of the image and $c$ is the number of channels, which in most cases is $3$ for the RGB channels.
An image needs to be preprocessed to match these shape specifications before feeding it to the network.
The fundamental building blocks of CNNs are multiple convolution layers, which extract features from the images. The first convolutional extract simple features like edges from the image, while the later convolutional layers combine these simple features to complex shapes like rectangles.
To reduce the spatial dimension of the feature maps, outputted by the convolution, a popular layer type added to CNNs are pooling layers.
After extracting the features the resulting tensor get flattened to a one dimensional tensor before getting fed to one or more fully connected layers.
The last fully connected layer contains the output of the network, in this case confidence levels for predefined labels (e.\,g. $80$\% cat).  


\begin{figure}[!htb]
%%bild mit simpler architecture? Bild->conv->pool->flatten->dense-> output
    \centering
    \resizebox{.95\linewidth}{!}{\input{Bilder/simple_CNN.tex}}
    \caption{Simple image classification CNN}
    \label{fig:simpleCNN}
\end{figure}

%Add calculation example
\subsubsection{Training}

After constructing the architecture of a model it can be trained to reach the highest accuracy.
Since most neural network are type of supervised learning, a training data set with ground truth labels $\mathbf{Y}$ is needed for the training.
This training is done with forward- and back-propagation, where one or more samples of the training set is forward propagated through the network until reaching the output layer. Afterwards the $Loss\mathbf{(Y,\Hat{Y})}$ can be calculated, which measures the difference between the prediction $\mathbf{\Hat{Y}}$ and the ground truth $\mathbf{Y}$.

To optimise the network, the loss needs to be minimised to a minimum, meaning that the different between prediction and ground truth is as small as possible and the accuracy as high as possible.
Since there is no closed form solution to find this minimum, neural network use gradient descent, specifically back-propagation, to minimise the loss in an iterative process.
Given the loss of the network, the gradients of all weights in the network can be computed and afterwards the weights can be updated using a learning rate in respect to the gradient and learning rate.

This process of forward- and back-propagation is repeated for all sample in the training data set multiple times until the minimum loss is reached and therefore the highest possible accuracy.

\subsubsection{Inference}
%only forward prop

After the model is trained it is ready to be deployed in a production environment, where it can serve its predictions to an application. This process of forward propagating the input through a neural network, performing all computational operations defined in the model resulting in an output/prediction, is called inference.

But since models often require the input to be of a certain shape and the input given by an application often does not match that shape, a preprocessing step is often needed before the actual inference can be performed.
This preprocessing and inference process can also be seen in figure \ref{fig:InfProcess}.
%%Add visualization of input here
%%die dann im deployment wieder aufgreifen
\begin{figure}[H]
\centering
\input{./Bilder/inference_process.tex}
\caption{Visualisation of the inference process including preprocessing}
\label{fig:InfProcess}
\end{figure}
Note that the architecture of the inference model is often slightly different from the training model, as some operators are only needed for the training process and are thus removed for inference (e.\,g. dropout).






\subsubsection{Quantization}
\label{chap:quant}
Quantization is a technique that trades in model precision for better inference latencies, memory consumption during inference and model sizes.
Quantization describes the process of reducing the "precision representations of weights and, optionally, activations" \cite{tfLiteQuant} from floating point precision to for example 8-bit.
The weights/activations are either quantized after training the model with floats (Post Training Quantization) or the model is training with quantized weights/activations from the start (Quantization Aware Training). In the paper "Quantizing deep convolutional networks for
efficient inference"\cite{Quantizing} Krishnamoorthi performed a study on these technique with the following results:
8-bit quantization can lead to a model size reduction by a factor of 4, a 2-3x latency speedup on CPUs and DSPs, while reducing the model accuracies by 1\%.


\subsection{Deployment of Deep Learning Models}
Deep learning deployment describes the process of deploying a trained machine learning model to a production environment for inference purposes. 

%%Write more here
There can be differentiated in two different deployment methods, the first one is deploying the model directly to edge devices, while the second outsources the model to a cloud-backend, where the inference is performed and the prediction is sent back to the edge device.
\subsubsection{Edge Deployment}
Edge devices are characterised by offering a limited amount of hardware specifications such as amount of available energy, memory and overall computational power.
These devices are often mobile devices like smartphones, cars or Raspberry Pis.
Despite its limits, edge deployment also has some advantages, particular in reliability and security. 
For example in the case of image classification images are needed for the inference, which often contain sensible information and thus raise a data privacy/security concern.
Therefore edge deployment should be the preferred choice, if the inference performance is sufficient for the use case of real time AI applications.

In order to increase this performance various accelerators like better GPUs, TPUs or other dedicated neural network hardware components have been developed for edge devices.



\subsubsection{Cloud Deployment}
While the breakthroughs in deep learning are very interesting for AI applications on edge devices, the computational demand needed for edge deployment often exceeds the available power to be viable.
That is why the option of outsourcing the models to a cloud-backend has become a popular solution in the recent years.
Cloud-backends offer a huge amount of computational power, especially suitable for deep learning in the form of GPUs, TPUs, etc.


The big downside of cloud-based inference is the needed network connection, in particular for edge devices such as cars, where a reliable network connection can often not be guaranteed for example in rural areas. Hence this is not a viable solution for applications that are critical like autonomous driving.

While for the edge deployment necessary preprocessing steps are always done on the edge, for the case of cloud inference there a two possibilities. Either the input data is preprocessed to the correct shape before getting sent to the cloud-backend, or the input is sent unpreprocessed to the cloud. 
In the latter case the input gets preprocessed on the cloud. 
This can be justified by the same reasons as for the cloud inference, more computational power for intensive preprocessing.



\subsection{Deep Learning Inference Framework}
For both edge and cloud inference the underlying inference framework is important for the inference performance, since optimised kernels/operators or support of accelerators like GPUs can lead to significant performance improvements.
The framework needs to support the model operations as well as the hardware components to achieve a good performance.

%cloud inference network protocol

This is especially important for edge devices, where new accelerators are being developed at a fast pace.


%%
After analysing the problem space of deep learning inference for real-time AI applications on edge devices we can propose a performance model, that maps the aspects of the problem space on the performance.



\section{Performance Model}
All the aspects of hardware, model and the used inference framework have an impact on the performance, thus are the influencing factors of the performance model (see figure \ref{fig:perfmodel}). 
These inputs affect various performance metrics, of which inference latency and throughput are the most important ones for real-time AI applications on the edge. However, as the hardware on edge devices is limited and most of the times more than one application needs to run in parallel, low usages of the other depicted metrics are vital as well.
In the following we present performance metrics essential for the performance of real-time AI applications and the rationale behind them.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/trade_offs.png}
\caption{Performance Model}
\label{fig:perfmodel}
\end{figure}

Except the data consumption, which is only needed for cloud inference, because data is sent to a remote server, all metrics are relevant for both edge and cloud inference.
While accuracy is one of the most critical metrics for neural networks, it is only affected by the characteristics of the model and not by the deployment environment. Therefore we do not focus on accuracy in this thesis.

To get the model output for a given model input two steps are needed. The first one is the preprocessing step and transforms a given input into the format that is required by the deep learning model. Only then the actual inference operation can be performed to obtain the model output. Therefore the preprocessing takes a vital part in the general inference process and should be included in the performance model.

\subsection{Performance Metrics}
\label{chap:metrics}
Several metrics are important to measure the performance of the preprocessing and the inference steps. For the most of the following metrics we break down each metric into two sub-measurements, one for the preprocessing  and the other for the inference part.

Note that we only measure the impact on inference on the resources on the edge devices, not on the cloud-backend in case of cloud inference.
\subsubsection{Latency}
Latencies are a essential to determine the performance, since AI application often need predictions in realtime, resulting in the need of low latencies.
The time needed to transform the original input to a shape fit for feeding into the deep learning model is called $Latency_{preprocessing}$.
%%WALL CLOCK TIME ADD HERE
$Latency_{inference}$ describes the time needed from requesting a prediction from a deep learning model given an specific input until getting the prediction.
The latency needed to perform both preprocessing and inference for a given input is called $Latency_{total}$.
\subsubsection{Throughput}
In order to accomplish realtime AI a certain level of throughput is essential. Therefore the number of predictions per second is a valuable metric. 
Since preprocessing is a vital part of the inference process, the throughput impact caused by preprocessed is also of interest.

Therefore we differentiate between three types of throughput: $Throughput_{preprocessing}$, $Throughput_{inference}$ and $Throughput_{total}$.
$Throughput_{total}$ is the throughput of the preprocessing and inference latencies combined.



\subsubsection{Energy Consumption}
$Energy_{preprocessing}$ and $Energy_{inference}$, which describe the amount of energy consumed during preprocessing and inference, are particular important for mobile edge devices, since they often are powered by batteries with a limited lifespan. If the preprocessing or inference process consumes too much energy, the application using the model is not viable.


\subsubsection{CPU/GPU Usage}
Since the preprocessing/inference operations are most of the time not the only processes running on a system and other processes need to run simultaneously, the usages of CPU ($CPU_{inference}$, $CPU_{preprocessing}$) and GPU ($GPU_{inference}$, $GPU_{preprocessing}$) or other available accelerators are an important metric.
High usages would also indicate that performance would eventually be slowed down on devices with less CPU/GPU power.


\subsubsection{Memory Usage}
Similar to CPU usage, preprocessing and inference should not occupy the whole memory of the system or even demand more memory than the available memory. Therefore $Memory_{inference}$ and $Memory_{preprocessing}$ are of interest.
%\subsubsection{GPU Usage}
%If a GPU or an another accelerator is available their usage is of interest.

\subsubsection{Data Consumption}
$Data_{transmitted}$ and $Data_{received}$ are only relevant for cloud inference, as a request has to be sent to a remote server and the according response with prediction has to be sent back to the client. A high data consumption could slow down the inference latency significantly if the up- and downstream of the network connection is too slow. 

This is in particular important for the case where the input data for the model is preprocessed on the cloud, because the input data is often resized to a smaller shape during preprocessing. Therefore cloud preprocessing increased the amount of data that needs to be transmitted to the server. For slow network connections this could result in higher inference latencies.

%%Add table here?



%%%%%%%%%%%%%%%%%%%%%%%%%%
This section presented the factors influencing the performance of cloud and edge inference and the for the performance significant metrics.
To model the relation between these factors and metrics, we perform experiments on both edge and cloud inference using state of the art hardware, model and framework components. The setup and the result of these experiments are covered in the next section.
\endinput 