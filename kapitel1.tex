\chapter{Introduction}


%%ADD: android application
The last few years have been marked by tremendous successes in the field of deep learning.
Deep learning models have become very accurate at complex tasks such as image classification, object recognition or natural language processing. 
With this success the number of possible AI application using these models increased rapidly, especially on edge devices such as cars, smartphones or IoT devices.
These application often need predictions at real-time to to unfold their full potential, while maintaining highly accurate.

However, as the accuracy of these models have risen, the computational demands of them have also due to their increasing architectural size, and this has taken a toll on the performance on these models apart from the accuracy, in particular on performance aspects such as inference latency, memory consumption, energy consumption, CPU usage and throughput. 
While inference time and throughput are the essential metrics to ensure real-time AI, memory usage, energy consumption and CPU usage are also crucial in order to avoid that one application consumes the majority of the system resources.
This is a concern especially for edge devices, where hardware or energy with mobile devices is often limited, and that is why the question arose whether the models should be deployed directly on the edge devices or on a cloud-backend where more computational power is available. 


In order to support users in the decision to select the optimal deployment option, this thesis proposes a performance model by taking all factors influencing inference performance into account and mapping them on the for real-time AI application critical performance metrics.
There are mainly three factors influencing the inference performance: the hardware specification of the deployment environment, the architecture of the deep learning model and the framework responsible for performing the inference operations of the deep learning model on the hardware components.

To back up this relationship proposed in the performance model with real workload measurements we perform an empiric study on both edge and cloud inference using a self-developed Android benchmark application as well as state of the art deep learning models, inference framework and hardware components, both at the edge and at the cloud-backend.
During these experiments, we study the impact of inference on the performance metrics mentioned above, which are relevant for real-time AI applications running on edge devices.

There exists a rich variety of different neural network types, with the most popular being convolutional neural networks(CNNs) and recurrent neural networks(RNNs).
Many of these models use unique operators with different impact on inference performance.
Since it is not feasible to benchmark all these different network types, this thesis will focus on CNNs, specifically image classification models classifying the contents of a given input image.




This rich variety of configurations also applies to the hardware aspects, where in addition to better CPUs and GPUs a rising number of specific accelerators such as TPUs, neuromorphic co-processors, FPGA have been developed for both edge and cloud.
For the experiments, we will use a OnePlus 6T, a state of the art smartphone, as our edge device and a server with a Nvidia Tesla P100 as the cloud-backend.


TensorFlow Serving and TensorFlow Lite, both open-source, will serve as the example frameworks for these experiments, as they support the deployment the deep learning models to a cloud-backend (TensorFlow Serving) or directly to edge devices (TensorFlow Lite). These frameworks supported many of the above-mentioned hardware accelerators as well as providing optimised operators used by image classification models.

Another aspect, aside from the actual inference, is the preprocessing step, which is needed to transform the input into the format required by the model. 
In the case of CNNs these steps often consist of image resizing and normalization.
Since this step is vital for the inference, our experiments will also include preprocessing aside from the actual inference.

\section{Structure of the Thesis}
The rest of this thesis is structured as follows: Chapter \ref{chap:relatedWork} provides an outline and evaluation of existing related work on this topic. Next, we propose a performance model characterising the deployment of deep learning models by mapping the problem space, which is presented beforehand, to important performance metrics.
Chapter \ref{chap:experiments} deals with the experiments, their design, instantiation and presentation of the results including an evaluation including recommendations for the optimal selection of the deployment option.
Finally, this thesis closes with a conclusion and possible future work topics inspired by this thesis.
