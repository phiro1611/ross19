\appendix
\chapter{Appendix}
\section{Additional Results}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_CPU.pdf}
\caption[Edge vs. Cloud Inference: $CPU_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference: $CPU_{preprocessing}$ - lower is better - 
All $CPU_{preprocessing}$ usages are around $12.5\%$, cloud inference with cloud preprocessing has tendencies to a slightly lower usage.}
\label{fig:CloudEdgePreproCPU}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies_WITHOUT_NETWORK.pdf}
\caption{Edge vs. Cloud Inference: $Latency_{inference}$ without $Latency_{network}$ - lower is better}
\label{fig:CloudEdgeInfLatWONetwork}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Latencies_only_CLOUD_NR.pdf}
\caption{Edge vs.  Cloud Inference for larger Batch Sizes: $Latency_{inference}$ result of cloud inference and $224^2/299^2$ images - lower is better}
\label{fig:BatchSizeLatenciesCloud}
\end{figure}






\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Received_Data.pdf}
   \caption{$Data_{received}$}
   \label{fig:BatchSizeReceivedData} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Transmitted_Data.pdf}
   \caption{$Data_{transmitted}$}
   \label{fig:BatchSizeTransmittedData}
\end{subfigure}

\caption{Edge vs.  Cloud Inference for larger Batch Sizes:  $Data_{received}$ vs. $Data_{transmitted}$ - lower is better}
\end{figure}


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_server_lat.pdf}
   \caption{$Latency_{server}$}
   \label{fig:BatchSizeServer} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_network_lat.pdf}
   \caption{$Latency_{network}$}
   \label{fig:BatchSizeNetwork}
\end{subfigure}

\caption{Edge vs.  Cloud Inference for larger Batch Sizes:  $Latency_{server}$ vs. $Latency_{network}$ - lower is better}
\end{figure}



%Add not so important result figures here%
%Table with the metrics
%\input{./Bilder/metrics_table.tex}
\begin{sidewaystable}


    \centering
    \caption{InceptionV4 - Averages for all Metrics including standard deviation}
\scalebox{0.345}{
\input{./Bilder/table_inception.tex}}

\label{measurementsInception}
\end{sidewaystable}
\begin{sidewaystable}


    \centering
    \caption{MobileNetV2 - Averages for all Metrics including standard deviation}
\scalebox{0.345}{
\input{./Bilder/table_mobilenet.tex}}

\label{measurementsMobilenet}
\end{sidewaystable}

