\chapter{Fundamentals}
\label{chap:fundamentels}
In this chapter, we briefly cover the fundamentals of deep learning models and their deployment on edge devices and cloud-backends for inference purposes.


%The inference performance of a deep learning model is mainly affected by three factors, the architecture of the deep learning model itself, the hardware environment where it gets deployed and the framework performing the operations of the model on the hardware environment.
%These three aspect are covered in this section.

\section{Deep Learning}
%%Bisschen genauer was deep learning input output...
%Einfluss auf inference latency an memory?
This section gives a brief, but nowhere complete overview of the deep learning field. We will focus on the basic structures and intuition behind deep learning models, as a comprehensive analysis would go beyond the scope of this thesis.

%Deep learning are a subclass of neural networks, which differ from normal neural network by having large architectures in the form of layers, leading to high computational demands.
Neural networks try make a prediction $\mathbf{\Hat{Y}}$, for a  given an input $\mathbf{X}$, that is as close as possible to the underlying ground truth $\mathbf{Y}$. 
Deep learning is a subclass of neural networks.
The difference between neural networks and deep learning networks is the number of layers and hidden units. Deep learning models consist of many hidden layers and hidden units leading to deep architectures.
These deep architectures with often millions of parameters lead to a significant increase in computational power for both training and inference.
%While for simple linear regression model with a single variable a single computation
% ground through Y and the net predictions Y hat
\begin{figure}[!htb]
%%bild mit simpler architecture? Bild->conv->pool->flatten->dense-> output
    \centering
    \resizebox{.8\linewidth}{!}{\input{Bilder/simple_neural_network.tex}}
    \caption{Simple neural network with 4 inputs, 3 hidden layers and 1 output.}
    \label{fig:simpleNN}
\end{figure}

Figure \ref{fig:simpleNN} shows the structure of a simple neural network with input $\mathbf{X}$ and output $\mathbf{\Hat{Y}}$, where $\mathbf{X}$ is a vector of size four, meaning the neural network takes four features as its input. 
The output $\mathbf{\Hat{Y}}$ for this example is a vector of only size one, hence the network predicts one value.
Between the input and output of this example network lay three fully connected hidden layers, also called dense layers, with five, three and again five hidden units.
Each of these hidden layers has a weight matrix $\mathbf{W\in\mathbb{R}^{m\times n}}$, where $\mathbf{m}$ is the number of hidden units and $\mathbf{n}$ the number of hidden units in the previous layer.
This weight matrices are needed to compute the state of each hidden layer by calculating the weighted sum of all its inputs.
For example, the state of the first hidden layer is calculated by multiplying $\mathbf{W_1X}$, where $\mathbf{W_1}$ is the weight matrix of layer $1$ and $\mathbf{X}$ the output of the previous layer, in this case, the input layer.
To calculate  $\mathbf{\Hat{Y}}$ in the output layer using $\mathbf{W_O}$, all previous states need to calculated consecutively. 
This chain of calculations needed to get from input $\mathbf{X}$ to output $\mathbf{\Hat{Y}}$ is also called forward-propagation because the values of the input vector get passed through all layers by multiplication.
For the example network in figure \ref{fig:simpleNN} all computations can be seen in equation \ref{eq:1}, with the shapes of all input/output vectors and weight matrices depicted in equation \ref{eq:2}.

\begin{equation} \label{eq:1}
\begin{gathered}
\mathbf{\hat{Y}(X)} = \mathbf{W_O(W_3(W_2(W_1X)))}
\end{gathered}
\end{equation}


\begin{equation} \label{eq:2}
\begin{gathered}
\mathbf{X} = \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           x_{3} \\
           x_{4}
         \end{bmatrix},
\mathbf{W_1} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}& w_{14}\\
           w_{21} & w_{22}& w_{23}& w_{24} \\
           w_{31} & w_{32}& w_{33}& w_{34} \\
           w_{41} & w_{42}& w_{43}& w_{44} \\
           w_{51} & w_{52}& w_{53}& w_{54} 
         \end{bmatrix},
\mathbf{W_2} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}& w_{14}& w_{15}\\
           w_{21} & w_{22}& w_{23}& w_{24}& w_{25} \\
           w_{31} & w_{32}& w_{33}& w_{34}& w_{35} 
         \end{bmatrix},\\ \\
\mathbf{W_3} = \begin{bmatrix}
           w_{11} & w_{12}& w_{13}\\
           w_{21} & w_{22}& w_{23} \\
           w_{31} & w_{32}& w_{33} \\
           w_{41} & w_{42}& w_{43} \\
           w_{51} & w_{52}& w_{53} 
         \end{bmatrix},
\mathbf{W_O} = \begin{bmatrix} w_{11} & w_{12}& w_{13}& w_{14}& w_{15} \end{bmatrix},
\mathbf{\hat{Y}} = \begin{bmatrix} y_1 \end{bmatrix}
%\hat{Y} = (1\times5)(5\times3)(3\times5)(5\times4) (4\times1) = (1\times5)(5\times3)(3\times5)(5\time1) = (1\times5)(5\times3)(3\times1) = (1\times5)(5\times1) = 1
\end{gathered}
\end{equation}

For the sake of simplicity the example does not use activation functions $\mathbf{\sigma}$ (e.\,g. ReLU) and biases $\mathbf{b}$, which would usually be used in practice in the form of $\mathbf{\sigma(WX+b)}$ instead of $\mathbf{WX}$.
Activation functions are needed to decide if a neuron (hidden unit) fires or not as well as minimizing the risk of exploding/vanishing gradients.
%beispielrechnung für das nn


This simple network only contains three hidden layers with at most five hidden layers and thus 45 parameters in total, but in practice state of the art deep learning models often contain millions of parameters. 
The forward propagation of such models often causes the need for more than $1$ GFLOPS, leading to high computational demands for real-time AI applications.

%15+5+15+20 = 45 parameters
%large network >10 000 000 param

There are many different operators and network architectures, all with different computational demands, thus making performance prediction very complex.
Since we will use image classification as a use case for the benchmarks in section \ref{chap:experiments}, we will now present the basics of Convolutional Neural Networks, which are used for this use case.

\paragraph{Convolutional Neural Network}
This specific class of neural network achieved wide success in the field of image classification and object detection.
The strength of Convolutional Neural Networks (CNN) is the ability to extract features like complex shapes or colour patterns from images. Thus this network is particularly suitable for image classification.
%Abschnitt über convolutions und pooling?

Image classification networks classify the contents of an image by outputting classes for the image with respective confidence levels, for example, how likely an animal on a picture represents a cat.
A simple architecture for this classification task can be seen in figure \ref{fig:simpleCNN}.
The network expects input images as a three-dimensional tensor $h\times w\times c$, where $h$ and $w$ are the fixed height and width of the image and $c$ is the number of channels, which in most cases is $3$ for the $3$ \emph{RGB} channels.
An image needs to be preprocessed to match these shape specifications before feeding it to the network.
The fundamental building blocks of CNNs are multiple convolution layers, which extract features from the images. The first convolutional extract simple features like edges from the image, while the later convolutional layers combine these simple features to complex shapes like rectangles.
To reduce the spatial dimension of the feature maps, outputted by the convolution, a popular layer type added to CNNs are pooling layers.
After extracting all features are extracted, the resulting tensor gets flattened to a one-dimensional tensor before getting reaching one or more fully connected layers.
The last fully connected layer contains the output of the network, in this case, confidence levels for predefined labels (e.\,g. $80$\% cat).  


\begin{figure}[!htb]
%%bild mit simpler architecture? Bild->conv->pool->flatten->dense-> output
    \centering
    \resizebox{.95\linewidth}{!}{\input{Bilder/simple_CNN.tex}}
    \caption{Simple CNN for image classification}
    \label{fig:simpleCNN}
\end{figure}

%Add calculation example
\subsection{Training}
Constructing the architecture of a model is the first step, but in order to enable models to make accurate predictions, they have to be trained, which means adjusting the weights of the network. 
Since most neural networks are a type of supervised learning, training data with ground truth labels $\mathbf{Y}$ is needed for the training.
This training is done with forward- and back-propagation, where one or more samples of the training set are forward propagated through the network until reaching the output layer. Afterwards, the $Loss\mathbf{(Y,\Hat{Y})}$ can be calculated, which measures the difference between the prediction $\mathbf{\Hat{Y}}$ and the ground truth $\mathbf{Y}$.

To optimize the network, the loss needs to be minimized, meaning that the difference between prediction and ground truth is as small as possible and thus the accuracy as high as possible.
Since there is no closed form solution to find this minimum most of the time, neural network use gradient descent, specifically back-propagation, to minimize the loss in an iterative process.
Given the loss of the network, the gradients of all weights in the network can be computed and afterwards the weights can be updated using a learning rate in respect to the gradient and learning rate.

This process of forward- and back-propagation is repeated for all sample in the training data set multiple times until the minimum loss is reached and therefore the highest possible accuracy.

\subsection{Inference}
%only forward prop

After the model is trained it is ready to be deployed in a production environment, where it can serve its predictions to an application. This process of forward propagating the input through a neural network, performing all computational operations defined in the model, resulting in an output/prediction, is called inference.

However, since models often require the input to be of a specific shape and the input given by an application often does not match that shape, a preprocessing step is often needed before the actual inference can be performed.
This preprocessing and inference process can also be seen in figure \ref{fig:InfProcess}.
%%Add visualization of input here
%%die dann im deployment wieder aufgreifen
\begin{figure}[H]
\centering
\input{./Bilder/inference_process.tex}
\caption{Visualization of the inference process for AI applications, including preprocessing.}
\label{fig:InfProcess}
\end{figure}
Note that the architecture of the inference model is often slightly different from the training model, as some operators are only needed for the training process and are thus removed for inference (e.\,g. dropout).





\subsection{Model Optimization}
There are various methods to optimize the architecture of a deep learning model to achieve better inference performance.
The difficulty in optimizing deep learning models is to improve inference performance while not loosing accuracy.
One technique is quantization, which we will briefly introduce in the following paragraph.
\subsubsection{Quantization}
\label{chap:quant}
Quantization is a model optimization technique that often trades in model precision for better inference latencies, memory consumption during inference and model sizes.
Quantization describes the process of reducing the "precision representations of weights and, optionally, activations" \cite{tfLiteQuant} from floating point precision to for example 8-bit.
The weights/activations are either quantized after training the model with floats (Post Training Quantization), or the model is trained with quantized weights/activations from the start (Quantization Aware Training). In the paper "Quantizing deep convolutional networks for
efficient inference"\cite{Quantizing} Krishnamoorthi performed a study on these techniques with the following results:
8-bit quantization can lead to a model size reduction by a factor of 4, a 2-3x latency speedup on CPUs and DSPs, while reducing the model accuracies by 1\%.


\section{Deployment of Deep Learning Models}
Deep learning deployment describes the process of deploying a (trained) machine learning model to a production environment for inference purposes. 

%%Write more here
There can be differentiated between two different deployment methods. The first one is deploying the model directly to edge devices, while the second outsources the model to a cloud-backend, where the inference is performed, and the prediction is then sent back to the edge device.
\subsection{Edge Deployment}
Edge deployment describes the process of deploying deep learning models to edge devices such as mobile devices like smartphones, cars or Raspberry Pis.
These edge devices are often characterized by offering a limited amount of computational power as well as limited energy.
This limited power often stands in contrast to the performance demands caused real-time AI applications using deep learning models.
Despite its limits, edge deployment also has some advantages, particularly in reliability and security. 
For example in the case of image classification images are needed for the inference, which often contain sensitive information and thus raise data privacy/security concerns.
Therefore edge deployment should be the preferred choice if the inference performance is sufficient for the use case of real-time AI applications.

In order to increase this performance, various accelerators like better GPUs, TPUs or other dedicated neural network hardware components have been developed for edge devices.
%%mention NNAPI here?



\subsection{Cloud Deployment}
While the breakthroughs in deep learning are very interesting for AI applications on edge devices, the computational demand needed for edge deployment often exceeds the available power to be viable.
That is why the option of outsourcing the models to a cloud-backend has become a popular solution in recent years.
Cloud-backends offer a large amount of computational power, especially suitable for deep learning in the form of large numbers of GPUs, TPUs or other accelerators.

The big downside of cloud-based inference is the needed network connection, in particular for edge devices such as cars, where a reliable network connection can often not be guaranteed for example in rural areas. Hence this is not a viable solution for applications like autonomous driving, where continuous and reliable predictions from deep learning models as critical for the feasibility of the application.

While for the edge deployment necessary preprocessing steps are always done on the edge, for the case of cloud inference there are two possibilities. Either the input data is preprocessed to the correct shape at the edge before getting sent to the cloud-backend, or the input is sent un-preprocessed to the cloud. 
In the latter case, the input gets preprocessed on the cloud. 
This can be justified by the same reasons as for the cloud inference, more computational power for intensive preprocessing.
Although slow network connection could diminish the speedup gained by the larger computational power, since un-preprocessed inputs are most of the times larger in payload size and therefore more data has to be sent to the server, resulting in the need of faster network connections to achieve the same network latency as a preprocessed input would.
However in the case of compressed inputs as for example \emph{JPEG} images, if the \emph{JPEG} and the preprocessed image have the same input size, the \emph{JPEG} image could lead to faster latencies, as less data has to be sent to the server and the I/O overhead at the server is minimized, as the server has to load a smaller image into memory assuming \emph{JPEG} decoding is faster than the I/O overhead of load a larger image. 



\section{Inference Framework}
For both edge and cloud inference, the underlying inference framework is important for the inference performance since optimized model operations or support of accelerators like GPUs can lead to significant performance improvements.
New hardware components like accelerators and new deep learning operations for example in the form of new activation functions are being developed at a fast pace. Therefore a well maintained and frequently updated framework is critical for the best inference performance possible.
Another challenge is the support of the wide variety of different types of edge devices with different system architectures.

An additional factor affecting cloud inference performance is the network component, especially the impact on latency.
Thus frameworks should implement low latency protocols with small payloads for environments with constrained networks.

In this thesis, we only evaluate the inference performance of a single client sending requests to a cloud-backend, but in reality, most of the time each edge device does not have exclusive access to a cloud-backend, resulting in multiple client making inference request to a single server, possibility even at the same time.
This raises questions on how to handle multiple requests coming from multiple sources in general and specifically how to handle multiple simultaneous requests and which requests should be prioritized.

Possible solutions include combining multiple requests and performing inference of multiple requests at once by combining them into a single batch.
This approach requires a policy on how many requests to combine at most and how long the interval is where the server waits on incoming requests if the optimal batch size is not yet reached. 
While this approach can increase throughput, it often comes at the cost of increased latency, thus negatively affecting the performance of real-time AI applications at the edge.


\vspace{0.5cm}
This chapters gave an introduction to the topic of deep learning inference and the deployment of deep learning models for this inference.
This knowledge will be the base for the following chapters.
The next chapter will give an outline of related work relevant for this thesis.
%%reduce data consumption?
%%REST vs gRPC
%%cloud inference needs to support multiple client-> low latency vs throughput
%%


