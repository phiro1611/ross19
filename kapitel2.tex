\chapter{Related Work}
\label{chap:relatedWork}
This chapter will give a brief outline and evaluation of related work on the topic of performance evaluation of deep learning inference. These related paper deal with the inference performance of either edge or cloud inference or both of them in a comparative evaluation.

\section{Performance of Edge vs Cloud Inference}
%\subsection{Speed/accuracy trade-offs for modern convolutional object detectors}
%In \cite{DBLP:journals/corr/HuangRSZKFFWSG016} the authors provide a guide for selecting the right convolutional object detection architecture for the desirable speed/memory/accuracy. To demonstrate this trade-offs the authors test various popular models such as Inception v2, MobileNet or Resnet 101. To create the benchmarks, "a machine with 32GB RAM,
%Intel Xeon E5-1650 v2 processor and an Nvidia GeForce
%GTX Titan X GPU card \cite{DBLP:journals/corr/HuangRSZKFFWSG016} is used.


\subsection{Cloud-based or On-device:
An Empirical Study of Mobile Deep Inference}
Guo "evaluate[s] the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application" \cite{DBLP:conf/ic2e/Guo18} for both cloud and edge inference. The hardware and frameworks used for the experiments leading to this evaluation can be seen in table \ref{frameworks_hardware_1}. The three tested CNNs are image classifcation models(AlexNet, NIN and SqueezeNet) with an image input size of 224x224.
They divided the experiment in 4 steps and measure the executing time of each step: 
The model loading time (Load model), the time needed to rescale the image into 224x224 bitmaps(Rescale bitmap), the time needed to upload the bitmap to the server in case of cloud inference(Upload bitmap) and the inference time(Compute probability).
Additionally they measured the CPU usage, battery and memory consumption during these 4 steps using Trepn Profiler.

They used 15 images in total, divided in three different image size groups (540x960, 1080x1920, 2160x3840) with the same 5 scaled images in each group.

After analyzing their measurements, they present the following results:

Cloud inference "exhibits substantial benefits
in terms of inference response time and mobile energy savings
over on-device inference, in this case by two orders of magnitude". As the responsible bottleneck for this conclusion they identify the model loading and inference times during edge inference.
Also it is more efficient to upload the 540x960 images directly to cloud without rescaling them on the edge beforehand.


\begin{table}[H]
\centering
\caption{Used hardware and inference frameworks in \cite{DBLP:conf/ic2e/Guo18}}
\begin{tabular}{@{}lll@{}}
\toprule
 & Frameworks & Hardware \\
 \midrule
Edge & Caffe Android Lib; CNNDroid & Nexus 5 \\
Cloud & Caffe & Intel Xeon E5-2670, Nvidia Grid K520, 15GB Memory\\
\bottomrule

\end{tabular}

\label{frameworks_hardware_1}
\end{table}
%%NICHT VOLLSTÄNDIG
After analysing their results the authors come to the conclusion that edge inference is inferior to cloud inference by factor two for both inference time and energy consumption. Thus cloud inference delivers better performance for the most cases. However due to improvements in models, hardware and frameworks, the authors "believe it is
very likely that on-device[edge] inference can be done efficiently in the near future". 
\subsection{Latency and Throughput Characterization of Convolutional
Neural Networks for Mobile Computer Vision}
Similar to Guo, the authors of this paper study the inference performance of CNNs on both edge devices and cloud backend, but use different hardware devices and frameworks as can be seen in table \ref{frameworks_hardware_2}. 
The authors use object recognition (MobileNet v1 and Inception v2) as well as object detection (SSD Mobilenet v1 COCO, SSD Inception v2 COCO,
and VGG16 FasterRCNN PASCAL VOC) models for their experiments, but we only cover the object recognition models, since these are more relevant for this thesis.
The authors focus on the metrics inference time and system throughput, in particular with regard to various batch sizes that they incrementally increase in the course of their experiments.

In the case of the Android experiments 480x640 jpeg images get captured. Afterwards they get resized, cropped and normalized during the preprocessing step before getting fed to the neural networks.

Like Guo they also identify the model loading process as a bottleneck on the Android phone, contrary to the inference time, which they call "comparatively fast".


For edge inference the authors also experiment with running two neural network instances simultaneously in separate threads instead of one. 
This leads to a higher throughput in the case of the Android phone, but to a decrease for the Jetson TX 2. They claim this decrease originates from the "context switch overhead of scheduling
the two processes".

For both TensorFlow Serving and TensorRT the throughput increases with increasing batch size until a saturation point that depends on factors such as the model, hardware and the configuration.

\begin{table}[H]
\centering
\caption{Used hardware and inference frameworks in \cite{DBLP:conf/mmsys/HanhirovaKSSHY18}}
\begin{tabular}{@{}lll@{}}
\toprule
 & Frameworks & Hardware \\
 \midrule
Edge & Tensorflow Java API; Snapdragon NPE & Nokia 8; Jetson TX 2 \\
Cloud & TensorFlow Serving; TensorRT & \begin{tabular}[c]{@{}l@{}}Intel i7 7700K, Nivida 1080GTX;\\ i7 8700K, Nvidia Titan V\end{tabular}\\
\bottomrule

\end{tabular}

\label{frameworks_hardware_2}
\end{table}

The authors conclude "that CNNs for mobile computer vision have significant latency–throughput trade-offs, but the behavior is very complex" caused by a number of different factors affecting the performance. This indicates that performance results can not be transferred to different model/hardware/framework settings.

\subsection{MLPerf}
MLPerf is a project that strives to create a "common set of benchmarks that enables the machine learning (ML) field to measure system performance for both training and inference from mobile devices to cloud services"\cite{mlperfWebsite}.
They want to create these benchmarks with fair, useful and replicatable measurements using representative workloads \cite{mlperf}. 

They aim to benchmark a wide variety of model types(image classification, translation, reinforcement learning,...), different models for each type(ResNet50, MobileNet,..), hardware and frameworks(Caffe2, TensorFlow Lite,...).

On December 12th 2018 the first results for model training were published, with results for inference yet to be released.

\subsection{Other publications}
Other related publications are: 
\cite{DBLP:journals/corr/abs-1810-01109} provides a study on the current state of model inference on Android smarthpones, including available frameworks, deep learning models and hardware accelerators. Additionally they present performance results for different hardware configuration using nine test including image recognition.

\cite{rethinkingDeployment} proposes a new approach for developing and deploying machine learning models optimised for inference on edge devices. Based on this approach they build a prototype to demonstrate the advantages of their proposed solutions.

%Clipper?

\cite{DBLP:journals/corr/HuangRSZKFFWSG016} presents "a guide for selecting a detection architecture that achieves the right
speed/memory/accuracy balance for a given application and platform"

\cite{DBLP:journals/corr/CanzianiPC16} presents a analysis of the impact of popular deep learning models like InceptionV4 on accuracy, memory, inference time and power consumption for practical applications.
\section{Evaluation of the Related Work}
%%Add advantages of NNAPI,quant here
For both the works of Guo\cite{DBLP:conf/ic2e/Guo18} and Hanhirova and others\cite{DBLP:conf/mmsys/HanhirovaKSSHY18} it can be stated that meanwhile better frameworks(TensorFlow Lite in combination with Android Neural Networks API) providing optimized operators/kernels and accelerator support, edge devices with accelerators and models that are specifically developed for edge deployment(MobileNet). Furthermore these models can also be further optimized with the help of techniques like quantization, that severely lead to improved speedup inference latency and model sizes, while only taking minimal impact on model accuracy.

MLPerf looks to be a promising project, but it is still in an early phase with no inference results presented so far. Hence it is still unclear which metrics besides inference time/throughput they measure and how they measure them.




 \endinput 
