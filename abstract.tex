\vspace*{2cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{1cm}

\begin{comment}
%\noindent When deploying realtime AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
%For the deployment of a model various things need to be considered, most importantly accuracy and inference time, but also energy consumption, memory consumption, throughput and retraining
\noindent When deploying real-time AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
For the model inference various things need to be considered, among other things preprocessing, computational demands,
specialized hardware (GPU, TPU, neuromorphic co-processors, FPGA),
network latencies and energy consumption. Most of these aspects depend on both the model as well as the environment where the model is deployed. 
In order to help with the optimal selection of cloud and edge inference to achieve real-time AI, this thesis proposes a performance model characterizing the deployment of a deep learning model and the resulting trade-offs.
Based on this performance model, the most essential trade-offs of the different deployment options get demonstrated by conducting multiple experiments using image classification as a use case. After the evaluation of these experiments, recommendations for the deployment are proposed.




When deploying real-time AI applications on edge devices, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or a cloud-backend.
While edge computing is often characterized by a lack of computational power and therefore struggles to provide a high throughput of predictions at a low-latency, outsourcing the inference to a cloud-backend raises concerns regarding security and reliability.
Although low-latencies and throughput are the most critical inference performance metric for real-time applications on edge devices, other metrics like CPU usage, data-, memory- or energy-consumption are affected by both cloud and edge inference and hence vital for the model deployment to consider.

%All these performance metrics are mainly affected by four factors: The model architecture, hardware, inference framework and the shape of the inference input.
%Significant research is being done in all these areas influencing the deployment question, resulting in the release of optimized frameworks (TensorFlow Lite, NNAPI, TensorFlow Serving), hardware accelerators (GPU, TPU, neuromorphic co-processors, FPGA) and models dedicated for edge devices (MobileNet).
-----------------------------------------------------------------------------
\end{comment}




\noindent In the wake of the recent achievements in deep learning, the number of possible real-time AI applications on edge devices using deep learning models has exploded.
However, deployment and execution of these models are often difficult as edge devices are more resource constrained than cloud-backends and cannot elastically scale on demand.
%Therefore, edge devices struggle to provide high inference throughput at low latencies.
Thus, understanding inference performance, sizing hardware and finding the optimal workload is crucial.
Outsourcing the inference to a cloud-backend can provide more computational power, but raises challenges regarding security, reliability and bandwidth constraints. 
While latency and throughput are the most critical inference performance metrics for real-time applications on edge devices, other metrics like CPU usage, data-, memory- or energy-consumption are also affected by both cloud and edge inference and hence vital for the model deployment to consider.

To help with the optimal selection of deployment options for deep learning inference, this thesis proposes a methodology for performance evaluation of deep learning inference.
This methodology investigates qualitative and quantitative differences between edge and cloud deployment options.
By instantiating this methodology, we get a better understanding of deep learning inference including benchmarks of the performance metrics mentioned above.
To generate these benchmarks we use image classification as a use case, supported by real-world workloads, hardware components and frameworks.
Evaluation shows, that edge inference can outperform cloud inference, given the right circumstances.
These include optimization of deep learning model architectures, utilization of optimized inference frameworks and minimization of preprocessing.
Based on these benchmarks we then develop a performance model, predicting the optimal deployment solution for a given system configuration using multiple linear regression.
Finally, we utilize the gained understanding to create a decision model supporting the deployment process for deep learning inference.



%%TODO add results 
%%Results: edge better for small models, cloud for large models; large batch sizes not viable at the edge, impact of preprocessing
%Our results show that edge devices can deliver better inference performance than cloud inference would for small models, but for large models cloud inference is still the preferred option for high performance, at least for the moment.
%Another key lesson is the impact of preprocessing on inference performance. The amount of preprocessing should be minimized

%proposes a performance model characterising the deployment of deep learning models and the resulting performance trade-offs.
