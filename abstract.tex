\vspace*{2cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{1cm}

\begin{comment}
%\noindent When deploying realtime AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
%For the deployment of a model various things need to be considered, most importantly accuracy and inference time, but also energy consumption, memory consumption, throughput and retraining
\noindent When deploying real-time AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
For the model inference various things need to be considered, among other things preprocessing, computational demands,
specialized hardware (GPU, TPU, neuromorphic co-processors, FPGA),
network latencies and energy consumption. Most of these aspects depend on both the model as well as the environment where the model is deployed. 
In order to help with the optimal selection of cloud and edge inference to achieve real-time AI, this thesis proposes a performance model characterising the deployment of a deep learning model and the resulting trade-offs.
Based on this performance model, the most essential trade-offs of the different deployment options get demonstrated by conducting multiple experiments using image classification as a use case. After the evaluation of these experiments, recommendations for the deployment are proposed.



-----------------------------------------------------------------------------
\end{comment}




\noindent When deploying real-time AI applications on edge devices, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or a cloud-backend.

While edge devices often lack computational power to provide high inference throughput, outsourcing the inference to a cloud-backend raises concerns regarding security and reliability.
Although low-latencies and throughput are the most critical inference performance metric for real-time applications, other metrics like CPU usage, data-, memory- or energy-consumption are affected by both cloud and edge inference and hence vital for the model deployment to consider.

All these performance metrics are mainly affected by three factors: The model architecture, the inference framework and the hardware constraints.
Significant research is being done in all three areas influencing the deployment question, resulting in the release of optimized frameworks (TensorFlow Lite, NNAPI, TensorFlow Serving), hardware accelerators (GPU, TPU, neuromorphic co-processors, FPGA) and models dedicated for edge devices (MobileNet).


In order to help with the optimal selection of cloud and edge inference to achieve real-time AI, this thesis proposes a methodology for performance evaluation of deep learning inference.
By instantiating this methodology we get a better system understanding of deep learning inference as well as measurements of the above mentioned performance metrics, using real life workloads in the form of an image classification use case.
The evaluation of these measurement show, that edge deployment can outperform cloud deployment, given the right circumstances.

These include optimization of deep learning model architectures, utilization of optimized inference frameworks and minimization of preprocessing.

Based on this system understanding we then propose a decision model, that supports the decision process of deploying a deep learning model for inference purposes.



%%TODO add results 
%%Results: edge better for small models, cloud for large models; large batch sizes not viable at the edge, impact of preprocessing
%Our results show that edge devices can deliver better inference performance than cloud inference would for small models, but for large models cloud inference is still the preferred option for high performance, at least for the moment.
%Another key lesson is the impact of preprocessing on inference performance. The amount of preprocessing should be minimized

%proposes a performance model characterising the deployment of deep learning models and the resulting performance trade-offs.
