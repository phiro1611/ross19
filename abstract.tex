\vspace*{2cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{1cm}

\begin{comment}
%\noindent When deploying realtime AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
%For the deployment of a model various things need to be considered, most importantly accuracy and inference time, but also energy consumption, memory consumption, throughput and retraining
\noindent When deploying real-time AI applications, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or on a cloud-backend.
For the model inference various things need to be considered, among other things preprocessing, computational demands,
specialized hardware (GPU, TPU, neuromorphic co-processors, FPGA),
network latencies and energy consumption. Most of these aspects depend on both the model as well as the environment where the model is deployed. 
In order to help with the optimal selection of cloud and edge inference to achieve real-time AI, this thesis proposes a performance model characterising the deployment of a deep learning model and the resulting trade-offs.
Based on this performance model, the most essential trade-offs of the different deployment options get demonstrated by conducting multiple experiments using image classification as a use case. After the evaluation of these experiments, recommendations for the deployment are proposed.




When deploying real-time AI applications on edge devices, a crucial aspect is the decision whether to deploy the underlying deep learning models on edge devices or a cloud-backend.
While edge computing is often characterized by a lack of computational power and therefore struggles to provide a high throughput of predictions at a low-latency, outsourcing the inference to a cloud-backend raises concerns regarding security and reliability.
Although low-latencies and throughput are the most critical inference performance metric for real-time applications on edge devices, other metrics like CPU usage, data-, memory- or energy-consumption are affected by both cloud and edge inference and hence vital for the model deployment to consider.

%All these performance metrics are mainly affected by four factors: The model architecture, hardware, inference framework and the shape of the inference input.
%Significant research is being done in all these areas influencing the deployment question, resulting in the release of optimized frameworks (TensorFlow Lite, NNAPI, TensorFlow Serving), hardware accelerators (GPU, TPU, neuromorphic co-processors, FPGA) and models dedicated for edge devices (MobileNet).
-----------------------------------------------------------------------------
\end{comment}




\noindent In the wake of the recent success of deep learning models, the number of possible real-time AI applications on edge devices has exploded.
However, the deployment of these model is often difficult as edge computing is often characterized by a lack of computational power and therefore struggles to provide high inference throughput at a low-latency rate.
Outsourcing the inference to a cloud-backend can provide more computational power, but raises concerns regarding security and reliability. 
Although low-latencies and throughput are the most critical inference performance metric for real-time applications on edge devices, other metrics like CPU usage, data-, memory- or energy-consumption are affected by both cloud and edge inference and hence vital for the model deployment to consider.

In order to help with the optimal selection of cloud and edge inference to achieve real-time AI, this thesis proposes a methodology for performance evaluation of deep learning inference.
By instantiating this methodology we get a better system understanding of deep learning inference including benchmarks of the above-mentioned performance metrics.

To generate these benchmarks we use image classification as use case, supported by real-world workloads and hardware components and frameworks.
The evaluation of these benchmarks show, that edge inference can outperform cloud inference, given the right circumstances.
These include optimization of deep learning model architectures, utilization of optimized inference frameworks and minimization of preprocessing.


Based on these benchmarks we then develop a performance model, predicting the optimal deployment solution for a given system configuration, using multiple linear regression.
Finally, we utilize the gained system understanding to create a generic decision model that supports the deployment process for deep learning inference.



%%TODO add results 
%%Results: edge better for small models, cloud for large models; large batch sizes not viable at the edge, impact of preprocessing
%Our results show that edge devices can deliver better inference performance than cloud inference would for small models, but for large models cloud inference is still the preferred option for high performance, at least for the moment.
%Another key lesson is the impact of preprocessing on inference performance. The amount of preprocessing should be minimized

%proposes a performance model characterising the deployment of deep learning models and the resulting performance trade-offs.
