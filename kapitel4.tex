\chapter{Experimentation}
\label{chap:experiments}
This chapter deals with the experiments, their design, execution and results.

 
\section{Experimental Design}
This section deals the specification of the Android benchmark app, used hardware, frameworks, models and how the measurements are conducted. 
Figure \ref{fig:expDesign} depicts a brief overview of the specifications of the experiments.
We use image classification as the use case, therefore an image needs to be sent to the model.
\begin{figure}[H]
\centering
\resizebox{.95\linewidth}{!}{\input{./Bilder/Exp_design.tex}}
\caption{Experimental Design Architecure}
\label{fig:expDesign}
\end{figure}


\subsection{Hardware Devices}
\subsubsection{Edge}
\label{chap:hardwareEdge}
As the edge device we will use the OnePlus 6T (ONEPLUS A6013). This state of the art smartphone is powered by a Qualcomm Snapdragon 845 CPU(Octa-core, up to 2.8 GHz), Adreno 630 GPU, 8 GB of memory and runs on OxygenOS 9.0.11, which is based on Android 9.
%%Cite from AI bechmark paper
\subsubsection{Cloud}
%The Nvidia DGX-1 will serve as the cloud-backend for the experiments. This server consists of 8$\times$Tesla V100 providing 1000 TFLOPS as well as 256 GB GPU memory and 512 GB system memory.
The virtual server has 32 cores (16 real cores with hyperthreading), 240 GB of memory, a Tesla P100 16 GB PCIe GPGPU and a 800 PCIe SSD.

The server runs on Ubuntu 16.04 CUDA 9.1 PGI 17.9 nvidia-docker 2.0.3+docker18.03.1-1.
\subsection{Deep Learning Inference Framework}
We use two open source machine learning frameworks based on TensorFlow for the experiments, TensorFlow Lite and TensorFlow Serving. We decided to use these framework because they are open source and support, at the point of this theses, the operations for most image classification models and also a increasing number of hardware accelerators and operating systems for both edge and cloud inference.
TensorFlow provides official releases, that are maintained and tested, of the models we like to evaluate
This section only gives a brief overview of the most important aspects of the frameworks, for detailed information consider the TensorFlow Lite\cite{tfLite}  and TensorFlow Serving\cite{tfServing} websites, on which this section is based on, or the corresponding GitHub repositories
We chose these frameworks since they both support TensorFlow models, ...%INSERT more reason here
\subsubsection{TensorFlow Lite}
\label{chap:TFLite}
TensorFlow Lite (Release 1.12.0) was developed for mobile and embedded devices and is a lightweight solution of TensorFlow and thus will be used for the edge inference experiments.

%%Mention NNAPI support?
At the moment only model inference can be done by TensorFlow Lite, not model training.
It supports acceleration with GPU or other accelerators as well was portability to Android, iOS and other IoT devices.

\paragraph{Android NNAPI}
\label{chap:NNAPI}
TensorFlow Lite is also compatible with the Android Neural Networks API (NNAPI). This API
is designed to speed up computationally intensive machine learning operations and can be used by TensorFlow Lite to improve inference performance. During inference NNAPI "can
efficiently distribute the computation workload across available on-device processors, including dedicated neural network chips, GPUs and DSPs"\cite{DBLP:journals/corr/abs-1810-01109}.



\paragraph{Hosting Models}
TensorFlow Lite expects models in their own FlatBuffer file  format(\emph{.tflite}). Therefore models need to be converted to this format before TensorFlow Lite can load them. This conversion can be done using the TensorFlow Lite Converter, which supports various formats of trained TensorFlow models.
The \emph{.tflite} model now can be loaded by a object of the Interpreter class.
\paragraph{Run Prediction}

To then run the inference process in TensorFlow Lite the run method of Interpreter object with a loaded model needs to be called. To call this function two objects need to be passed, first the input for the given model and second the output object, where the prediction response from the inference operation gets stored. 
\begin{figure}[H]
\centering
\input{./Bilder/edge.tex}
\caption{Functionality of TensorFlow Lite}
\label{fig:edge}
\end{figure}
\subsubsection{TensorFlow Serving}
\label{chap:TFServing}
As for cloud inference TensorFlow Serving (Release 1.12.0 VERIFY THIS AGAIN) will be used, since it provides a framework to serve machine learning models in production environments. 



\paragraph{Hosting Models}
In order to host a model as a Servable in TensorFlow Serving, first a TensorFlow model needs to exported using TensorFlow's SavedModelBuilder, resulting in a SavedModel protocol buffer file along with the model’s variables and assets (Although TensorFlow Serving is optimized for TensorFlow models, the framework can be extended to serve other types of models).
%%Wieviel schreiben über signature, predict function etc?

Now the exported model can be loaded by a instance of Tensorflow Serving.
We use docker to start that instance, specifically nvidia-docker that allows us to run the inference operations on a GPU. For that TensorFlow Serving provides two docker images of their framework, one with CPU and the other with GPU support.

\paragraph{Run Prediction}
TensorFlow Serving supports two API for clients to create predictions requests: gRPC and REST. Since the gRPC protocol is supposed to deliver a better performance, we will use the gRPC API.
For a client to a request to the server a gRPC stub needs to be created in the first place, that allows us to call all methods implemented on the server. In our case we need to call TensorFlow Serving's Predict method to start the inference process. The method needs to be passed a PredictRequest object, which contains among other things the input data for the model, the shape of the input and the requested model.%model signature?

After the request is sent and handled the server response by sending back a \emph{PredictResponse} object. This object holds the predictions for the given input data in the form specified by the exported model.
This request and response process can also be seen in figure \ref{fig:cloud}

\begin{figure}[H]
\centering
\input{./Bilder/cloud.tex}
\caption{Functionality of TensorFlow Serving}
\label{fig:cloud}
\end{figure}
\subsection{Models}
\label{chap:models}
We will use two different image classification models for the experiment, MobileNetV2 and InceptionV4, the first is optimised for mobile deployment and the second towards high accuracy.
Both models are trained on a ImageNet dataset consisting of 1001 image classes (1000 image classes + 1 class for other image classes).

For the TensorFlow Lite version we use the models provided on the TensorFlow Lite website \cite{tfLiteModels}.
To convert model suitable for TensorFlow Serving we use the TensorFlow-Slim library \cite{tfSlim}, where maintained and tested implementations of popular image classification models are being published. For both Serving and Lite we use the same training checkpoint to get the same weights for the graphs.

In the following we give a brief overview of the models, their unique building blocks and the intuition behind them.
For full details please refer to \cite{DBLP:journals/corr/abs-1801-04381} and \cite{InceptionV4}.

\subsubsection{MobileNetV2}
MobileNetV2 (version 1.0) is a successor of MobileNetV1 and is "specifically tailored for mobile and resource
constrained environments" \cite{DBLP:journals/corr/abs-1801-04381}. The authors do this by "significantly decreasing the number of operations and the memory needed while retaining the same accuracy"  \cite{DBLP:journals/corr/abs-1801-04381} and introducing a new layer module called "the
inverted residual with linear bottleneck".

This module is visualized in figure \ref{fig:bottleneckBlock} and consists of two parts: The inverted residual block and a shortcut from the input to the output.
In a first step the inverted residual block the channel dimension of the input get expanded with the use of a pointwise $1\times1$ convolution layer. 
Afterwards depthwise $3\times3$ convolution is applied to the expanded input. Then again a pointwise $1\times1$ convolution gets used, but this time the dimensions are decreased instead of increased.

The intuition behind this module is that the expansion decodes information ensuring that the features can be extracted during the depthwise convolution. The extracted features then get encoded again by reducing the dimensions.
To improve the backpropagation of the gradient across multiple layers during the training the authors add the shortcut to the module, resulting in faster training and better accuracy.
The bottleneck module can be implemented very memory efficient, thus particularly fit for edge inference.

Figure \ref{fig:MobileNetArchi} displays the design of the model, with the majority of the building blocks being the bottleneck modules.
All bottleneck modules in a sequence have the same number of output channels and use a stride of $1$, except for the first bottleneck block in a sequence. All spatial convolutions use $3\times3$ kernels. 
Although not depicted on the figure, the model uses dropout and batch normalization during training.
%%expanstionf actor?
%%$$shortcut!! expanded by a "expansion factor" varying for different seqences

\begin{figure}[!htb]
\centering
   \resizebox{.7\linewidth}{!}{\input{Bilder/MobileNet_bottleneck.tex}}
\caption{Inverted Residual Block (bottleneck)}
\label{fig:bottleneckBlock}
%%NO DROPOUT AT INFERENCE
\end{figure}


\begin{comment}


\begin{table}[]

\centering
\caption{MobilenetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
\label{table:mobilenetArchi}
\begin{tabular}{@{}llllll@{}}

\toprule
Input & Operator & t & c & n & s \\ \midrule
$224^2\times 3$ & conv2d & - & 32 & 1 & 2 \\
$112^2\times 32$ & bottleneck & 1 & 16 & 1 & 1 \\
$112^2\times 16$ & bottleneck & 6 & 24 & 2 & 2 \\
$56^2\times 24$ & bottleneck & 6 & 32 & 3 & 2 \\
$28^2\times 23$ & bottleneck & 6 & 64 & 4 & 2 \\
$14^2\times 64$ & bottleneck & 6 & 96 & 3 & 1 \\
$14^2\times 96$ & bottleneck & 6 & 160 & 3 & 2 \\
$7^2\times 160$ & bottleneck & 6 & 320 & 1 & 1 \\
$7^2\times 320$ & conv2d 1x1 & - & 1280 & 1 & 1 \\
$7^2\times 1280$ & avgpool 7x7 & - & - & 1 & - \\
$1\times 1\times 1280$ & conv2d 1x1 & - & k & - &  \\ \bottomrule
\end{tabular}
\end{table}
\end{comment}
\paragraph{Quantization}
Quantization of MobileNetV2, using the techniques presented in section \ref{chap:quant}, results in a  0.7\% loss of top-5 accuracy, but therefore has lost 75\% of its model size and a optimised performance, which we will evaluate in the experiments.



\subsubsection{Inception V4}
%%Add infos about stem and reduction
InceptionV4, published in \cite{InceptionV4}, is a large image classification network with high accuracy, but also with a high number of parameters leading to higher inference times than MobileNetV2.
In comparison to its previous versions InceptionV4 is built with "a more uniform simplified architecture and more inception modules". 

The general architecture of the network can be seen in figure \ref{fig:inceptionv4Archi} with the fundamental building blocks being varying Inception (A-C) and Reduction(A-B) modules. 
An example for both of these modules can be seen in figures \ref{fig:InceptionA} and \ref{fig:InceptionReduction}.
Inception modules are built of multiple convolutions with multiple filters and pooling layers in parallel within the same layer.
After the convolutions the output of all convolutions are concatenated.
The intuition behind this parallelism is to give the model multiple convolution choices for a given input and let the model learn itself  the best feature extractor. An additional benefit is that the model can extract both local and more complex feature from an input.

To reduce the dimensionality before computational expensive large convolutions, and thus speed up training, the authors introduce the Reduction block containing small $1\times1$.convolutions.   %genauer erklären




\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{.95\textwidth}
\centering
   \resizebox{.8\linewidth}{!}{\input{Bilder/InceptionAModule.tex}}
   \caption{Inception-A module}
   \label{fig:InceptionA} 
\end{subfigure}

\vspace{1em}
\begin{subfigure}[b]{.95\textwidth}
\centering
   %%Verify the architectures again
   \resizebox{.6\linewidth}{!}{\input{Bilder/InceptionReductionModule.tex}}
   \caption{Reduction module (convolutions with "V" are valid, rest is same padded )}
   \label{fig:InceptionReduction}
\end{subfigure}

\caption{Special modules used by InceptionV4}

\end{figure}




Besides dropout InceptionV4 also benefits from the use of batch normalization during training.
%\begin{figure}[]
%\centering
%\resizebox{.45\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
%\includegraphics[angle=90,width=0.94\textwidth]{./Bilder/inceptionV4_architecture.png}
%\caption{InceptionV4 architecture \cite{InceptionV4}}
%\label{fig:inceptionv4}
%\end{figure}

\begin{figure}[!htb]
\centering
\begin{subfigure}[t]{0.47\textwidth}
   \resizebox{.99\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
   \caption{InceptionV4 architecture \cite{InceptionV4}}
   \label{fig:inceptionv4Archi} 
\end{subfigure}%
\begin{subfigure}[t]{0.47\textwidth}
   %%Verify the architectures again
   \resizebox{.99\linewidth}{!}{\input{Bilder/MobileNetV2_archi.tex}}
   \caption{MobileNetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
   \label{fig:MobileNetArchi}
\end{subfigure}

\caption{Architectures of InceptionV4 and MobileNetV2}
%%NO DROPOUT AT INFERENCE
\end{figure}
\begin{table}[]
%CITE inception params ned vergessen
%http://dgschwend.github.io/netscope/#/preset/inceptionv4
\caption{Overview of used models}
\label{table:modelOverview}
\begin{tabular}{@{}lllll@{}}
\toprule
Model & Parameters & Top-5 Accuracy\cite{modelspecs} & Input Size & TF Lite Model Size \\
\midrule
InceptionV4 & 42.68M & 95.1\% & 299x299 & 107.7MB \\
MobileNetV2 1.0 & 3.47M\cite{DBLP:journals/corr/abs-1801-04381} & 90.6 & 224x224 & 14MB \\
MobileNetV2 1.0 & 3.47M\cite{DBLP:journals/corr/abs-1801-04381} & 89.9\% & 224x224 & 3.4MB\\
\bottomrule
\end{tabular}
\end{table}


\subsection{Android Benchmark Application}
To conduct the experiments for both edge and cloud inference we developed and implemented an Android benchmark application using Kotlin.
This application implements all function needed to preprocess real workload images and perform the inference on either the Android device itself or sent the image to a cloud-backend.
For both the preprocess and inference operation the application logs metrics such as inference latency, time of the experiments etc. and stores them to a \emph{CSV} file, which then can be used to evaluation purposes.

Both edge and cloud inference implementations are based on the example implementations in the respective GitHub repositories to ensure optimal performance. 


\begin{figure}[htb]
\centering
\includegraphics[width=0.97\textwidth]{./Bilder/FlowChart_App.png}
\caption{Flowchart of the Benchmark Application}
\label{fig:app}
\end{figure}
In figure \ref{fig:app} the work-flow to perform the inference is seen. First, a image needs to be selected. Afterwards the image classification model needs to be chosen. Now the user selects whether the inference should be performed on the cloud-backend or directly on the edge device, in this case a android phone. In the case of edge inference it needs to be decided if the NNAPI should be used by TensorFlow Lite. For cloud inference the preprocessing mode needs to be selected (edge/cloud). Now the preprocessing operations can be performed based on the previous selected options (Even for the case of cloud inference with cloud preprocessing some preprocessing needs to be done on the edge beforehand like building a \emph{PredictRequest} containing the request for the cloud-backend). Now, that the input is preprocessed, the actual inference is performed resulting in a prediction. For both preprocessing and inference the measurements mentioned in section \ref{chap:insta_measurements} are logged.

\begin{figure}[htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/UML.png}
\caption{UML class diagram of the benchmark Application}
\label{fig:UML}
\end{figure}
Figure \ref{fig:UML} depicts an UML class diagram of the application featuring the most important classes, functions and variables. 
The main class is called \emph{MainActivity} and implements all of the graphical aspects. 
The \emph{MainActivity} handles requests for both cloud and edge inference by delegating the requests to instances of the classes \emph{TFServingClient} and \emph{ImageClassifier}, respectively. Both of these classes also take care of the preprocessing steps.
The abstract class \emph{ImageClassifier} has two subclasses \emph{FloatClassifier} and \emph{QuantClassifier}. The first class runs the inference for floating point models and the second for quantized models.
This seperation is needed since these different model types require different input and output types.

The \emph{MainActivity} writes the collected measurements and the parameter configurations of a experiment to a instance of \emph{SingleExperiment}. After the experiment is completed \emph{MainActivity} tells a Logger instance to save the contents of the \emph{SingleExperiment} object. The Logger then saves all collected data to a \emph{CSV} file.

\subsubsection{Preprocessing}
%%mention png
For the case of image classification, the images need to have the correct size ($224\times224$ for MobilenetV2 and $299\times299$ for InceptionV4) and the RGB values need to be normalized to the interval $[-1,1]$. After preprocessing the image has been transformed into the shape $224\times224\times3$ with all values between $[-1,1]$, where the the first two dimensions represent the image height and width, while the last dimension represent the number of channels (3 since the images are rgb)
\begin{figure}[H]
\centering
\input{Bilder/Preprocessing.tex}
\caption{Preprocessing steps for image classifcation}
\label{fig:prepro}
\end{figure}
\paragraph{Edge Preprocessing}
In the case of edge preprocessing all preprocessing steps are done on the edge device, meaning that the input can be fed directly into the neural afterwards, either on the edge device itself or on a cloud-backend.
We perform these steps the following way: After loading the PNG image into an \emph{InputStream} we created a scaled Bitmap of the image (scaled to either $224\times224$ or $299\times299$) by calling \emph{createScaledBitmap}. 
Afterwards we normalize the RGB values to $[-1,1]$ during the conversion from the bitmap to a \emph{ByteBuffer}. 
For the case of float models these buffer contain all the pixels in float format and for quantized model in bytes.
%%%mehr erklären?
We do this conversion since feeding \emph{ByteBuffers} to TensorFlow Lite is performance enhancing %add cite here
For cloud inference we then construct \emph{PredictRequest} object containing this \emph{ByteBuffer}.

For batch size larger than one we parallize the preprocessing to speed up the proprocessing latency at the cost of higher maximum memory consumption. We start $n$ threads, where each thread is preprocessing one image in the way described above. After each image is preprocessed we concatenate all \emph{ByteBuffers} into a single one, which then can be fed to the TensorFlow Lite interpreter. Note that $n$ is determined by both batch size and available CPU cores on the edge device. There are never more threads than available cores, but if the batch size is smaller than the number of cores, we only start $n$ threads, where $n$ is the batch size. 

\paragraph{Cloud Preprocessing}

In the case of cloud inference the images can also be preprocessed directly on the cloud, resulting in nearly no preprocessing done on the edge. While the resizing and scaling steps are no longer done on the edge, the image still needs to converted into a \emph{PredictRequest} object that TensorFlow Serving can handle.
To achieve this we again load the PNG image into an \emph{InputStream}, convert it to a \emph{ByteArray} which then can be feed to the \emph{PredictRequest} object as a \emph{ByteString}. 

%Add TensorFlow Serving preprocessing here


\subsubsection{Inference}
To get the predictions for our now preprocessed image we need to feed it into the deep learning model, which is loaded either directly on the edge or on the remote cloud-backend. 
The inference is done using the steps described in section \ref{chap:TFLite} (TensorFlow Lite) and \ref{chap:TFServing} (TensorFlow Serving).
\paragraph{Edge Inference}
To perform the inference operation we two things into the run function of an interpreter of TensorFlow Lite: The \emph{ByteBuffer} created in the preprocessing step and an array (float models: \emph{FloatArray}, quantized models: \emph{ByteArray}) with the length 1001 (number of classes). TensorFlow lite then writes the confidence levels of the different classes to this array. We then sort the array for the five classes with the highest confidence and print them to the screen.

\paragraph{Cloud Inference}

For the cloud inference we send the \emph{PredictRequest} object created in the preprocessing process to the TensorFlow Serving server, where the inference (and in the case of cloud preprocessing also the preprocessing) computations are executed. Afterwards the client receives the \emph{PredictResponse} from the server containing the predictions for the sent image. We configured the TensorFlow Serving models to return the five classes with the highest confidence, hence we extract these five classes from the \emph{PredictResponse} and print them to the screen.

To preprocess the image on the cloud we add TensorFlows preprocessing functions to the model graphs before we export them to the TensorFlow Serving format. The images arrive at the serve in the PNG format, therefore we first decode them with \emph{tf.image.decode\_jpeg}, then resize with \emph{tf.image.resize\_bilinear} and finally normalize the tensor values to $[-1,1]$ using the the \emph{subtract} and \emph{multiply} functions. Now the input has the same shape as if they would have been preprocessed on the edge. Now the input can be fed to the actual model graphs.
\section{Instantiation}
Using the experimental design presented in the previous section, we now describe which parameters we change in the course of the experiments as well as how the performance metrics are measured.
\subsection{Parameters}
We run each parameter configuration 20 times to reduce variance. During the experiments no other applications are running on either the edge or the cloud device.
In the course of the experiments we change the configurations of the following parameters:
%%%293,performance analysis buch
\paragraph{Model}
We conduct experiments for all three models listed presented in \ref{chap:models}: InceptionV4, MobileNetV2 and MobileNetV2 quantized.

\paragraph{Preprocessing Mode}
For the case of cloud inference the major parts of the needed preprocessing is either done on the edge before sending the image to the cloud or done on cloud. Therefore we evaluate both options.
\paragraph{Inference Mode}
The inference can either be performed on the edge or an a cloud-backend.

\paragraph{Image Size}
%%Add table here?
%224: 83KB
%299: 141KB
%2MP: 2411KB
%4MP: 4309KB
%8MP: 7515KB
%16MP: 10077KB
We evaluate the performance of 2MP($1732\times1155$, $2411$KB), 4MP($2449\times1633$, $4309$KB), 8MP($3464\times2309$, $7515$KB) and 16MP($4899\times3266$, $10077$KB) PNG images. This way the effect of different image sizes on the performance of the preprocessing step can assessed. We also evaluate an image where no resizing is needed ($224\times224$, $83$KB or $299\times299$, $141$KB depending on the model) to study the impact of image resizing. A picture of a cat (see figure \ref{fig:cat}) scaled to the different sizes will serve as the picture for the experiments.
\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{./Bilder/European_cat_02_16_mp.jpg}
\caption{Picture used for the experiments \cite{cat}}
\label{fig:cat}
\end{figure}
\paragraph{Batch Size}
%%mathe defs of throughput?
Batch Size denotes the number of images fed into the deep learning model in a single inference operation. 
Feeding more than one image to the network can lead to a higher throughput, if enough computational power is available. This increase in throughput often comes at the cost of increased latency and general resource consumption.

To study these trade-offs we conduct experiments with the following batch sizes: 1, 2, 16, 32. Since at the point of these experiments a batch size greater than one is not supported for the TensorFlow Lite versions of MobilenetV2 (both float and quantized), measurements with these batch sizes cannot be performed for the case of edge inference at this point.
\paragraph{NNAPI}
The Android Neural Network API (NNAPI), presented in section \ref{chap:NNAPI}, is supposed to enhance the inference performance of TensorFlow Lite. Therefore we take a look into the effect of this framework.
%%Mention GPU use here
\paragraph{GPU Usage}
Since January 16 an experimental release of TensorFlow Lite supporting GPU usage on Android using OpenGL ES 3.1 Compute Shaders \cite{tfLiteGPU}.
So far only four public models and their operators are supported, including MobileNetV2, but not InceptionV4. 
Therefore we omit the test of this new feature in our experimentation, but our initial testing indicated that the use of NNAPI provides faster inference latencies for MobileNetV2 than the GPU version.
%%https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7
%experimental
%only mobilenet supported
%worse performance than NNAPI so far


\begin{figure}[H]
\centering
 \scalebox{.7}{\input{./Bilder/tree.tex}}

\caption{Excerpt of the performed experiments configurations}
\label{fig:tree}
\end{figure}
Figure \ref{fig:tree} shows a part of our experimentation tree. In theory there would be 240 different parameter configurations, but due to a number of reasons the number of different configurations reduced to 113 in total.

First, the Usage of NNAPI is independent of the image size hence we only perform experiments without NNAPI for a single image size. Also the usage of NNAPI leads to a way better performance for edge inference and we want to compare edge and cloud inference, so we only carry out experiments without NNAPI for a batch size of one.
We omit the cloud inference for the quantized MobileNetV2 because the model is so small the network overhead would be overwhelming.%INSERT REASON.

%of limited capacity/time and
Lastly due to lack of support certain operations in TensorFlow Lite for the MobileNetV2 models, we can not perform inference experiment for MobileNetV2 with a batch size larger than one.
\subsection{Performance Metrics}
\label{chap:insta_measurements}
In the following section we describe how we measure the performance metrics defined in \ref{chap:metrics}.
We conduct the measurements either directly in the source code or by using Android Studio Profiler (Version 3.3). Since Android Studio cannot collect all metrics the way we wanted, we wanted to used Trepn as a secondary profiling application. But since Trepn does not support our test device, we had to omit these metrics.

In the following definitions $t_{start}$ denotes the starting point of either the preprocessing or inference, while $t_{end}$ denotes the end point of the respective process.
\subsubsection{Latency}
We measure $Latency_{preprocessing}$ by measuring the time difference between start and end of the preprocessing process.
Note that all latency measurements are reported in milliseconds(ms).
\begin{equation*}
\begin{gathered}
Latency_{preprocessing} = t_{endPreprocessing} - t_{startPreprocessing}
\end{gathered}
\end{equation*}
To measure inference latency we need to distinguish between edge and cloud inference, since for the latter the network latency needs to be considered.

\paragraph{Edge Inference}To measure edge inference latency we measure the time the TensorFlow Lite interpreter needs to run the inference operation on the loaded model given the input image.
\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{endInference} - t_{startInference}
\end{gathered}
\end{equation*}
\paragraph{Cloud Inference}
To measure the cloud inference latency we need to measure two latencies, which combined yield $Latency_{inference}$. The first latency is  $Latency_{server}$. This server latency describes the time difference between the point where TensorFlow Serving receives the inference request and the point in time where TensorFlow Serving sends the response back to the client.
The second latency $Latency_{network}$ denotes the time the prediction request needs to reach the cloud-backend.
These latencies are illustrated in figure \ref{fig:serverLat}.
\begin{figure}[!htb]
\centering
\input{./Bilder/server_lat.tex}
\caption{Measurement of $Latency_{server}$ and $Latency_{network}$ for Cloud Inference}
\label{fig:serverLat}
\end{figure}


Similar to the edge inference, $Latency_{inference}$ is being measured by calculating the time difference between starting the inference process and receiving the prediction for the given inference request. This whole process is covered by the \emph{predict} function of TensorFlow Serving. Therefore we measure the wall clock time of this function.

Since TensorFlow Serving does not output the server latency, we needed to tweak the source code of gRPC, which is the underlying protocol of TensorFlow Serving. gRPC already logs this latency, so we adjust the source code to output this latency when a call to TensorFlow Serving is finished, repackage the source code and change to dependencies of TensorFlow Serving pointing to the adjusted packages.

We then calculate $Latency_{network}$ implicitly by subtracting $Latency_{server}$ from $Latency_{inference}$.

\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{end} - t_{start}\\
Latency_{server}= t_{receive Request} - t_{send Response}\\
Latency_{network} = Latency_{inference} - Latency_{server}
\end{gathered}
\end{equation*}


The total latency $Latency_{total}$ for both edge and cloud inference is simply calculated by summing up the latencies of both preprocessing and inference.
\begin{equation*}
\begin{gathered}
Latency_{total} = Latency_{preprocessing} + Latency_{inference}
\end{gathered}
\end{equation*}
\subsubsection{Energy Consumption}
Since Android Studio Profiler only estimates the energy consumption in the form of low, medium and high, the tool is not fit to provide empiric measurements. The Trepn Power Profiler would provide such measurements, but does not support the device used for the experiments (OnePlus 6T).
\subsubsection{CPU Usage}
We measure the CPU usage(\%) of both preprocessing as the maximum CPU usage during the respective processes.
\begin{equation*}
\begin{gathered}
%%CPU_{preprocessing} = max_{[start_{preprocessing}, end_{preprocessing}]}(CPU)\\
%%CPU_{inference} = max_{[start_{inference}, end_{inference}]}(CPU)\\
CPU_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} CPU_{Usage}(t)\\
CPU_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} CPU_{Usage}(t)
\end{gathered}
\end{equation*}
Android Studios’ CPU Profiler allows us to record the maximum CPU usage for both preprocessing and inference. To minimize the impact on the Android Profiler on the performance of the application we disable allocation tracking.
\begin{figure}[H]
\centering  
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_CPU}
\caption{Android CPU Profiler}
\label{fig:prof_cpu}
\end{figure}
\subsubsection{Memory Usage}
We measure the memory usage by recording the maximum memory consumption during both processing and inference. Both of the following metrics are reported in megabytes(MB) in the result section.
\begin{equation*}
\begin{gathered}
Memory_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} Memory(t)\\\\
Memory_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} Memory(t)\\
\end{gathered}
\end{equation*}
The Memory Profiler is part of Android Studio Profiler and shows the memory consumption of the app it is profiling. Memory allocations by the operating system or other apps are not recorded. Besides recording the total amount of memory allocated the Profiler also tracks the different categories, for example memory allocated by Java/Kotlin code. We always record the maximum consumed memory for each operation.
%%memory not accurate over 1GB

Figure \ref{fig:prof_mem} depicts an example of the Memory Profiler for a single experiment. The first peak in memory consumption is the preprocessing step, while the second peak is caused by the inference process.
The little trash can at the bottom of the figure shows that the garbage collection was called. The garbage collection was always manually called after the preprocessing in case not all unneeded memory allocations are collected before running the inference operation. 
Note that we report total memory consumption of the application, including memory consumed for the graphical interface or the \emph{Logger} class.
%In the case of preprocessing only the preprocessed image is needed, so the 
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_MEM}
\caption{Android Memory Profiler}
\label{fig:prof_mem}
\end{figure}
\subsubsection{GPU Usage}
Neither Android Studio nor Trepn can provide GPU metrics for the OnePlus 6T. Hence no GPU measurements can be conducted.
\subsubsection{Throughput}
We differentiate three types of throughput: the inference throughput, preprocessing throughput and the total throughput, which also includes preprocessing besides inference.
We calculate throughput in operations per second by 

\begin{equation*}
\begin{gathered}
Throughput_{preprocessing} =\frac{1000}{(Latency_{preprocessing}) / batchsize}\\
Throughput_{inference} =\frac{1000}{(Latency_{inference}) / batchsize}\\
Throughput_{total}  =\frac{1000}{(Latency_{total}) / batchsize}
\end{gathered}
\end{equation*}
\subsubsection{Data Consumption}
We measure both transmitted and received data by using the Android TrafficStats package (https://developer.android.com/reference/android/net/TrafficStats). We start measuring both transmitted and received bytes when the inference operation is started and stop when the response from the server is returned. We report both $Data_{transmitted}$ and $Data_{received}$ in kilobytes(KB).
\begin{equation*}
\begin{gathered}
Data_{transmitted} = \sum_{t_{startInference}}^{t_{endInference}} Data_{transmitted}(t)\\
Data_{received} = \sum_{t_{startInference}}^{t_{endInference}} Data_{received}(t)
\end{gathered}
\end{equation*}
\section{Results and Evaluation}
The previous section covered the experimental design, including used models, hardware devices and framework, as well as definition of performance metrics. Using this design specifications, we will now present and evaluate the results of the conducted experiments.

First, we take a look into the individual results of edge and cloud inference and afterwards compare them against each other. In each of these sections first the preprocessing and then the inference results are presented. 
In the last subsection we take a look at batch sizes larger than one.

%die 83mb aufschlüsseln in GUI etc 
%Logger verbrauch erwähnen
Note that the Android application consumed around 83MB in memory and 0\% CPU during idle.

For precise mean values and standard deviations of all measured metrics refer to tables \ref{measurementsInception} and \ref{measurementsMobilenet}.

The black bar on top of each bar of the following bar chart and translucent areas around the lines of the line plots represent the standard deviation.

Preprocessing is performed without any models loaded on the edge. but with the image loaded into memory before the start of preprocessing to prevent the I/O operations to influence the preprocessing performance.
Edge Inference is done using a loaded model. We do not consider model loading latency in this thesis.

All experiments are done using the eduroam Wi‑Fi, since the cloud-backend server is only reachable within the Münchner Wissenschaftsnetz (MWN) and a VPN would have an impact on network latencies.

We conducted the experiments in multiple sessions, where each session consists of $100$ experiments distributed over about two hours, thus preventing the results to be skewed by thermal throttling.
\subsection{Edge Inference}
This section covers the results of the edge inference experiments with OnePlus 6T device presented in section \ref{chap:hardwareEdge}.

Note that this section only covers the results for a batch size of one, for the results of larger batch sizes please refer to section \ref{chap:resultsBatchSize}.

\FloatBarrier
\subsubsection{Preprocessing}
\label{chap:edgePrepro}
Figure \ref{fig:EdgePrepro} shows the $Latency_{preprocessing}$ and $Memory_{preprocessing}$ for the different image sizes and models.

There is little difference in both latency and memory for the different models. 
InceptionV4 uses more memory than the MobileNetV2 models, which is caused by the models higher input size (InceptionV4 $299\times299$, MobileNetV2 $224\times224$). 
These small differences are expected, since both all three model require roughly the same input specifications, with the exception of input size, but the difference between the two input sizes is relatively small.

While the different models have little difference in memory and latency, the image size has an significant impact on both of these metrics.
A $16$ megapixel image takes on average across all models more than $10$ times as long to preprocess as well as using $1.5$ times more memory in comparison to a $224^2/299^2$ image.
Figure \ref{fig:EdgePrepro} shows a steady increase of $Latency_{preprocessing}$ and $Memory_{preprocessing}$ across the increasing image sizes.
Preprocessing of large images is therefore mainly affected by the resizing process, not the pixel normalization or conversion to a \emph{ByteBuffer}, since these steps are performed after the resizing, thus having the same impact on performance as they would have on an $224^2/299^2$ image.

The standard deviation of both memory and latency are both low, indicating a stable resource consumption.


%%factors reinbringen auch kb größe der bilder
%%mehr impact on latency than memory

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Preprocessing.pdf}
\caption[Edge Inference - Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$]{Edge Inference - Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$ -  lower is better: Increasing image sizes have increasing impact on both memory and latency, independent of model type}
\label{fig:EdgePrepro}
\end{figure}
%%Was über CPU hier sagen
%%CPU usage in obere grafik integrieren?
Looking the CPU usages during preprocessing (see figure \ref{fig:CloudEdgePreproCPU}) one can see that the usages are very similar across all models and image sizes. This is probably due to the fact that the preprocessing of a single image is done on a single core. Since the OnePlus 6T has $8$ cores, the maximal CPU usage cause by a single core is $12.5\%$, which also is displayed in the plot, where all usages are close to this number.
%%High variance
%%around?

\FloatBarrier
\subsubsection{Inference}
This section presents the results for edge inference, in particular the effect of the Android Neural Network API.
Different image sizes have no effect on edge inference, as image are always preprocessed when they reach the inference step.
Hence we do not consider the different image sizes in the majority of this section.


\begin{comment}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Inference.pdf}
\caption{Edge Inference - Inference}
\label{fig:EdgeInference}
\end{figure}


\end{comment}
\subsubsection{Effect of NNAPI}
%%effcient
The Android Neural Network API (NNAPI) is supposed to speed up inference of neural network on Android by introducing optimised kernels/operators. 
The effect of this framework can be seen in figure \ref{fig:NNAPI} and show the significant performance improvement caused by the NNAPI, not only affecting $Latency_{inference}$, but also $Memory_{inference}$ and $CPU_{inference}$.

%%Genauere factors einfügen : inferecen zweimal so schnell...
This effect can be observed across all tested models, but especially on the InceptionV4 network.

Note that the NNAPI uses the GPU of the OnePlus 6T for a part of its inference, therefore while reducing the CPU usage during inference, the NNAPI probably causes higher GPU usages.

Since the NNAPI leads to performance improvements in all measured metrics, it will be used for all further edge inference results.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{./Bilder/single_plots/edge_inference_plots/NNAPI_behavior.pdf}
\caption[Edge Inference - Effect of NNAPI on Inference]{Edge Inference - Effect of NNAPI on Inference - lower is better: NNAPI has a significant positive effect on the inference metrics latency, memory and CPU usage.}
\label{fig:NNAPI}
\end{figure}

%, InceptionV4, MobileNetV2 and the quantized version of MobileNetV2,
When comparing the different models with NNAPI enabled it can be stated that there a significant performance differences between these models.
%%MobileNet und Inception vergleichen
%begründen: effect of quant mit paper vergleichen
%unterschiede mobileNet inception vergleichen
%%danahc MobileNet mit quantized
On average, InceptionV4 inference causes higher inference latencies of factor $8$ and consumes twice as much memory as MobileNetV2.
This contrast in performance is as expected, since MobileNetV2 architecture only contains $8\%$ of InceptionV4's parameter number (see table \ref{table:modelOverview}).
This bigger architecture as well as bigger input size lead to higher memory demands and latencies, thus explaining the performance differences.

A similar difference in inference latency can be seen when comparing MobileNetV2 against its quantized version, where the quantized version is more than $2$ times faster, confirming the study results of \cite{Quantizing} presented in section \ref{chap:quant}.
While there is a difference in latency between the quantized and non quantized versions of MobileNetV2, there is nearly no discrepancy in memory consumption.



While having big impact on both latency and memory, the different models have 
negligible CPU usage differences, but probably not on GPU usage, which we can not report in this thesis.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%Edge Inference Prepro Vs Inference%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure \ref{fig:EdgeInferenceRatio} depicts the latency ratio between preprocessing and inference.
As image sizes increases, the preprocessing becomes more and more the bottleneck, especially for the MobileNetV2 models.
For the MobileNetV2 even $224\times224$ images take longer to preprocess than to perform the inference on them.
%The Inception models are far more computational intensive, hence the longer inference latencies, but still 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Preprocessing_+_Inference.pdf}
\caption[Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$]{Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$ - lower is better: Preprocessing becomes the decisive factor for $Latency_{total}$ of both models for larger image sizes.}
\label{fig:EdgeInferenceRatio}
\end{figure}


\paragraph{Edge Inference - Key Takeaways}
\emph{
\begin{itemize}
    \item Preprocessing becomes bottleneck for larger image sizes.\\
          $16$ MP image in comparison to a $224^2/299^2$ image needs
    \begin{itemize}
        \item $10\times$ slower $Latency_{preprocessing}$
        \item $1.5\times$ more $Memory_{preprocessing}$
    \end{itemize}
    \item NNAPI leads to way better performance across all models:
    \begin{itemize}
        \item $4\times$ faster $Latency_{inference}$
        \item $1.2\times$ less $Memory_{inference}$
        \item $3\times$ less $CPU_{inference}$
    \end{itemize}
    \item In comparison to InceptionV4 MobileNetV2 leads to:
    \begin{itemize}
        \item $8.6\times$ faster $Latency_{inference}$
        \item $2.2\times$ more $Memory_{inference}$
    \end{itemize}
    \item Quantization of MobileNetV2 results in $2.3$x faster $Latency_{inference}$
    \item Preprocessing affects latency more than inference across all images sizes for small networks.
\end{itemize}}

\FloatBarrier
\subsection{Cloud Inference}
This section deals with the results of the cloud inference experiments and their evaluation, divided into preprocessing and inference.
Note that this section only covers the results for a batch size of one, for the results of larger batch sizes please refer to section \ref{chap:resultsBatchSize}.
We are evaluating $Latency_{inference}$ for the most cloud inference results, thus including the network component $Latency_{network}$, because this factor would also be present in real world AI applications. In our case we have a high speed connection with low network latency, thus being a lower bound for real-time AI applications.
%%add network proof (ping and upload)
\subsubsection{Preprocessing}
Cloud Inference allows two preprocessing methods, either on the edge beforehand or directly on the cloud.
This section present the results of these two methods, especially their impact on the resource consumption of edge devices.

Figures \ref{fig:cloudInferencePreproLat} and \ref{fig:cloudInferencePreproMemory} display the effect of preprocessing on either edge or cloud on the preprocessing latencies and memory consumption on the edge in respect to the different deep learning models and image sizes.

For Edge preprocessing, $Memory_{preprocessing}$ and $Latency_{preprocessing}$ are heavily affected by rising image sizes, but not by the different models and their different image input sizes.
Since preprocessing on the edge is very similar as in the edge inference case, please refer to section \ref{chap:edgePrepro} for full details on the edge preprocessing results.

Preprocessing on the cloud leads to an significant decrease in $Latency_{preprocessing}$, which is expected since nearly no preprocessing steps are done on the edge except building a \emph{PredictRequest} object for TensorFlow Serving.
Preprocessing an $224^2/299^2$ image on the edge instead of the cloud causes $20$ times higher $Latency_{preprocessing}$ and an increase of factor $10$ for an $16MP$ image.
The impact of larger image sizes on memory in the case of cloud preprocessing is marginal, especially in comparison for the edge preprocessing counterparts. This is expected, since only compressed \emph{PNG} images are loaded into memory, in contrary to edge preprocessing, where in addition to the decoded \emph{PNG} images the resized images are also loaded to memory simultaneously.
While $Memory_{preprocessing}$ is lower for cloud preprocessing, the difference is nowhere as substantial as the latency differences. $16MP$ images need $1.28$ times more memory if preprocessing on the edge instead on the cloud.
%%Mention CPU

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Latency.pdf}
\caption{Cloud Inference -  $Latency_{preprocessing}$}
\label{fig:cloudInferencePreproLat}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Memory.pdf}
\caption{Cloud Inference -  $Memory_{preprocessing}$ - lower is better:}
\label{fig:cloudInferencePreproMemory}
\end{figure}

\FloatBarrier
\subsubsection{Inference}
This section focuses on performance differences between the models and the impact of cloud preprocessing on cloud inference.
The inference metrics for cloud preprocessing include both preprocessing and inference.

%%Also includes preprocessing!!!!

Looking at the memory consumption during inference in figure \ref{fig:cloudInferenceInferenceMemory} one can see that $Memory_{inference}$ is very stable across all image sizes for edge preprocessing, which logical, since all image have been preprocessed to the same shape ($224^2/299^2$), therefore all requests sent to TensorFlow Serving have the same size.
MobileNetV2 uses $10.5$MB less memory on average than InceptionV4, because of its smaller model input size.

For cloud preprocessing the memory consumption increases for increasing image sizes, since larger images are being sent to the server, thus larger images are loaded into the \emph{PredictRequest} object, that is being sent to TensorFlow Serving.
There are no $Memory_{inference}$  differences between the two models, since unpreprocessed images are being sent, except for the $224^2/299^2$ images, where the smaller $224^2$ image consumes $7.5$MB less memory on average.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Memory.pdf}
\caption{Cloud Inference -  $Memory_{inference}$}
\label{fig:cloudInferenceInferenceMemory}
\end{figure}
The $Latency_{inference}$ results can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
Like for the memory, the latency for edge preprocessing stays the same across all image sizes, since all image sizes are preprocessed on the edge beforehand. 
Mean latencies are $181.2$ms and $108.46$ms for InceptionV4 and MobileNetV2 respectively, therefore MobileNetV2 is $1.7$ times faster.
Figure \ref{fig:CloudInferenceRatioEdgetotal} displays the share of $Latency_{network}$ and $Latency_{server}$ in $Latency_{inference}$ and one can see that the network is accountable for about $20\%$ of the  inference latency for InceptionV4 and $30\%$ for MobileNetV2.

If the images are preprocessed on the cloud, the preprocessing step on the cloud has an significant impact on $Latency_{inference}$, as can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
While an $224^2/299^2$ image has an latency of $95.8/64.5$ms for InceptionV4/MobileNetV2, a $16MP$ image needs $21.9/17.8$ times longer with a $1702.3/1414.8$ms mean latency.
\textbf{This increase applies to all image sizes in a linear proportion.}
Looking at  the share of $Latency_{network}$ in $Latency_{inference}$ in figure \ref{fig:CloudInferenceratioCloudtotal}, the network takes up to $50\%$ of the inference latency for $224^2/299^2$ images, but shrinks to less than $15\%$ for all remaining image sizes.
Therefore it can be stated that the network is not reason for the increase in latency for the larger image sizes, but rather the preprocessing done on the cloud-backend by TensorFlow Serving.
We think Tensorflow's \emph{resize\_bilinear} function, which we use to resize the images, causes the bottleneck in the preprocessing step on the cloud.%%add GPU kernel optimisations issues
%(enter source here)
The use of a other resize approach like nearest-neighbor could speed up preprocessing, but would probably have an impact on the accuracy of the predictions.
While the network connection in our experimentation environment is fast enough too prevent any bottlenecks caused by the network, a slower network connection could slow down inference for large image sizes.
%warum preprocessing s langsam bei tensorflow serving?

%%cloud preprocessing bigger images higher variance
Comparing the two cloud inference options, edge and cloud preprocessing,  $Latency_{inference}$ values for edge preprocessing of both models are faster for all image sizes except the $224^2/299^2$ images.
We believe the $224^2/299^2$ images are faster because of the lower I/O overhead in comparison to the larger preprocessed counter parts, even if the unpreprocessed images still have the decoded and normalized.


This stays also true for $Latency_{total}$ (see figures \ref{fig:CloudInference+PreproCloud} and \ref{fig:CloudInference+PreproEdge}), which includes both $Latency_{preprocessing}$ and $Latency_{inference}$. 
For $224^2/299^2$ images need $223.1/147.2$ms $Latency_{total}$ for InceptionV4/MobileNetV2 for edge preprocessing, while cloud preprocessing needs up $96.9/64.3$ms, therefore cloud preprocessing being $2.3$ times faster.
In contrast cloud inference with edge preprocessing is faster by a factor of $2.7$ for $16MP$ images, with the latencies of InceptionV4/MobileNetV2 being $626.7/532$ms for edge preprocessing and $1673.4/1453.9$ms for cloud preprocessing. $2MP$ images are $1.7/1.8$ times faster when using edge preprocessing, $4MP$ $2.1/2.2$ and $8MP$ $2.6/3$ times faster.
%latency edge vs cloud prepro inference:
While MobileNetV2 is $1.75$ times faster in $Latency_{inference}$ for edge preprocessing than InceptionV4, this latency difference shrinks for cloud preprocessing for larger images, started by a difference of factor $1.52$ for $224^2/299^2$ images and shrinking to $1.25/1.21/1.09/1.15$ for the respective image sizes $2/4/8/16MP$.
We believe this decrease is due to the fact as images for MobileNetV2 need to be resized to a smaller input size than InceptionV4, hence increasing the already high resize overhead and thus narrowing the latency difference between both models.
When comparing memory consumption of cloud inference with ether cloud preprocessing or edge preprocessing, the difference is only $3\%$ for  $224^2/299^2$ images ($121.5$MB for edge preprocessing and $118.1$Mb for cloud preprocessing), but $14\%$ for $16MP$ images (edge preprocessing $120.0$Mb, cloud preprocessing $136.9$MB).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Latency.pdf}
\caption{Cloud Inference -  $Latency_{inference}$}
\label{fig:cloudInferenceInferenceLatency}
\end{figure}



\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_cloudprepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInferenceratioCloudtotal} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_edgeprepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInferenceRatioEdgetotal}
\end{subfigure}

\caption{Cloud Inference -  $Latency_{inference}$ including $Latency_{network}$ and $Latency_{server}$}
\end{figure}

\begin{comment}


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_ratio_server_total_latency_(cloud_prepro).pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInferenceratioCloudrel} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_ratio_server_total_latency_(edge_prepro).pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInferenceRatioEdgerel}
\end{subfigure}

\caption{Cloud Inference -  Ratio between $Latency_{network}$ and $Latency_{server}$}
\end{figure}
\end{comment}


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_cloud_prepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInference+PreproCloud} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_edge_prepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInference+PreproEdge}
\end{subfigure}

\caption{Cloud Inference -  $Latency_{preprocessing}$ and $Latency_{inference}$ combined}
\end{figure}

Figures \ref{fig:CloudInferenceReceivedData} and \ref{fig:CloudInferenceTransmittedData} show the data transmitted and received from the edge client to the cloud-backend server for the different image sizes and preprocessing options.
For edge processed image the mean transmitted data is $1061.9/608.7$KB for InceptionV4/MobileNetV2 and $16.7/10.3$KB data received by the respective models.
Therefore $299^2$ images require $453.2$ kilobytes more than $224^2$ images.
For cloud preprocessing $Data_{transmitted}$ the amount of data sent is at maximum $100$KB larger than the sizes of the \emph{PNG} images ($224^2$: $83$KB, $299^2$: $141$KB, $2MP$: $2411$KB, $4MP$: $4309$KB, $8MP$: $7515$KB,  $16MP$: $10077$KB).
For both edge and cloud preprocessing the $Data_{received}$ rises for rising $Data_{transmitted}$ values, since the communication between server and client is done via TCP, thus more sent data results in more packages resulting in more ACK signals getting sent back to the client.
%%NR:[114.5] transmitted
%%NR:[3.] received
%%2MP:[2446.2] transmitted
%%2MP:[38.2] received
%%4MP:[4365.5] transmitted
%%4MP:[64.8] received
%%8MP:[7603.6] transmitted
%%8MP:[114.9] received
%%16MP:[10176.8] transmitted
%%16MP:[141.4] received
%299 png kleiner als 299 preporcessing
%%mit echter bildgröße abgleichen
%2MP($1732\times1155$, $2411$KB), 4MP($2449\times1633$, $4309$KB), 8MP($3464\times2309$, $7515$KB) and 16MP($4899\times3266$, $10077$KB) ($224\times224$, $83$KB or $299\times299$, $141$KB


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Received_Data.pdf}
   \caption{$Data_{received}$}
   \label{fig:CloudInferenceReceivedData} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Transmitted_Data.pdf}
   \caption{$Data_{transmitted}$}
   \label{fig:CloudInferenceTransmittedData}
\end{subfigure}

\caption{Cloud Inference -  $Data_{received}$ vs. $Data_{transmitted}$}
\end{figure}

\FloatBarrier
\paragraph{Cloud Inference - Key Takeaways}
%cloud prepro less preprocessing memory and latency
%cloud prepro more inference memory
%cloud preprocessing very slow for large image sizes
%edge prepro faster for all image sizes except 224/299
%network not a bottlneck
%resize binlinear bad implementation?
\emph{
\begin{itemize}
    \item Preprocessing on the cloud in comparison to edge preprocessing causes:
    \begin{itemize}
        \item $20\times$ faster $Latency_{preprocessing}$ for $224^2/299^2$ images
        \item $10\times$ faster $Latency_{preprocessing}$ for $16$MP images
        \item $1.28\times$ less $Memory_{preprocessing}$ for $16$MP images
    \end{itemize}
    \item Inference on the cloud including preprocessing in comparison to cloud inference without preprocessing causes:
    \begin{itemize}
        \item $2.3\times$ faster InceptionV4/MobileNetV2 $Latency_{total}$ for $224^2/299^2$ images
        \item $2.7\times$ slower InceptionV4/MobileNetV2 $Latency_{total}$ for $16MP$ images
        \item $0.97\times$ less $Memory_{inference}$ for $224^2/299^2$ images
        \item $1.14\times$ more $Memory_{inference}$ for $16MP$ images
    \end{itemize}
    \item MobileNetV2 is up to $75\%$ faster than InceptionV4 for smaller images, but shrinking to around $10\%$ faster for large image in case of cloud preprocessing.
    \item Network is accountable for up to $50\%$ of $Latency_{inference}$ for $224^2/299^2$ images and networks, but less than $30\%$ for larger images and network for both cloud and edge preprocessing.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Edge Preprocessing has faster $Latency_{total}$ latencies for all image sizes except $224^2/299^2$.
\end{itemize}
}%emph end


\subsection{Edge vs. Cloud Inference}
Previous two sections presented the results for both edge and cloud inference.
Now, the results of the cloud and the edge inference with batch size one, including preprocessing, are compared against each other.

Each plot of this section contains five subplots, one for each image sizes, the x-axis contains the deep learning model, while the y-axis displays the various performance metrics. 
The legend used in all those plots in explained in table \ref{table:legendPlots}.
\begin{table}[H]
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}
\centering
\caption{Explanation of the plot legends}
\label{table:legendPlots}
\begin{tabular}{@{}lll@{}}
\toprule
Inference on & Description & Color \\ \midrule
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:CLOUD\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ cloud-backend.\end{tabular} &  \crule[gruen]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference is done on cloud-backend, but images are \\ preprocessed on edge beforehand.\end{tabular} &  \crule[orangedunkel]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:EDGE;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ edge.\end{tabular} & \crule[lila]{0.8cm}{0.8cm} \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Preprocessing}

Figures \ref{fig:EdgeVsCloudPreproMemory}, \ref{fig:EdgeVsCloudPreproLat} and \ref{fig:CloudEdgePreproCPU} report the preprocessing metrics $Memory_{preprocessing}$, $Latency_{preprocessing}$ and $CPU_{preprocessing}$.
Each figure 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Memory.pdf}
\caption{Edge vs. Cloud Inference -  $Memory_{preprocessing}$}
\label{fig:EdgeVsCloudPreproMemory}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Latencies.pdf}
\caption{Edge vs. Cloud Inference -  $Latency_{preprocessing}$}
\label{fig:EdgeVsCloudPreproLat}
\end{figure}

\FloatBarrier
\subsubsection{Inference}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Memory.pdf}
\caption{Edge vs. Cloud Inference -  $Memory_{inference}$}
\label{fig:EdgeVsCloudInferenceMemory}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies.pdf}
\caption{Edge vs. Cloud Inference -  $Latency_{inference}$}
\label{fig:EdgeVsCloudInferenceLat}
\end{figure}



\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Throughput_with_Preprocessing.pdf}
\caption{Edge vs. Cloud Inference -  $Throughput_{total}$ (Inference + Preprocessing)}
\label{fig:EdgeVsCloudTotalThroughput}
\end{figure}

\FloatBarrier
\paragraph{Edge vs. Cloud Inference - Key Takeaways}
\emph{
\begin{itemize}
    \item x
    \item y
    \item z
\end{itemize}
}

\subsection{Effect of larger Batch Sizes}
\label{chap:resultsBatchSize}
This section covers the results of the experiments with batch size equal or larger than one. Since the TensorFlow Lite of the used MobileNet version does not allow batch sizes larger than one, edge inference experiments with these models are omitted from this section.
\subsubsection{Preprocessing}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_CPU_Usage.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $CPU_{preprocessing}$}
\label{fig:BatchSizePreproCPU}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Latencies.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Latency_{preprocessing}$}
\label{fig:BatchSizePreproLatency}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Memory.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Memory_{preprocessing}$}
\label{fig:BatchSizePreproMemory}
\end{figure}

\FloatBarrier
\subsubsection{Inference}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Memory.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Memory_{inference}$}
\label{fig:BatchSizeInferenceMemory}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Latencies.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Latency_{inference}$}
\label{fig:BatchSizeInferenceLatency}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Throughput.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Throughput_{inference}$}
\label{fig:BatchSizeInferenceThroughput}
\end{figure}


\FloatBarrier
\paragraph{Effect of larger Batch Sizes - Key Takeaways}
\emph{
\begin{itemize}
    \item x
    \item y
    \item z
\end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%
%Überleitung

\endinput 
