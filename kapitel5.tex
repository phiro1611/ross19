\chapter{Experimentation}
\label{chap:experiments}
This chapter deals with the experiments, their design, execution and results.

 
\section{Experimental Design}
This section deals the specification of the Android benchmark application, used hardware, frameworks, models and how the measurements are conducted. 
Figure \ref{fig:expDesign} depicts a brief overview of the design of the experiments, more precisely the used models, frameworks and hardware components, which are presented in greater detail in this section.
The role of the Android benchmark application is to simulate a AI application that delegates the inputs, which in our use case are images, to the edge or cloud inference framework for the inference process and gets the predictions as a response from the frameworks.

\begin{figure}[H]
\centering
\resizebox{.95\linewidth}{!}{\input{./Bilder/Exp_design.tex}}
\caption{Experimental Design Architecture}
\label{fig:expDesign}
\end{figure}


\subsection{Hardware Devices}
\subsubsection{Edge}
\label{chap:hardwareEdge}
As the edge device we will use the OnePlus 6T (ONEPLUS A6013). This state of the art smartphone is powered by a Qualcomm Snapdragon 845 CPU(Octa-core, up to 2.8 GHz), Adreno 630 GPU, 8 GB of memory and runs on OxygenOS 9.0.11, which is based on Android 9.
%%Cite from AI bechmark paper
\subsubsection{Cloud}
%The Nvidia DGX-1 will serve as the cloud-backend for the experiments. This server consists of 8$\times$Tesla V100 providing 1000 TFLOPS as well as 256 GB GPU memory and 512 GB system memory.
We use a virtual server hosted at the LRZ, which has 32 cores (16 real cores with hyperthreading), 240 GB of memory, a Tesla P100 16 GB PCIe GPGPU and a 800 PCIe SSD.

The server runs on Ubuntu 16.04 CUDA 9.1 PGI 17.9 nvidia-docker 2.0.3+docker18.03.1-1.
\subsection{Inference Framework}
We use two open source machine learning frameworks, both based on TensorFlow, for the experiments, TensorFlow Lite and TensorFlow Serving. We decided to use these framework, because they are open source and support, at the point of this theses, most of the operations needed for the popular image classification models as well as an increasing number of hardware accelerators and operating systems for both edge and cloud inference.
TensorFlow provides official releases of deep learning models, that are well maintained and tested, including the ones we use in this thesis.

This section only gives a brief overview of the most important aspects of the frameworks, for detailed information consider the TensorFlow Lite\cite{tfLite}  and TensorFlow Serving\cite{tfServing} websites, on which this section is partly based on, or the corresponding GitHub repositories.
\subsubsection{TensorFlow Lite}
\label{chap:TFLite}
TensorFlow Lite (Release 1.12.0) was developed for mobile and embedded devices and is a lightweight solution of TensorFlow and thus will be used for the edge inference experiments.

%%Mention NNAPI support?
At the moment only model inference can be done by TensorFlow Lite, not model training.
It supports acceleration with GPU or other accelerators as well was portability to Android, iOS and other IoT devices.

\paragraph{Android NNAPI}
\label{chap:NNAPI}
TensorFlow Lite is also compatible with the Android Neural Networks API (NNAPI). This API
is designed to speed up computationally intensive machine learning operations and can be used by TensorFlow Lite to improve inference performance. During inference NNAPI "can
efficiently distribute the computation workload across available on-device processors, including dedicated neural network chips, GPUs and DSPs"\cite{DBLP:journals/corr/abs-1810-01109}.

Figure \ref{fig:NNAPIarchitecture} shows the architecture of the NNAPI. In our use case the application is our android benchmark application presented in section \ref{chap:androidApp} and the machine learning framework we use is TensorFlow Lite.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./Bilder/nnapi_architecture.png}
\caption{System architecture for NNAPI framework \cite{NNAPI}}
\label{fig:NNAPIarchitecture}
\end{figure}


\paragraph{Hosting Models}
TensorFlow Lite expects models in their own \emph{FlatBuffer} file  format(\emph{.tflite}). Therefore models need to be converted to this format, before TensorFlow Lite can load them. This conversion can be done using the TensorFlow Lite Converter, which supports various formats of trained TensorFlow models.
After conversion a \emph{.tflite} model can be loaded by an object of the Interpreter class.
\paragraph{Run Prediction}

To then run the inference process in TensorFlow Lite the run method of Interpreter object with a loaded model needs to be called. To call this function two objects need to be passed, first the input for the given model and second the output object, where the prediction response from the inference operation get stored. 
\begin{figure}[H]
\centering
\input{./Bilder/edge.tex}
\caption{Functionality of TensorFlow Lite}
\label{fig:edge}
\end{figure}
\subsubsection{TensorFlow Serving}
\label{chap:TFServing}
TensorFlow Serving (Release 1.12.0 VERIFY THIS AGAIN) will be used as the cloud inference framework, since it provides a framework to serve machine learning models in production environments. 



\paragraph{Hosting Models}
In order to host a model as a Servable in TensorFlow Serving, first a TensorFlow model needs to exported using TensorFlow's \emph{SavedModelBuilder}, resulting in a \emph{SavedModel protocol} buffer file along with the model’s variables and assets (Although TensorFlow Serving is optimized for TensorFlow models, the framework can be extended to serve other types of models).
%%Wieviel schreiben über signature, predict function etc?

Now the exported model can be loaded by a instance of Tensorflow Serving.
We use docker to start that instance, specifically nvidia-docker that allows us to run the inference operations on a GPU. For that TensorFlow Serving provides two docker images of their framework, one with CPU and the other with GPU support.

\paragraph{Run Prediction}
TensorFlow Serving supports two API for clients to create predictions requests: gRPC and REST. Since the gRPC protocol is supposed to deliver a better performance in the form of lower latencies and smaller payloads, we will use the gRPC API in this thesis.
Before a client can make a resquest to the server, a gRPC stub needs to be created in the first place, that allows us to call all methods implemented on the server. In our case we need to call TensorFlow Serving's \emph{Predict} method to start the inference process. The method needs to be passed a \emph{PredictRequest} object, which contains among other things the input data for the model, the shape of the input and the requested model.%model signature?

After the request is sent and handled the server response by sending back a \emph{PredictResponse} object. This object holds the predictions for the given input data in the form specified by the exported model.
This request and response process can also be seen in figure \ref{fig:cloud}

\begin{figure}[H]
\centering
\input{./Bilder/cloud.tex}
\caption{Basic Functionality of TensorFlow Serving}
\label{fig:cloud}
\end{figure}
\subsection{Models}
\label{chap:models}
We will benchmark two different image classification models in this thesis, MobileNetV2 and InceptionV4, the first is optimized for mobile deployment and the second towards high accuracy.
Both models are trained on a ImageNet dataset consisting of 1001 image classes (1000 image classes + 1 class for other image classes).
An overview of the models can be seen in table \ref{table:modelOverview} in the form of top-5 accuracy (a prediction is classified as accurate if the five labels with the highest confidence contain the real class of the input image), the input size of the model, the number of parameters in millions and the model size in the TensorFlow Lite format.
InceptionV4 has $4.5\%$ more accuracy than MobileNetV2, but also more than $12$ times more parameters and $7.5$ times larger TensorFlow Lite model size.
MobileNetV2 also uses a smaller input sizes of $224\times224$ than InceptionV4's $299\times299$ input size.
\begin{table}[H]
%CITE inception params ned vergessen
%http://dgschwend.github.io/netscope/#/preset/inceptionv4
\caption{Overview of used models}
\label{table:modelOverview}
\begin{tabular}{@{}lllll@{}}
\toprule
Model & Parameters & Top-5 Accuracy\cite{modelspecs} & Input Size & TF Lite Model Size \\
\midrule
InceptionV4 & $42.68$M & $95.1\%$ & $299\times299$ & $107.7$MB \\
MobileNetV2 1.0 & $3.47$M\cite{DBLP:journals/corr/abs-1801-04381} & $90.6\%$ & $224\times224$ & $14$MB \\
\begin{tabular}[c]{@{}l@{}}MobileNetV2 1.0\\quantized\end{tabular}  & $3.47$M\cite{DBLP:journals/corr/abs-1801-04381} & $89.9\%$ & $224\times224$ & $3.4$MB\\
\bottomrule
\end{tabular}
\end{table}
For the TensorFlow Lite version we use the models provided on the TensorFlow Lite website \cite{tfLiteModels}.
To convert model suitable for TensorFlow Serving, we use the TensorFlow-Slim library \cite{tfSlim}, where maintained and tested implementations of popular image classification models are being published. For both Serving and Lite we use the same training checkpoint to get the same weights for the graphs.

In the following we give a brief overview of the models, their unique building blocks and the intuition behind them.
For full details please refer to \cite{DBLP:journals/corr/abs-1801-04381} and \cite{InceptionV4}.

\subsubsection{MobileNetV2}
MobileNetV2 (version 1.0) is a successor of MobileNetV1 and is "specifically tailored for mobile and resource
constrained environments" \cite{DBLP:journals/corr/abs-1801-04381}. The authors do this by "significantly decreasing the number of operations and the memory needed while retaining the same accuracy"  \cite{DBLP:journals/corr/abs-1801-04381} and introducing a new layer module called "the
inverted residual with linear bottleneck".

This module is visualized in figure \ref{fig:bottleneckBlock} and consists of two parts: The inverted residual block and a shortcut from the input to the output.
In a first step of the inverted residual block the channel dimension of the input are expanded with the use of a pointwise $1\times1$ convolution layer. 
Afterwards depthwise $3\times3$ convolution is applied to the expanded input. Then again a pointwise $1\times1$ convolution gets used, but this time the dimensions are decreased instead of increased.

The intuition behind this module is that the expansion decodes information ensuring that the features can be extracted during the depthwise convolution. The extracted features then get encoded again by reducing the dimensions.
To improve the backpropagation of the gradient across multiple layers during the training the authors add the shortcut to the module, resulting in faster training and better accuracy.
The bottleneck module can be implemented very memory efficient, thus particularly fit for edge inference.

Figure \ref{fig:MobileNetArchi} displays the overall architecture of the model, with the majority of the building blocks being the bottleneck modules.
All bottleneck modules in a sequence have the same number of output channels and use a stride of $1$, except for the first bottleneck block in a sequence. All spatial convolutions use $3\times3$ kernels. 
Although not depicted on the figure, the model uses dropout and batch normalization during training.
%%expanstionf actor?
%%$$shortcut!! expanded by a "expansion factor" varying for different seqences

\begin{figure}[!htb]
\centering
   \resizebox{.7\linewidth}{!}{\input{Bilder/MobileNet_bottleneck.tex}}
\caption{Inverted Residual Block (bottleneck)}
\label{fig:bottleneckBlock}
%%NO DROPOUT AT INFERENCE
\end{figure}


\begin{comment}


\begin{table}[]

\centering
\caption{MobilenetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
\label{table:mobilenetArchi}
\begin{tabular}{@{}llllll@{}}

\toprule
Input & Operator & t & c & n & s \\ \midrule
$224^2\times 3$ & conv2d & - & 32 & 1 & 2 \\
$112^2\times 32$ & bottleneck & 1 & 16 & 1 & 1 \\
$112^2\times 16$ & bottleneck & 6 & 24 & 2 & 2 \\
$56^2\times 24$ & bottleneck & 6 & 32 & 3 & 2 \\
$28^2\times 23$ & bottleneck & 6 & 64 & 4 & 2 \\
$14^2\times 64$ & bottleneck & 6 & 96 & 3 & 1 \\
$14^2\times 96$ & bottleneck & 6 & 160 & 3 & 2 \\
$7^2\times 160$ & bottleneck & 6 & 320 & 1 & 1 \\
$7^2\times 320$ & conv2d 1x1 & - & 1280 & 1 & 1 \\
$7^2\times 1280$ & avgpool 7x7 & - & - & 1 & - \\
$1\times 1\times 1280$ & conv2d 1x1 & - & k & - &  \\ \bottomrule
\end{tabular}
\end{table}
\end{comment}
\paragraph{Quantization}
Quantization of MobileNetV2, using the techniques presented in section \ref{chap:quant}, results in a  0.7\% loss of top-5 accuracy, but therefore has lost 75\% of its model size (see table \ref{table:modelOverview}) and is supposed to deliver a better inference performance, which we will evaluate in the experiments.



\subsubsection{Inception V4}
%%Add infos about stem and reduction
InceptionV4, published in \cite{InceptionV4}, is a large image classification network with high accuracy, but also with a high number of parameters leading to higher computational demands than MobileNetV2 and thus larger impact on inference performance.
In comparison to its previous versions InceptionV4 is built with "a more uniform simplified architecture and more inception modules". 

The general architecture of the network can be seen in figure \ref{fig:inceptionv4Archi} with the fundamental building blocks being multiple Inception (A-C) and Reduction(A-B) modules. 
An example for both of these modules can be seen in figures \ref{fig:InceptionA} and \ref{fig:InceptionReduction}.
Inception modules are built of multiple convolutions with multiple filters and pooling layers in parallel within the same layer.
After the convolutions the output of all convolutions are concatenated.
The intuition behind this parallelism is to give the model multiple convolution choices for a given input and let the model learn itself the best feature extractor. An additional benefit is that the model can extract both local and more complex feature from an input.

To reduce the dimensionality before computational expensive large convolutions, and thus speed up training, the authors introduce the Reduction block containing small $1\times1$ convolutions.   %genauer erklären




\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{.95\textwidth}
\centering
   \resizebox{.8\linewidth}{!}{\input{Bilder/InceptionAModule.tex}}
   \caption{Inception-A module}
   \label{fig:InceptionA} 
\end{subfigure}

\vspace{1em}
\begin{subfigure}[b]{.95\textwidth}
\centering
   %%Verify the architectures again
   \resizebox{.6\linewidth}{!}{\input{Bilder/InceptionReductionModule.tex}}
   \caption{Reduction module (convolutions with "V" are valid, rest is same padded )}
   \label{fig:InceptionReduction}
\end{subfigure}

\caption{Special modules used by InceptionV4}

\end{figure}




Besides dropout InceptionV4 also benefits from the use of batch normalization during training.
%\begin{figure}[]
%\centering
%\resizebox{.45\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
%\includegraphics[angle=90,width=0.94\textwidth]{./Bilder/inceptionV4_architecture.png}
%\caption{InceptionV4 architecture \cite{InceptionV4}}
%\label{fig:inceptionv4}
%\end{figure}

\begin{figure}[!htb]
\centering
\begin{subfigure}[t]{0.47\textwidth}
   \resizebox{.99\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
   \caption{InceptionV4 architecture \cite{InceptionV4}}
   \label{fig:inceptionv4Archi} 
\end{subfigure}%
\begin{subfigure}[t]{0.47\textwidth}
   %%Verify the architectures again
   \resizebox{.99\linewidth}{!}{\input{Bilder/MobileNetV2_archi.tex}}
   \caption{MobileNetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
   \label{fig:MobileNetArchi}
\end{subfigure}

\caption{Architectures of InceptionV4 and MobileNetV2}
%%NO DROPOUT AT INFERENCE
\end{figure}



\subsection{Android Benchmark Application}
\label{chap:androidApp}
To conduct the experiments for both edge and cloud inference we developed and implemented an Android benchmark application using Kotlin.
This application implements all function needed to preprocess real workload images and perform the inference on either the Android device itself or sent the image to a cloud-backend.
For both preprocessing and inference the application logs metrics such as inference latency, time of the experiments etc. and stores them to a \emph{CSV} file, which then can be used to evaluation purposes.

Both edge and cloud inference implementations are based on the example implementations in the respective GitHub repositories to ensure optimal performance. 


\begin{figure}[htb]
\centering
\includegraphics[width=0.97\textwidth]{./Bilder/FlowChart_App.png}
\caption{Flowchart of the Benchmark Application showing the whole inference process of selecting an image until getting a prediction}
\label{fig:app}
\end{figure}
In figure \ref{fig:app} the work-flow to perform the inference is seen. First, a image needs to be selected. Afterwards the image classification model needs to be chosen. Now the user selects whether the inference should be performed on the cloud-backend or directly on the edge device, in this case a Android phone. In the case of edge inference it needs to be decided if the NNAPI should be used by TensorFlow Lite. For cloud inference the preprocessing mode needs to be selected (edge/cloud). Now the preprocessing operations can be performed based on the previous selected options (Even for the case of cloud inference with cloud preprocessing some preprocessing needs to be done on the edge beforehand like building a \emph{PredictRequest} containing the request for the cloud-backend). Now, that the input is preprocessed, the actual inference is performed as a last step, resulting in a prediction. For both preprocessing and inference the measurements mentioned in section \ref{chap:insta_measurements} are logged.

\begin{figure}[htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/UML.pdf}
\caption{UML class diagram of the benchmark application}
\label{fig:UML}
\end{figure}
Figure \ref{fig:UML} depicts an UML class diagram of the application featuring the most important classes, functions and variables. 
The main class is called \emph{MainActivity} and implements all of the graphical aspects. 
The \emph{MainActivity} handles requests for both cloud and edge inference by delegating the requests to instances of the classes \emph{TFServingClient} and \emph{ImageClassifier}, respectively. Both of these classes also take care of the preprocessing steps.
The abstract class \emph{ImageClassifier} has two subclasses \emph{FloatClassifier} and \emph{QuantClassifier}. The first class runs the inference for floating point models and the second for quantized models.
This separation is needed since these different model types require different input and output types.

The \emph{MainActivity} writes the collected measurements and the parameter configurations of an experiment to a instance of \emph{SingleExperiment}. After the experiment is completed \emph{MainActivity} tells a \emph{Logger} instance to save the contents of the \emph{SingleExperiment} object. The Logger then saves all collected data to a \emph{CSV} file.

\subsubsection{Preprocessing}
%%mention png
For the case of image classification, the images need to have the correct size ($224\times224$ for MobilenetV2 and $299\times299$ for InceptionV4) and the RGB values need to be normalized to the interval $[-1,1]$. After preprocessing the image has been transformed into the shape $224\times224\times3$ with all values between $[-1,1]$, where the the first two dimensions represent the image height and width, while the last dimension represent the number of channels (3 since the images are RGB)
\begin{figure}[H]
\centering
\input{Bilder/Preprocessing.tex}
\caption{Preprocessing steps for image classifcation: Resizing to the model input size and normalization of the pixel values to $[-1,1]$}
\label{fig:prepro}
\end{figure}
\paragraph{Edge Preprocessing}
\label{chap:preproImpl}
In the case of edge preprocessing all preprocessing steps are done on the edge device, meaning that the input can be fed directly into the neural network afterwards, either on the edge device itself or on a cloud-backend.
We perform these steps the following way: After loading the PNG image into an \emph{InputStream} we create a scaled Bitmap of the image (scaled to either $224\times224$ or $299\times299$) by calling the \emph{createScaledBitmap} function. 
Afterwards we normalize the RGB values to $[-1,1]$ during the conversion from the bitmap to a \emph{ByteBuffer}. 
For the case of float models this buffer contains all the pixels in float format and for quantized model in bytes, since quantized models work with lower represenations.
%%%mehr erklären?
We do this conversion since feeding \emph{ByteBuffers} to TensorFlow Lite is performance enhancing. %add cite here
For cloud inference we then construct \emph{PredictRequest} object containing this \emph{ByteBuffer}.

For batch size larger than one we parallize the preprocessing to speed up the proprocessing latency at the cost of higher maximum memory consumption. We start $n$ threads, where each thread is preprocessing one image in the way described above. After each image is preprocessed we concatenate all \emph{ByteBuffers} into a single one, which then can be fed to the TensorFlow Lite interpreter. Note that $n$ is determined by both batch size and available CPU cores on the edge device. There are never more threads than available cores, but if the batch size is smaller than the number of cores, we only start $n$ threads, where $n$ is the batch size. 

\paragraph{Cloud Preprocessing}

In the case of cloud inference the images can also be preprocessed directly on the cloud, resulting in nearly no preprocessing done on the edge. While the resizing and scaling steps are no longer done on the edge, the image still needs to converted into a \emph{PredictRequest} object that TensorFlow Serving can handle.
To achieve this we again load the PNG image into an \emph{InputStream}, convert it to a \emph{ByteArray} which then can be feed to the \emph{PredictRequest} object as a \emph{ByteString}. 

%Add TensorFlow Serving preprocessing here


\subsubsection{Inference}
To get the predictions for our now preprocessed image we need to run the inference operation of the inference framework with the deep learning model laoded, which is loaded either directly on the edge or on the remote cloud-backend. 
The inference is done using the steps described in section \ref{chap:TFLite} (TensorFlow Lite) and \ref{chap:TFServing} (TensorFlow Serving).
\paragraph{Edge Inference}
To perform the inference operation we two things into the \emph{run} function of an interpreter of TensorFlow Lite: The \emph{ByteBuffer} created in the preprocessing step and an array (float models: \emph{FloatArray}, quantized models: \emph{ByteArray}) with the length 1001 (number of classes). TensorFlow lite then writes the confidence levels of the different classes to this array. We then sort the array for the five classes with the highest confidence and print them to the screen.

\paragraph{Cloud Inference}
\label{chap:CloudInfImpl}

For the cloud inference we send the \emph{PredictRequest} object created in the preprocessing process to the TensorFlow Serving server, where the inference (and in the case of cloud preprocessing also the preprocessing) computations are executed.
To sent the \emph{PredictRequest} object to the server we call the \emph{predict} function of the previous created TensorFlow Serving gRPC stub.
Afterwards the client receives the \emph{PredictResponse} from the server containing the predictions for the sent image. We configured the TensorFlow Serving models to return the five classes with the highest confidence, hence we extract these five classes from the \emph{PredictResponse} and print them to the screen.

To preprocess the image on the cloud we add TensorFlows preprocessing functions to the model graphs before we export them to the TensorFlow Serving format. The images arrive at the server in the \emph{PNG} format, therefore we first decode them with \emph{tf.image.decode\_jpeg}, then resize with \emph{tf.image.resize\_bilinear} and finally normalize the tensor values to $[-1,1]$ using the the \emph{subtract} and \emph{multiply} functions. Now the input has the same shape as if they would have been preprocessed on the edge can be fed to the actual model graphs.
\section{Instantiation}
Using the experimental design presented in the previous section, we now describe which parameters we change in the course of the experiments as well as how the performance metrics are measured.
\subsection{Parameters}
We run each parameter configuration 15 times to reduce variance and stabilize our results.
During the experiments no other applications are running on either the edge or the cloud device.
In the course of the experiments we change the configurations of the following parameters:
%%%293,performance analysis buch
\paragraph{Model}
We conduct experiments for all three models listed presented in \ref{chap:models}: InceptionV4, MobileNetV2 and MobileNetV2 quantized.

\paragraph{Preprocessing Mode}
For the case of cloud inference the major parts of the needed preprocessing is either done on the edge before sending the image to the cloud or done on cloud. Therefore we evaluate both options.
\paragraph{Inference Mode}
The inference can either be performed on the edge or an a cloud-backend.

\paragraph{Image Size}
%%Add table here?
%224: 83KB
%299: 141KB
%2MP: 2411KB
%4MP: 4309KB
%8MP: 7515KB
%16MP: 10077KB
We evaluate the performance of 2, 4, 8 and 16  megapixel (MP) PNG images, as the OnePlus 6T is capable of taking pictures with 16 megapixels. This way the effect of different image sizes on the performance of the preprocessing step can assessed. We also evaluate an image where no resizing is needed ($224\times224$ and $299\times299$ depending on the model) to study the impact of image resizing. A picture of a cat (see figure \ref{fig:cat}) scaled to the different sizes will serve as the picture for the experiments.
The sizes of the images in kilobytes as well as the according resolutions can be seen in table \ref{table:imagesOverview}.

\begin{table}[!htb]
\centering
%CITE inception params ned vergessen
%http://dgschwend.github.io/netscope/#/preset/inceptionv4
\caption{Overview of used image sizes}
\label{table:imagesOverview}
\begin{tabular}{@{}lll@{}}
\toprule
Image   & Resolution       & PNG Size  \\ \midrule
$224^2$ & $224\times224$   & $83$KB    \\
$299^2$ & $299\times299$   & $141$KB   \\
$2$MP   & $1732\times1155$ & $2411$KB  \\
$4$MP   & $2449\times1633$ & $4309$KB  \\
$8$MP   & $3464\times2309$ & $7515$KB  \\
$16$MP  & $4899\times3266$ & $10077$KB \\ \bottomrule
\end{tabular}
\end{table}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.3\textwidth]{./Bilder/European_cat_compressed.jpg}
\caption{Picture used for the experiments \cite{cat}}
\label{fig:cat}
\end{figure}
\paragraph{Batch Size}
%%mathe defs of throughput?
Batch Size denotes the number of images fed into the deep learning model in a single inference operation. 
Feeding more than one image to the network can lead to a higher throughput, if enough computational power is available. This increase in throughput often comes at the cost of increased latency and general resource consumption.

To study these trade-offs we conduct experiments with the following batch sizes: 1, 2, 16, 32. Since at the point of these experiments a batch size greater than one is not supported for the TensorFlow Lite versions of MobilenetV2 (both float and quantized), measurements with these batch sizes cannot be performed for the case of edge inference at this point.
\paragraph{NNAPI}
The Android Neural Network API (NNAPI), presented in section \ref{chap:NNAPI}, is supposed to enhance the inference performance of TensorFlow Lite. Therefore we take a look into the effect of this framework.
%%Mention GPU use here
\paragraph{GPU Usage}
Since January 16 an experimental release of TensorFlow Lite supporting GPU usage on Android using OpenGL ES 3.1 Compute Shaders \cite{tfLiteGPU}.
So far only four public models and their operators are supported, including MobileNetV2, but not InceptionV4. 
Therefore we omit the test of this new feature in our experimentation, but our initial testing indicated that the use of NNAPI provides faster inference latencies for MobileNetV2 than the GPU version.
%%https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7
%experimental
%only mobilenet supported
%worse performance than NNAPI so far


\begin{figure}[H]
\centering
 \scalebox{.7}{\input{./Bilder/tree.tex}}

\caption{Excerpt of the performed experiments configurations}
\label{fig:tree}
\end{figure}
Figure \ref{fig:tree} shows a part of our experimentation tree. In theory there would be 240 different parameter configurations, but due to a number of reasons the number of different configurations reduced to 113 in total.

First, the Usage of NNAPI is independent of the image size hence we only perform experiments without NNAPI for a single image size. Also the usage of NNAPI leads to a way better performance for edge inference and we want to compare edge and cloud inference, so we only carry out experiments without NNAPI for a batch size of one.
We omit the cloud inference for the quantized MobileNetV2 because the model is so small the network overhead would be overwhelming.%INSERT REASON.

%of limited capacity/time and
Lastly due to lack of support certain operations in TensorFlow Lite for the MobileNetV2 models, we can not perform inference experiment for MobileNetV2 with a batch size larger than one.
\subsection{Performance Metrics}
\label{chap:insta_measurements}
In the following section we describe how we measure the performance metrics defined in \ref{chap:metrics}.
We conduct the measurements either directly in the source code or by using Android Studio Profiler (Version 3.3). Since Android Studio cannot collect all metrics the way we wanted, we wanted to used Trepn as a secondary profiling application. But since Trepn does not support our test device, we had to omit these metrics.

In the following definitions $t_{start}$ denotes the starting point of either the preprocessing or inference, while $t_{end}$ denotes the end point of the respective process.
\subsubsection{Latency}
We measure $Latency_{preprocessing}$ by measuring the time difference between start and end of the preprocessing process.
Note that all latency measurements are reported in milliseconds(ms).
\begin{equation*}
\begin{gathered}
Latency_{preprocessing} = t_{endPreprocessing} - t_{startPreprocessing}
\end{gathered}
\end{equation*}
To measure inference latency we need to distinguish between edge and cloud inference, since for the latter the network latency needs to be considered.

\paragraph{Edge Inference}To measure edge inference latency we measure the time the TensorFlow Lite interpreter needs to run the inference operation on the loaded model given the input image.
\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{endInference} - t_{startInference}
\end{gathered}
\end{equation*}
\paragraph{Cloud Inference}
To measure the cloud inference latency we need to measure two latencies, which combined yield $Latency_{inference}$. The first latency is  $Latency_{server}$. This server latency describes the time difference between the point where TensorFlow Serving receives the inference request and the point in time where TensorFlow Serving sends the response back to the client.
The second latency $Latency_{network}$ denotes the time the prediction request needs to reach the cloud-backend.
These latencies are illustrated in figure \ref{fig:serverLat}.
\begin{figure}[!htb]
\centering
\input{./Bilder/server_lat.tex}
\caption{Measurement of $Latency_{server}$ and $Latency_{network}$ for Cloud Inference}
\label{fig:serverLat}
\end{figure}


Similar to the edge inference, $Latency_{inference}$ is being measured by calculating the time difference between starting the inference process and receiving the prediction for the given inference request. This whole process is covered by the \emph{predict} function of TensorFlow Serving. Therefore we measure the wall clock time of this function.

Since TensorFlow Serving does not output the server latency, we needed to tweak the source code of gRPC, which is the underlying protocol of TensorFlow Serving. gRPC already logs this latency, so we adjust the source code to output this latency when a call to TensorFlow Serving is finished, repackage the source code and change to dependencies of TensorFlow Serving pointing to the adjusted packages.

We then calculate $Latency_{network}$ implicitly by subtracting $Latency_{server}$ from $Latency_{inference}$.

\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{endInference} - t_{startInference}\\
Latency_{server}= t_{receive Request} - t_{send Response}\\
Latency_{network} = Latency_{inference} - Latency_{server}
\end{gathered}
\end{equation*}


The total latency $Latency_{total}$ for both edge and cloud inference is simply calculated by summing up the latencies of both preprocessing and inference.
\begin{equation*}
\begin{gathered}
Latency_{total} = Latency_{preprocessing} + Latency_{inference}
\end{gathered}
\end{equation*}
\subsubsection{Energy Consumption}
Since Android Studio Profiler only estimates the energy consumption in the form of low, medium and high, the tool is not fit to provide empiric measurements. The Trepn Power Profiler would provide such measurements, but does not support the device used for the experiments (OnePlus 6T).
\subsubsection{CPU Usage}
We measure the CPU usage(\%) of both preprocessing as the maximum CPU usage during the respective processes.
\begin{equation*}
\begin{gathered}
%%CPU_{preprocessing} = max_{[start_{preprocessing}, end_{preprocessing}]}(CPU)\\
%%CPU_{inference} = max_{[start_{inference}, end_{inference}]}(CPU)\\
CPU_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} CPU_{Usage}(t)\\
CPU_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} CPU_{Usage}(t)
\end{gathered}
\end{equation*}
Android Studios’ CPU Profiler allows us to record the maximum CPU usage for both preprocessing and inference. To minimize the impact on the Android Profiler on the performance of the application we disable allocation tracking.
\begin{figure}[H]
\centering  
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_CPU}
\caption{Android CPU Profiler}
\label{fig:prof_cpu}
\end{figure}
\subsubsection{Memory Usage}
We measure the memory usage by recording the maximum memory consumption during both processing and inference. Both of the following metrics are reported in megabytes(MB) in the result section.
\begin{equation*}
\begin{gathered}
Memory_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} Memory(t)\\\\
Memory_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} Memory(t)\\
\end{gathered}
\end{equation*}
The Memory Profiler is part of Android Studio Profiler and shows the memory consumption of the app it is profiling. Memory allocations by the operating system or other apps are not recorded. Besides recording the total amount of memory allocated the Profiler also tracks the different categories, for example memory allocated by Java/Kotlin code. We always record the maximum consumed memory for each operation.
%%memory not accurate over 1GB
Note that for values greater than $1000$MB the profiler only reports gigabyte values with one decimal place. For example the profiler reports $1410$MB as $1.4$GB.

Figure \ref{fig:prof_mem} depicts an example of the Memory Profiler for a single experiment. The first peak in memory consumption is the preprocessing step, while the second peak is caused by the inference process.
The little trash can at the bottom of the figure shows that the garbage collection was called. The garbage collection was always manually called after the preprocessing in case not all unneeded memory allocations are collected before running the inference operation. 
Note that we report total memory consumption of the application, including memory consumed for the graphical interface or the \emph{Logger} class.
%In the case of preprocessing only the preprocessed image is needed, so the 
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_MEM}
\caption{Android Memory Profiler showing the memory consumption during preprocessing and inference}
\label{fig:prof_mem}
\end{figure}
\subsubsection{GPU Usage}
Neither Android Studio nor Trepn can provide GPU metrics for the OnePlus 6T. Hence no GPU measurements can be conducted.
\subsubsection{Throughput}
We differentiate three types of throughput: the inference throughput, preprocessing throughput and the total throughput, which also includes preprocessing besides inference.
We calculate throughput in operations per second by 

\begin{equation*}
\begin{gathered}
Throughput_{preprocessing} =\frac{1000}{(Latency_{preprocessing}) / batchsize}\\
Throughput_{inference} =\frac{1000}{(Latency_{inference}) / batchsize}\\
Throughput_{total}  =\frac{1000}{(Latency_{total}) / batchsize}
\end{gathered}
\end{equation*}
\subsubsection{Data Consumption}
We measure both transmitted and received data by using the Android TrafficStats package (https://developer.android.com/reference/android/net/TrafficStats). We start measuring both transmitted and received bytes when the inference operation is started and stop when the response from the server is returned. We report both $Data_{transmitted}$ and $Data_{received}$ in kilobytes(KB).
\begin{equation*}
\begin{gathered}
Data_{transmitted} = \sum_{t_{startInference}}^{t_{endInference}} Data_{transmitted}(t)\\
Data_{received} = \sum_{t_{startInference}}^{t_{endInference}} Data_{received}(t)
\end{gathered}
\end{equation*}
\section{Results and Evaluation}
The previous section covered the experimental design, including used models, hardware devices and framework, as well as definition of performance metrics. Using this design specifications, we will now present and evaluate the results of the conducted experiments.
For precise mean values and standard deviations of all measured metrics refer to tables \ref{measurementsInception} and \ref{measurementsMobilenet}.

First, we take a look into the individual results of edge and cloud inference and afterwards compare them against each other. In each of these sections the preprocessing results are presented first, followed by the inference results.
In the last subsection we take a separate look at batch sizes larger than one.

To visualize the results we mostly use bar and line plot, which report the average values.
The black bar on top of each bar of the following bar charts and translucent areas around the lines of the line plots represent the standard deviation.

%die 83mb aufschlüsseln in GUI etc 
Note that the Android application consumes around 83MB in memory and 0\% CPU during idle. The logger, which is used to save the measurements of the experiments consumes memory as well, has additional impact on the memory consumption as well, hence the memory consumption shown in the results include both logger and other memory overhead may caused by the graphical interface etc.

Preprocessing is performed without any models loaded on the edge. but with the image loaded into memory before the start of preprocessing to prevent the I/O operations to influence the preprocessing performance.
Edge Inference is done using a loaded deep learning model. 
We do not consider model loading latency in this thesis.

All experiments are done using the eduroam Wi‑Fi, since the cloud-backend server hosted at the LRZ is only reachable within the Münchner Wissenschaftsnetz (MWN) and a VPN would have an impact on network latencies.
We conducted the experiments in multiple sessions, where each session consists of $100$ experiments distributed over about two hours, thus preventing the results to be skewed by thermal throttling.
\subsection{Edge Inference}
This section present the results of the edge inference experiments with OnePlus 6T device presented in section \ref{chap:hardwareEdge}.

Note that this section only covers the results for a batch size of one, for the results of larger batch sizes please refer to section \ref{chap:resultsBatchSize}.

\FloatBarrier
\subsubsection{Preprocessing}
\label{chap:edgePrepro}

\paragraph{$\mathbf{Latency_{preprocessing}}$ \& $\mathbf{Memory_{preprocessing}}$}
Figure \ref{fig:EdgePrepro} shows the $Latency_{preprocessing}$ and $Memory_{preprocessing}$ results for the different image sizes and models.
There is little difference in both latency and memory for the different models. 
InceptionV4 uses on average $7.5$MB more memory (or $5\%$) than the MobileNetV2 models, which is caused by the models higher input size (InceptionV4 $299\times299$, MobileNetV2 $224\times224$). 
These small differences are expected, since both all three model require roughly the same input specifications, with the exception of input size, but the difference between the two input sizes is relatively small.

While the different models have little difference in memory and latency, the image size has an significant impact on both of these metrics.
A $224^2/299^2$ image consumes on average $114.5$MB memory and takes $38.2$ms to preprocess, whilst  $16$MP images consume $177.8$MB and take $430.8$ms.
Therefore $16$ megapixel images take on average  more than $11$ times as long to preprocess across all models, as well as using $1.5$ times more memory in comparison to a $224^2/299^2$ image.
Figure \ref{fig:EdgePrepro} shows a steady increase in $Latency_{preprocessing}$ and $Memory_{preprocessing}$ across the increasing image sizes.
Preprocessing large images is therefore mainly affected by the resizing process, not the pixel normalization or conversion to a \emph{ByteBuffer}, since these steps are performed after the resizing, thus having the same impact on performance as they would have on an $224^2/299^2$ image.

The standard deviation of both memory and latency are both low, indicating a stable resource consumption.


%%factors reinbringen auch kb größe der bilder
%%mehr impact on latency than memory

\begin{figure}[!htb]
\centering
\includegraphics[width=0.96\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Preprocessing.pdf}
\caption[Edge Inference: Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$]{Edge Inference: Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$ -  lower is better: Increasing image sizes have increasing impact on both memory and latency, independent of model type}
\label{fig:EdgePrepro}
\end{figure}
%%Was über CPU hier sagen
%%CPU usage in obere grafik integrieren?
$\mathbf{CPU_{preprocessing}}$
Looking the CPU usages during preprocessing (see figure \ref{fig:CloudEdgePreproCPU}) one can see that the usages are very similar across all models and image sizes. This is probably due to the fact that the preprocessing of a single image is done on a single core. Since the OnePlus 6T has $8$ cores, the maximal CPU usage cause by a single core is $12.5\%$, which also displays in the mentioned plot, where all usages are close to this number.
%%High variance
%%around?

\FloatBarrier
\subsubsection{Inference}
This section presents the results for edge inference, in particular the effect of the Android Neural Network API.
Different image sizes have no effect on edge inference, as image are always preprocessed when they reach the inference step.
Hence we do not consider the different image sizes in the majority of this section.


\begin{comment}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Inference.pdf}
\caption{Edge Inference: Inference metrics - lower is better}
\label{fig:EdgeInference}
\end{figure}


\end{comment}
\subsubsection{Effect of NNAPI}
%%effcient
The Android Neural Network API (NNAPI) is supposed to speed up inference of neural network on Android by introducing optimized kernels/operators. 
The effect of this framework can be seen in figure \ref{fig:NNAPI} and show the significant performance improvement caused by the NNAPI, not only affecting $Latency_{inference}$, but also $Memory_{inference}$ and $CPU_{inference}$.
In general the usage of NNAPI leads to $4$ times lower inference latencies, $1.2$ times less memory consumption as well as $3$ times lower CU usages.

This effect can be observed across all tested models, but especially on the InceptionV4 network, as this large model especially benefits from the optimized operations.
Note that the NNAPI uses the GPU of the OnePlus 6T for a part of its inference, therefore while reducing the CPU usage during inference, the NNAPI probably causes higher GPU usages.

Since the NNAPI leads to performance improvements in all measured metrics, it will be used for all further edge inference results.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.96\textwidth]{./Bilder/single_plots/edge_inference_plots/NNAPI_behavior.pdf}
\caption[Edge Inference - Effect of NNAPI on Inference]{Edge Inference - Effect of NNAPI on Inference - lower is better: NNAPI has a significant positive effect on the inference metrics $Latency_{inference}$, $Memory_{inference}$ and $CPU_{inference}$.}
\label{fig:NNAPI}
\end{figure}

%, InceptionV4, MobileNetV2 and the quantized version of MobileNetV2,
When comparing the different models with NNAPI enabled it can be stated that there a significant performance differences between these models.
%%MobileNet und Inception vergleichen
%begründen: effect of quant mit paper vergleichen
%unterschiede mobileNet inception vergleichen
%%danahc MobileNet mit quantized
\paragraph{$\mathbf{Latency_{inference}}$ \& $\mathbf{Memory_{inference}}$}
On average, InceptionV4 inference causes higher inference latencies of factor $7$ ($33.1$ms vs. $237$ms) and consumes more than twice as much memory as MobileNetV2 ($126.2$MB vs. $276.3$MB).
This contrast in performance is as expected, since MobileNetV2 architecture only contains $8\%$ of InceptionV4's parameter number (see table \ref{table:modelOverview}).
This bigger architecture as well as bigger input size lead to higher memory demands and latencies, thus explaining the performance differences.

A similar difference in inference latency can be seen when comparing MobileNetV2 against its quantized version, where the quantized version is more than $2.8$ times faster ($33.1$ms vs. $11.7$ms), confirming the study results of \cite{Quantizing} presented in section \ref{chap:quant}.
While there is a difference in latency between the quantized and non quantized versions of MobileNetV2, there is no significant discrepancy in memory consumption ($126.2$MB vs. $119.5$MB), when considering the standard deviation of the quantized MobileNetV2, which is $10.8$MB.


\paragraph{$\mathbf{CPU_{inference}}$}
While having big impact on both latency and memory, the different models have 
negligible CPU usage differences (see figure \ref{fig:EdgeVsCloudInferenceCPU}), but probably not on GPU usage, which we can not report in this thesis.
The $CPU_{inference}$ of all three models are around $8\%$, which no model using singificantly less or more than the other models.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%Edge Inference Prepro Vs Inference%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mathbf{Latency_{total}}$}
Figure \ref{fig:EdgeInferenceRatio} shows $Latency_{total}$ results of edge inference, specifically the latency ratio between preprocessing and inference.
As image sizes increases, the preprocessing becomes more and more the bottleneck, especially for the MobileNetV2 models.
For the MobileNetV2 even $224\times224$ images take longer to preprocess than to perform the inference on them.
%The Inception models are far more computational intensive, hence the longer inference latencies, but still 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Preprocessing_+_Inference.pdf}
\caption[Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$]{Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$ - lower is better: Preprocessing becomes the decisive factor for $Latency_{total}$ of both models for larger image sizes.}
\label{fig:EdgeInferenceRatio}
\end{figure}


\paragraph{Edge Inference - Key Takeaways}
\emph{
\begin{itemize}
    \item Preprocessing becomes bottleneck for larger image sizes.\\
          $16$ MP image in comparison to a $224^2/299^2$ image needs
    \begin{itemize}
        \item $10\times$ slower $Latency_{preprocessing}$
        \item $1.5\times$ more $Memory_{preprocessing}$
    \end{itemize}
    \item NNAPI leads to way better performance across all models:
    \begin{itemize}
        \item $4\times$ faster $Latency_{inference}$
        \item $1.2\times$ less $Memory_{inference}$
        \item $3\times$ less $CPU_{inference}$
    \end{itemize}
    \item In comparison to InceptionV4 MobileNetV2 leads to:
    \begin{itemize}
        \item $8.6\times$ faster $Latency_{inference}$
        \item $2.2\times$ more $Memory_{inference}$
    \end{itemize}
    \item Quantization of MobileNetV2 results in $2.3$x faster $Latency_{inference}$
    \item Preprocessing affects latency more than inference across all images sizes for small networks.
\end{itemize}}

\FloatBarrier
\subsection{Cloud Inference}
This section deals with the results of the cloud inference experiments and their evaluation, divided into preprocessing and inference.
Note that this section only covers the results for a batch size of one, for the results of larger batch sizes please refer to section \ref{chap:resultsBatchSize}.
We are evaluating $Latency_{inference}$ for the most cloud inference results, thus including the network component $Latency_{network}$, because this factor would also be present in real world AI applications. In our case we have a high speed connection with low network latency, thus the network latency represents a lower bound for real-time AI applications.
%%add network proof (ping and upload)
\subsubsection{Preprocessing}
Cloud Inference allows two preprocessing methods, either on the edge beforehand or directly on the cloud.
This section present the results of these two methods, especially their impact on the resource consumption of edge devices.

\paragraph{$\mathbf{Latency_{preprocessing}}$ \& $\mathbf{Memory_{preprocessing}}$}
Figures \ref{fig:cloudInferencePreproLat} and \ref{fig:cloudInferencePreproMemory} display the effect of preprocessing on either edge or cloud on the preprocessing latencies and memory consumption on the edge in respect to the different deep learning models and image sizes.

For Edge preprocessing, $Memory_{preprocessing}$ and $Latency_{preprocessing}$ are heavily affected by rising image sizes, but not by the different models and their different image input sizes.
Since preprocessing on the edge is very similar as in the edge inference case, please refer to section \ref{chap:edgePrepro} for full details on the edge preprocessing results.

Preprocessing on the cloud leads to an significant decrease in $Latency_{preprocessing}$, which is expected since nearly no preprocessing steps are done on the edge except building a \emph{PredictRequest} object for TensorFlow Serving.
Preprocessing an $224^2/299^2$ image on the edge instead of the cloud causes $20$ times ($1.9/39.8$ms) higher $Latency_{preprocessing}$ and an increase of factor $11$ ($37.9/431.1$ms) for an $16$MP image.
The impact of larger image sizes on memory in the case of cloud preprocessing is marginal, especially in comparison for the edge preprocessing counterparts. This is expected, since only compressed \emph{PNG} images are loaded into memory, in contrary to edge preprocessing, where in addition to the decoded \emph{PNG} images the resized images are also loaded to memory simultaneously.
While $Memory_{preprocessing}$ is lower for cloud preprocessing, the difference is nowhere as substantial as the latency differences. $16$MP images need $1.3$ times ($137.7/179.6$MB) more memory if preprocessing on the edge instead on the cloud.
%%Mention CPU

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Latency.pdf}
\caption[Cloud Inference:  $Latency_{preprocessing}$ - lower is better]{Cloud Inference:  $Latency_{preprocessing}$ - lower is better - Cloud preprocessing is significantly faster than edge preprocessing for all image sizes.}
\label{fig:cloudInferencePreproLat}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Memory.pdf}
\caption[Cloud Inference:  $Memory_{preprocessing}$ - lower is better]{Cloud Inference:  $Memory_{preprocessing}$ - lower is better - The larger the image, the larger the difference between the preprocessing methods for cloud inference.}
\label{fig:cloudInferencePreproMemory}
\end{figure}

\FloatBarrier
\subsubsection{Inference}
This section focuses on performance differences between the models and the impact of cloud inference with cloud preprocessing on cloud inference.
The inference metrics for cloud preprocessing include inference and the preprocessing steps, which are done at the cloud.

%%Also includes preprocessing!!!!
\paragraph{$\mathbf{Memory_{inference}}$}
Looking at the memory consumption during inference in figure \ref{fig:cloudInferenceInferenceMemory} one can see that $Memory_{inference}$ is very stable across all image sizes for edge preprocessing, which logical, since all image have been preprocessed to the same shape ($224^2/299^2$), therefore all requests sent to TensorFlow Serving have the same size.
MobileNetV2 uses $10.5$MB less memory on average than InceptionV4, because of its smaller model input size.

For cloud preprocessing the memory consumption increases for increasing image sizes, as larger images are being sent to the server, thus larger images are loaded into the \emph{PredictRequest} object, that is being sent to the  TensorFlow Serving instance at the cloud-backend.
$299^2/224^2$ images consume $123.8/112.34$MB, while $16$MP images $140.1/133.76$, therefore $Memory_{inference}$ increases by $19/13\%$ for InceptionV4/MobileNetV2 respectively.
%%add factor here
There are no $Memory_{inference}$  differences between the two models, since un-preprocessed images are being sent, except for the $224^2/299^2$ images, where the smaller $224^2$ image consumes $7.5$MB less memory on average.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Memory.pdf}
\caption[Cloud Inference:  $Memory_{inference}$ - lower is better]{Cloud Inference:  $Memory_{inference}$ - lower is better - Cloud preprocessing causes higher memory consumption during inference during larger payloads being sent the TensorFlow Serving cloud-backend.}
\label{fig:cloudInferenceInferenceMemory}
\end{figure}
\paragraph{$\mathbf{Latency_{inference}}$}
The $Latency_{inference}$ results can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
Similar to the memory results, the latency for edge preprocessing stays the same across all image sizes, since all image sizes are preprocessed on the edge beforehand. 
Mean latencies are $181.2$ms and $108.46$ms for InceptionV4 and MobileNetV2 respectively, therefore MobileNetV2 is $1.7$ times faster.
Figure \ref{fig:CloudInferenceRatioEdgetotal} displays the share of $Latency_{network}$ and $Latency_{server}$ in $Latency_{inference}$ and one can see that the network is accountable for about $20\%$ of the  inference latency for InceptionV4 and $30\%$ for MobileNetV2, independent of image size.

If the images are preprocessed on the cloud, the preprocessing step on the cloud has an significant impact on $Latency_{inference}$, as can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
While an $224^2/299^2$ image has an latency of $95.8/64.5$ms for InceptionV4/MobileNetV2, a $16$MP image needs $21.9/17.8$ times longer with a $1702.3/1414.8$ms mean latency.
This increase can be observed across other images sizes as well for the same extent. 
Looking at  the share of $Latency_{network}$ in $Latency_{inference}$ in figure \ref{fig:CloudInferenceratioCloudtotal}, the network takes up to $50\%$ of the inference latency for $224^2/299^2$ images, but shrinks to less than $15\%$ for all remaining image sizes.
Therefore it can be stated that the network is not reason for the increase in latency for the larger image sizes, but rather the preprocessing done on the cloud-backend by TensorFlow Serving.
We think Tensorflow's \emph{resize\_bilinear} function, which we use to resize the images, causes the bottleneck in the preprocessing step on the cloud.
%%add GPU kernel optimisations issues
The use of a other resize approach like nearest-neighbor could speed up preprocessing, but would probably have an impact on the accuracy of the predictions.
Note that while the network connection in our experimentation environment is fast enough too prevent any bottlenecks caused by the network, a slower network connection could slow down inference for large image sizes.
%warum preprocessing s langsam bei tensorflow serving?

%%cloud preprocessing bigger images higher variance
Comparing the two cloud inference options, edge and cloud preprocessing,  $Latency_{inference}$ values for edge preprocessing of both models are faster for all image sizes except the $224^2/299^2$ images.
We believe the $224^2/299^2$ images are faster because of the lower I/O overhead in comparison to the larger preprocessed counterparts, even if the un-preprocessed images still have the decoded and normalized.

\paragraph{$\mathbf{Latency_{total}}$}
This stays also true for $Latency_{total}$ (see figures \ref{fig:CloudInference+PreproCloud} and \ref{fig:CloudInference+PreproEdge}), which includes both $Latency_{preprocessing}$ and $Latency_{inference}$. 
For $224^2/299^2$ images need $223.1/147.2$ms $Latency_{total}$ for InceptionV4/MobileNetV2 for edge preprocessing, while cloud preprocessing needs only $96.9/64.3$ms, therefore cloud preprocessing is $2.3$ times faster.
In contrast cloud inference with edge preprocessing is faster by a factor of $2.7$ for $16$MP images, with the latencies of InceptionV4/MobileNetV2 being $626.7/532$ms for edge preprocessing and $1673.4/1453.9$ms for cloud preprocessing. $2$MP images are $1.7/1.8$ times faster when using edge preprocessing, $4$MP $2.1/2.2$ and $8$MP $2.6/3$ times faster.
%latency edge vs cloud prepro inference:

While MobileNetV2 is $1.75$ times faster in $Latency_{inference}$ for edge preprocessing than InceptionV4, this latency difference shrinks for cloud preprocessing for larger images, started by a difference of factor $1.52$ for $224^2/299^2$ images and shrinking to $1.25/1.21/1.09/1.15$ for the respective image sizes $2/4/8/16$MP.
We believe this decrease is due to the fact as images for MobileNetV2 need to be resized to a smaller input size than InceptionV4, hence increasing the already high resize overhead and thus narrowing the latency difference between both models.
When comparing memory consumption of cloud inference with ether cloud preprocessing or edge preprocessing, the difference is only $3\%$ for  $224^2/299^2$ images ($121.5$MB for edge preprocessing and $118.1$Mb for cloud preprocessing), but $14\%$ for $16$MP images (edge preprocessing $120.0$Mb, cloud preprocessing $136.9$MB).

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Latency.pdf}
\caption[Cloud Inference:  $Latency_{inference}$ - lower is better]{Cloud Inference:  $Latency_{inference}$ - lower is better - Cloud preprocessing is faster for $224^2/299^2$ images, but slower for all other image sizes due to cost of resizing.}
\label{fig:cloudInferenceInferenceLatency}
\end{figure}



\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_cloudprepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInferenceratioCloudtotal} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_edgeprepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInferenceRatioEdgetotal}
\end{subfigure}

\caption[Cloud Inference:  $Latency_{inference}$ including $Latency_{network}$ and $Latency_{server}$ - lower is better]{Cloud Inference:  $Latency_{inference}$ including $Latency_{network}$ and $Latency_{server}$ - lower is better - 
Share of $Latency_{network}$ in $Latency_{inference}$ does not significantly increase across the images sizes, thus being not the reason for performance drop off of cloud inference with cloud preprocessing. For Edge preprocessing the network accounts up to $30\%$ of the inference latency in case of MobileNetV2.}
\end{figure}

\begin{comment}


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_ratio_server_total_latency_(cloud_prepro).pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInferenceratioCloudrel} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_ratio_server_total_latency_(edge_prepro).pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInferenceRatioEdgerel}
\end{subfigure}

\caption{Cloud Inference:  Ratio between $Latency_{network}$ and $Latency_{server}$ - lower is better}
\end{figure}
\end{comment}


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_cloud_prepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInference+PreproCloud} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_edge_prepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInference+PreproEdge}
\end{subfigure}

\caption[Cloud Inference:  $Latency_{total}$  - lower is better]{Cloud Inference:  $Latency_{total}$ ($Latency_{preprocessing}+Latency_{inference}$) - lower is better - 
Cloud preprocessing performs only better than cloud inference with edge preprocessing for $224^2/299^2$ images, all other image sizes perform worse.}
\end{figure}

\paragraph{$\mathbf{Data_{transmitted}}$ \& $\mathbf{Data_{received}}$}
Figures \ref{fig:CloudInferenceReceivedData} and \ref{fig:CloudInferenceTransmittedData} show the data transmitted and received from the edge client to the cloud-backend server for the different image sizes and preprocessing options.
For edge processed image the mean transmitted data is $1061.9/608.7$KB for InceptionV4/MobileNetV2 and $16.7/10.3$KB data received by the respective models.
Therefore $299^2$ images require $453.2$ kilobytes more than $224^2$ images.
For cloud preprocessing $Data_{transmitted}$ the amount of data sent is at maximum $100$KB larger than the sizes of the \emph{PNG} images ($224^2$: $83$KB, $299^2$: $141$KB, $2$MP: $2411$KB, $4$MP: $4309$KB, $8$MP: $7515$KB,  $16$MP: $10077$KB).
For both edge and cloud preprocessing the $Data_{received}$ rises for rising $Data_{transmitted}$ values, since the communication between server and client is done via TCP, thus more sent data results in more packages resulting in more ACK signals getting sent back to the client.
%%NR:[114.5] transmitted
%%NR:[3.] received
%%2MP:[2446.2] transmitted
%%2MP:[38.2] received
%%4MP:[4365.5] transmitted
%%4MP:[64.8] received
%%8MP:[7603.6] transmitted
%%8MP:[114.9] received
%%16MP:[10176.8] transmitted
%%16MP:[141.4] received
%299 png kleiner als 299 preporcessing
%2MP($1732\times1155$, $2411$KB), 4MP($2449\times1633$, $4309$KB), 8MP($3464\times2309$, $7515$KB) and 16MP($4899\times3266$, $10077$KB) ($224\times224$, $83$KB or $299\times299$, $141$KB


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Received_Data.pdf}
   \caption{$Data_{received}$}
   \label{fig:CloudInferenceReceivedData} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Transmitted_Data.pdf}
   \caption{$Data_{transmitted}$}
   \label{fig:CloudInferenceTransmittedData}
\end{subfigure}

\caption[Cloud Inference:  $Data_{received}$ vs. $Data_{transmitted}$ - lower is better]{Cloud Inference:  $Data_{received}$ vs. $Data_{transmitted}$ - lower is better - 
With rising image size the data transmitted for cloud preprocessing rises as un-preprocessed images get larger.
For edge preprocessing the amount of data transmitted is constant across images sizes, as all preprocessed images are the same shape.
$Data_{received}$ is increasing along with $Data_{transmitted}$, since communication is done via TCP, thus more sent packages result in more acknowledgements from the server.
}
\end{figure}

\FloatBarrier
\paragraph{Cloud Inference - Key Takeaways}
%cloud prepro less preprocessing memory and latency
%cloud prepro more inference memory
%cloud preprocessing very slow for large image sizes
%edge prepro faster for all image sizes except 224/299
%network not a bottlneck
%resize binlinear bad implementation?
\emph{
\begin{itemize}
    \item Preprocessing on the cloud in comparison to edge preprocessing causes:
    \begin{itemize}
        \item $20\times$ faster $Latency_{preprocessing}$ for $224^2/299^2$ images
        \item $10\times$ faster $Latency_{preprocessing}$ for $16$MP images
        \item $1.28\times$ less $Memory_{preprocessing}$ for $16$MP images
    \end{itemize}
    \item Inference on the cloud including preprocessing in comparison to cloud inference without preprocessing causes:
    \begin{itemize}
        \item $2.3\times$ faster InceptionV4/MobileNetV2 $Latency_{total}$ for $224^2/299^2$ images
        \item $2.7\times$ slower InceptionV4/MobileNetV2 $Latency_{total}$ for $16$MP images
        \item $0.97\times$ less $Memory_{inference}$ for $224^2/299^2$ images
        \item $1.14\times$ more $Memory_{inference}$ for $16$MP images
    \end{itemize}
    \item MobileNetV2 is up to $1.75\times$ faster than InceptionV4 for smaller images, but shrinking to around $10\%$ faster for large image in case of cloud preprocessing.
    \item Network is accountable for up to $50\%$ of $Latency_{inference}$ for $224^2/299^2$ images and networks, but less than $30\%$ for larger images and network for both cloud and edge preprocessing.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Edge Preprocessing has faster $Latency_{total}$ latencies for all image sizes except $224^2/299^2$.
\end{itemize}
}%emph end


\subsection{Edge vs. Cloud Inference}
Previous two sections presented the results for both edge and cloud inference.
Now, the results of the cloud and the edge inference with batch size one, including preprocessing, are compared against each other.
First preprocessing and inference are evaluated separately and afterwards both of the steps combined. 
Each plot of this section contains five subplots, one for each image sizes, the x-axis contains the deep learning model, while the y-axis displays the various performance metrics. 
The legend used in all those plots in explained in table \ref{table:legendPlots}.
\begin{table}[!htb]
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}
\centering
\caption{Explanation of the plot legends}
\label{table:legendPlots}
\begin{tabular}{@{}lll@{}}
\toprule
Inference on & Description & Color \\ \midrule
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:CLOUD\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ cloud-backend.\end{tabular} &  \crule[gruen]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference is done on cloud-backend, but images are \\ preprocessed on edge beforehand.\end{tabular} &  \crule[orangedunkel]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:EDGE;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ edge.\end{tabular} & \crule[lila]{0.8cm}{0.8cm} \\ \bottomrule
\end{tabular}
\end{table}

Not that for the quantized MobileNetV2 we only conduct edge inference experiments, thus no cloud inference results for this model can be seen in the plots of this section.
\subsubsection{Preprocessing}

Figures \ref{fig:EdgeVsCloudPreproMemory}, \ref{fig:EdgeVsCloudPreproLat} and \ref{fig:CloudEdgePreproCPU} report the preprocessing metrics $Memory_{preprocessing}$, $Latency_{preprocessing}$ and $CPU_{preprocessing}$.

%Memory
\paragraph{$\mathbf{Memory_{preprocessing}}$}
Both edge preprocessing options, for either edge inference or cloud inference, have very similar memory consumption results with the cloud inference mean values being at most $5.5\%$ higher from the edge inference values for all image sizes.
This due to the fact that both options share most of the preprocessing steps, the only difference is that in case of cloud inference with edge preprocessing the \emph{ByteBuffer}, containing the preprocessed image, gets used to build the \emph{PredictRequest} for TensorFlow Serving. 
For edge inference no such step is needed as the \emph{ByteBuffer} can be directly be used to call the run function of the TensorFlow Lite interpreter, which starts the inference process at the edge.
Thus the $5.5\%$ overhead in memory can be explained by the additional \emph{PredictRequest} object that has to loaded into memory for cloud inference.

In case of small images the memory consumption of cloud preprocessing is very close to its edge preprocessing counterpart, the larger the image the larger the memory difference in favor of cloud preprocessing.
While the difference is only $1.9\%$ for an $224^2/299^2$ image ($114.53/116.69$MB), the difference grows to an $22.5\%$ decrease ($177.8/137.8$MB) in memory for $16$MP images ($2$MP: $1.1\%$, $4$MP: $7.9\%$, $8$MP: $9.9\%$).
This difference is caused by the fact that for cloud preprocessing only the compressed \emph{PNG} gets loaded into memory, which is considerable smaller than the decoded image, especially for large images such as the $16$MP image.

When comparing the different models InceptionV4 ($140.1$MB) consumed about $7.9$MB more $Memory_{preprocessing}$ than MobileNetV2 ($132.2$MB), or about $6\%$, independent of image size or inference mode (edge inference, cloud inference with edge preprocessing and cloud inference with cloud preprocessing).
Edge inference of the quantized version of MobileNetV2 causes the same memory consumption as the non quantized version.
When differentiating between the different image sizes and inference modes the difference is always between $4-8\%$, except for cloud preprocessing of an $224^2/299^2$ image, where the difference rises to $10\%$ ($11.35$MB). This is due to the fact that MobileNetV2 preprocesses $224^2$ images and InceptionV4 $299^2$ images.

%Latency
\paragraph{$\mathbf{Latency_{preprocessing}}$}
The preprocessing latency results are similar to the memory results. Both edge preprocessing options are very similar in performance, while cloud preprocessing performs better, but in case of latency the difference is larger.

The difference between the edge preprocessing options (cloud inference with edge preprocessing and edge preprocessing) is at most $4.9\%$ ($5.1$ms) across all images sizes (not differentiating between the different models).

When comparing edge preprocessing to cloud preprocessing the latency gap for $224^2/299^2$ images is a factor of $20$ in the favor of cloud preprocessing ($1.9/38.2$ms). For larger image sizes this gap shrinks, but still amounts $11\times$ for $16$MP images ($37.9/430.78$ms).

The different models have little to no impact on the preprocessing when comparing same images sizes.
For cloud preprocessing the MobileNetV2 and InceptionV4 differ at most $4.9$ms (for $8$MP, all other image size are below that for both relative and total difference). This difference is not significant since the standard deviation for the mean values is more than twice as high for both models.
%%difference between models for edge preporcessing
%%%model compare
%cloud prepro no diff for models
%%diff for NR large for edge prepro -> normalizing more work
This is also true for edge preprocessing for all image sizes, regardless of edge or cloud inference.
Although the difference between the models is up to $29\%$ or $10.3$ms ($224^2/299^2$ image), the speedup is not significant as the standard deviation for both models is above $9$ms.

%CPU
\paragraph{$\mathbf{CPU_{preprocessing}}$}
Looking at the CPU usages in figure \ref{fig:CloudEdgePreproCPU} it can be seen that the variance of all inference modes, models and images sizes is too large to make conclusions, although there is a indication that cloud preprocessing causes lower usages.
Overall it can be stated that all usages are around $12.5\%$. This makes sense since the preprocessing of a single image is done on a single thread, thus running on one core and since the OnePlus 6T has 8 cores, the full utilization of a single core causes an overall $CPU_{preprocessing}$  usage of $12.5\%$. 
This infers that the preprocessing uses $100\%$ of a single core, especially for large images.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Memory.pdf}
\caption{Edge vs. Cloud Inference:  $Memory_{preprocessing}$ - lower is better}
\label{fig:EdgeVsCloudPreproMemory}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Latencies.pdf}
\caption{Edge vs. Cloud Inference:  $Latency_{preprocessing}$ - lower is better}
\label{fig:EdgeVsCloudPreproLat}
\end{figure}

\FloatBarrier
\subsubsection{Inference}
\paragraph{$\mathbf{Memory_{inference}}$}
Figure \ref{fig:EdgeVsCloudInferenceMemory} reports $Memory_{inference}$ for all inference modes, models and image sizes.
Image size has no impact on the memory consumption during inference, where image have been previously preprocessed at the edge, which is logical, since the preprocessed image have the same memory constraints regardless of the size before preprocessing.
In contrary cloud inference with cloud preprocessing memory consumption is impacted by different image sizes, since the images are not preprocessed and thus larger images are being sent to the server.
$16$MP images use $19\%$ more memory for InceptionV4 in comparison to a $224^2/299^2$ image and $13\%$ more for MobileNetV2.


While different images sizes only have a small impact on $Memory_{inference}$ different inference modes (edge inference, cloud inference with either edge or cloud preprocessing) do have considerable performance differences, especially in the case of the large InceptionV4 network.
Edge Inference with InceptionV4 consumes more than twice ($2.2\times$) as much memory than both cloud inference options.

While cloud inference for InceptionV4 can half memory consumption, MobileNetV2 does not profit nearly as much from cloud inference, as edge inference with MobileNetV2 only uses $10.5\%$ ($13.6$MB) more an average (independent of image size) than cloud inference with edge preprocessing due to its small model size.
Edge inference with MobileNetV2 uses less memory than cloud inference with cloud preprocessing for $16$MP images, as the un-preprocessed image consumes more memory than the loaded model. 

The quantization of MobileNetV2 has no significant impact on memory consumption.


%inception on egde much more, mobilenet nearly equal (more for small images, less forlarge images than cloud prepro)
\paragraph{$\mathbf{Latency_{inference}}$}
Results for $Latency_{inference}$ can be seen in figures \ref{fig:EdgeVsCloudInferenceLat} and \ref{fig:EdgeVsCloudInferenceLatNR}, while the latter only contains the latency results for the $224^2/299^2$ size for better readability and the first one for all images sizes.
These figures show the substantial difference in latency between the different deployment options, models and image sizes.
Recall that the cloud inference results include the network latency $Latency_{network}$, because this factor would also be present in real world AI applications.  In our case we have a high speed connection with low network latency, thus being a lower bound for real-time AI applications. 
The same plot without $Latency_{network}$ can be seen in figure \ref{fig:CloudEdgeInfLatWONetwork}.

For InceptionV4 and $224^2/299^2$ images cloud inference with cloud preprocessing is the fastest inference method with a mean of $94.9$ms, followed by cloud inference with edge inference ($180.4$ms) and cloud inference with edge preprocessing ($235.9$ms).
Therefore cloud inference with cloud preprocessing is $1.9$ times faster than cloud inference with edge preprocessing and $2.5$ times faster than edge inference, in case of InceptionV4.
We believe this difference between the two cloud inference options is due to lower I/O overhead as well as smaller network latency in case of cloud preprocessing, since the compressed $224^2/299^2$ \emph{PNG} images are smaller in size than the decoded counterparts. 

In contrary in the case of MobileNetV2 edge inference is the fastest option for $224^2/299^2$ images with a mean of $27.7$ms, followed by cloud inference with cloud preprocessing ($62.5$ms) and lastly cloud inference with edge preprocessing ($110.2$ms).
Hence edge inference of MobileNetV2 is $2.3$ times faster than cloud inference with cloud preprocessing and $4$ times faster than cloud inference with edge preprocessing.
When neglecting the $Latency_{network}$ part in $Latency_{inference}$, cloud inference with cloud preprocessing is nearly as fast as edge inference with MobileNetV2 with a difference of $0.26$ms.

Therefore the fastest $224^2/299^2$ image option for MobileNetV2 is the edge inference option, while for InceptionV4 cloud inference is the optimal choice.

The quantized version of MobileNetV2 has a mean inference latency of $11.7$ms, thus being $1.9$ times faster than edge inference of non quantized MobileNetV2 and $8.1$ times faster than the best InceptionV4 result.

While the various images sizes have no impact on inference methods, where images have been previously preprocessing on the edge, they have severe impact on cloud inference with cloud preprocessing.
With the average latencies for InceptionV4/MobileNetV2 for an $16$MP images being $1633.9/1417.6$ms, the difference to a $224^2/299^2$ image is more than factor $17/22$.
%%standard deviation larger for cloud preprocessing of large images

\paragraph{$\mathbf{CPU_{inference}}$}
%not interesting
Figure \ref{fig:EdgeVsCloudInferenceCPU} shows that there are no severe differences in CPU usage between edge and cloud inference, regardless of model and image size, at least for the case of batch size of one.
All usages are below $12.5\%$, therefore no core is utilized fully, since the OnePlus 6T used in our experiments has $8$ cores.


\paragraph{$\mathbf{Throughput_{inference}}$}
Since $Throughput_{inference}$ is directly computed from $Latency_{inference}$, the results for this metric in figure \ref{fig:EdgeVsCloudinferneceThroughput} have the same tendencies as the latencies.
Hence cloud inference with cloud preprocessing is the best inference option for InceptionV4, at least in the case of $224^2/299^2$ images with a mean throughput of $10.8$ predictions per second.
Meanwhile for MobileNetV2 reaches up to $36.6$ predictions per second, when inference is done at the edge.
The quantized version of MobileNetV2 can make up to $83.7$ predictions per second.

While $Throughput_{inference}$ stays constant across all images sizes for edge preprocessing options, cloud inference with cloud preprocessing is impacted severely with a drop to $0.6$ predictions per second for the case of InceptionV4, $16$MP.


\paragraph{$\mathbf{Throughput_{total}}$}
So far we only evaluated the results of preprocessing and inference separately, but especially for the metrics throughput and latency the sum of both preprocessing and inference is of interest.%,since 
When looking at the $Throughput_{total}$, which is derived from the sum of $Latency_{preprocessing}$ and $Latency_{inference}$ in figure \ref{fig:EdgeVsCloudTotalThroughput} the impact of preprocessing on the throughput can be seen.
While this impact is small for cloud inference with cloud preprocessing, because $Latency_{preprocessing}$ values for this option are very small, the impact is significant for inference methods with edge preprocessing.
But even when accounting for both preprocessing and inference, cloud inference with cloud preprocessing performs way worse for large image sizes than both edge preprocessing methods.

First we evaluate the results for the $224^2/299^2$ images.
When including preprocessing  the throughput of MobileNetV2 on the edge drops from $36.6$ to $16.1$, since preprocessing takes longer than the actual inference, even for a image where no resizing has to be done.
which is very close to the best cloud inference option for this model ($15.7$).
For the quantized version of MobileNetV2 the preprocessing latency is the same as for the non quantized counterpart, but the lower inference latency leads to a total throughput of $23.1$.
The highest $Throughput_{total}$ for InceptionV4 is $10.4$, which is achieved by cloud inference with cloud preprocessing.

When analyzing the larger images sizes preprocessing takes a significant toll on all deployment options, as throughput of MobileNetV2/InceptionV4 drops to $7.8/3.6$, $5.3/2.8$, $3.3/2.2$, $2.1/1.6$ for the image sizes $2$MP, $4$MP, $8$MP and $16$MP respectively (reporting the values for the best inference option for each image size).
Even tough cloud inference with preprocessing has the lowest preprocessing latency, the remain the worst inference option except for the $224^2/299^2$ images.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Memory.pdf}
\caption[{Edge vs. Cloud Inference:  $Memory_{inference}$ - lower is better}]{Edge vs. Cloud Inference:  $Memory_{inference}$ - lower is better - InceptionV4 has significant impact on memory consumption in case of edge inference, while MobileNetV2 small size has very little impact.}
\label{fig:EdgeVsCloudInferenceMemory}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies_onlyNR.pdf}
\caption[Edge vs. Cloud Inference:  $Latency_{inference}$ of $224^2/299^2$ images - lower is better]{Edge vs. Cloud Inference:  $Latency_{inference}$ of $224^2/299^2$ images - lower is better - For InceptionV4 cloud inference is the fastest option, while for MobileNetV2 edge inference is the preferred option.}
\label{fig:EdgeVsCloudInferenceLatNR}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies.pdf}
\caption{Edge vs. Cloud Inference:  $Latency_{inference}$ - lower is better}
\label{fig:EdgeVsCloudInferenceLat}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_CPU.pdf}
\caption[Edge vs. Cloud Inference:  $CPU_{inference}$ - lower is better]{Edge vs. Cloud Inference:  $CPU_{inference}$ - lower is better}
\label{fig:EdgeVsCloudInferenceCPU}
\end{figure}


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Throughput_without_Preprocessing.pdf}
\caption{Edge vs. Cloud Inference:  $Throughput_{inference}$ - higher is better}
\label{fig:EdgeVsCloudinferneceThroughput}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Throughput_with_Preprocessing.pdf}
\caption{Edge vs. Cloud Inference:  $Throughput_{total}$ (Inference + Preprocessing) - higher is better}
\label{fig:EdgeVsCloudTotalThroughput}
\end{figure}




Concluding it can be stated that the edge is the preferred deployment option for MobileNetV2, both quantized and non quantized versions, in respect to latency/throughput as well as memory consumption.
The impact of I/O and network latency added by cloud inference is not feasible for these small models.

While the edge is the preferred for small models like MobileNetV2, InceptionV4's architecture is too demanding for the current edge devices, thus cloud inference is the better option for these large models.
Cloud preprocessing is only viable for the small $224^2/299^2$ images, as it lowers I/O costs and data consumption. For all other images sizes it increases $Latency_{total}$, $Data_{transmitted}$ and $Memory_{inference}$, while only having little positive effect on $Memory_{preprocessing}$ compared to the other inference options.

In general it can be stated that the impact of preprocessing is big, even for small images sizes, thus the preprocessing should be minimized as much as possible.

\FloatBarrier
\paragraph{Edge vs. Cloud Inference - Key Takeaways}
\emph{
\subparagraph*{Preprocessing}
\begin{itemize}
    \item Both edge preprocessing methods for either edge or cloud inference are very similar in preprocessing performance.
    \item No major preprocessing performances difference between different model types.
    \item Cloud inference with cloud preprocessing compared to edge preprocessing options causes
    \begin{itemize}
        \item  $1.9\%$ for NR, $22.5\%$ for 16MP less $Memory_{preprocessing}$ 
        \item $20\times$ for NR, $11\times$ for 16MP faster $Latency_{preprocessing}$ 
        \item No significant differences in $CPU_{preprocessing}$
    \end{itemize}
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Preprocessing cost should be minimized as much as possible.
\end{itemize}
\subparagraph*{Inference}
\begin{itemize}
    \item Different images sizes only impact the inference performance of cloud inference with cloud preprocessing.
    \item Edge inferences compared to cloud inference causes:
    \begin{itemize}
        \item  $2.2\times$ higher $Memory_{inference}$ for InceptionV4, for MobileNetV2 only $10.5\%$ increase (also quantized).
        \item $1.9\times$ higher $latency_{inference}$ for Inception, $2.3\times$ faster for MobileNetV2.
        \item No significant difference in $CPU_{inference}$.
    \end{itemize}
    \item Best inference option for InceptionV4 can achieve a $Throughput_{inference}$ of $10.8$ (cloud inference with cloud preprocessing), MobileNetV2 $36.6$ (edge inference) and MobileNetV2 quantized $83.7$ (edge inference) for $224^2/299^2$ images.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Edge inference performs better for small models, cloud inference performs better for large models.
 \item Highest $Throughput_{total}$ for InceptionV4, including both preprocessing and inference, is $10.4$ (cloud inference with cloud preprocessing), for MobileNetV2 $16.1$ (edge inference) and for MobileNetV2 quantized $23.1$ (edge inference).
\end{itemize}
}

\subsubsection{Effect of larger Batch Sizes}
\label{chap:resultsBatchSize}
So far we only reported the results of the of the experiments with batch size of one, but in this section we present the results for the batch sizes $1$, $2$, $16$, $32$.
Increasing the batch size  is supposed to increase throughput at the expense of low latencies.
Since the TensorFlow Lite of the used MobileNetV2 version does not allow batch sizes larger than one, we can not report edge inference results for these models.

Since the batch size is added as a new dimension in this section, the plots visualizing the results need to be expanded.
Now the x-axis represents the various batch sizes and the models are moved to the legend. 
The legend is similar to table \ref{table:legendPlots}, but the model is added as an additional feature to the inference mode.
\subsubsection{Preprocessing}

\paragraph{$\mathbf{CPU_{preprocessing}}$}
While the CPU usages for batch sizes of one were not different for the different deployment options, for larger batch sizes this changes, since in case of edge preprocessing the parallelism comes into play.
Recall that for batch sizes larger than one images get resized and normalized in parallel in the case of edge preprocessing to increase throughput (for implementation details refer to section \ref{chap:preproImpl}).

For rising images size and batch size the CPU usages rise up to $92\%$, as can be seen in figure \ref{fig:BatchSizePreproCPU}, hence all core are utilized closed to the fullest.


While CPU usages of the cloud preprocessing options also rise for increasing image/batch sizes, they stagnate at below $20\%$.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_CPU_Usage.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $CPU_{preprocessing}$ - lower is better}
\label{fig:BatchSizePreproCPU}
\end{figure}

\paragraph{$\mathbf{Memory_{preprocessing}}$}
While the memory consumption results pictured in figure \ref{fig:BatchSizePreproMemory} of all deployment options stay close together for the small images sizes over the difference batch sizes, the larger the image gets, the further the gap between the cloud and edge preprocessing methods becomes.
This growing gap is explained by the fact that for cloud preprocessing only encoded \emph{PNG} images need to be loaded, but for edge preprocessing these images needed to be encoded, hence more memory is needed.
As the batch size and images size increases, the gap between decoded and encoded images grows, hence explaining the gap between the preprocessing methods.

The gap for a batch size of $32$ is $76.6$MB ($1.56\times$ increase) for $224^2/299^2$ images, $133.8$MB ($0.74\times$ increase) for $4$MP images and $842.2$MB ($2.88\times$ increase) for $16$MP images. 
A similar progression can be observed for the other batch sizes (e.g. batch size $16$: $36.5$MB ($224^2/299^2$), $134.4$MB ($4$MP), $674.6$MB ($16$MP)).

For cloud preprocessing there is no difference in memory consumption between the difference models, but for edge preprocessing InceptionV4 tends to use more memory, because of the models larger input size (InceptionV4 $299^2$, MobileNetV2 $224^2$).


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Memory.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Memory_{preprocessing}$ - lower is better}
\label{fig:BatchSizePreproMemory}
\end{figure}

\paragraph{$\mathbf{Latency_{preprocessing}}$}
Figure \ref{fig:BatchSizePreproLatency} shows the progression of preprocessing latency results across the various image and batch sizes.
Similar to the $Memory_{preprocessing}$ results, there is no significant difference between the different models, but large differences between the preprocessing methods for increasing image and batch sizes.

If preprocessing $16$ megapixel images at the edge, it takes $1.1$ times longer to preprocess $2$ images instead of $1$, $2.9$ times longer to preprocess $16$ images instead of two and $1.96$ times longer to preprocess $32$ images instead of $16$.
This illustrates the effect of the parallel preprocessing.

$Latency_{preprocessing}$ for cloud preprocessing haves nearly linear, as $2$ $16$MP images take $1.85$ times longer than $1$ image does, $16$ $17.1$ times longer and $32$ images $34.7$ times longer.
This is explained by the fact that in case of cloud preprocessing all images only need to be concatenated consecutively before getting used to build the \emph{PredictRequest} for TensorFlow Serving. 


While Cloud preprocessing takes $12.8$ms to preprocess $32$ $224^2/299^2$ images, edge preprocessing takes $197.1$ms, hence an increase of $184.3$ms or factor $15.4$. For $4$MP images using the same batch size this gap grows to $556.9$ms (factor $2$), and for $16$MP images to $1375.3$ms (factor $2$).
For batch size $16$ the gap grows as follows: $92.3$ms ($224^2/299^2$), $297.6$ms ($4$MP), $724$ms ($16$MP).


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Latencies.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Latency_{preprocessing}$ - lower is better}
\label{fig:BatchSizePreproLatency}
\end{figure}



\FloatBarrier
\subsubsection{Inference}

\paragraph{$\mathbf{CPU_{inference}}$}
Looking at the CPU usages in figure \ref{fig:BatchSizeInferenceCPU} the three different deployment options can be clearly identified for the larger image sizes.
While for a batch size of one the CPU usages was of little interest, since there weren't any major difference between the different deployment options, this changes for larger batch sizes.

The option with the highest usage is edge inference $CPU_{inference}$ does not grow for a batch size of $2$, but it grows to $92\%$ for batch size $16$ and stagnates at this value for the larger $32$ batch.
%%CPU NNAPI hier nennen?

The two other options only consume a fraction of the edge inference usage for the batch sizes $16$ and $32$.
CPU usage of Cloud inference with cloud preprocessing continuously grow over the course of larger images up to $18\%$ usages for the larger batch sizes starting at the $4$MP images.

Cloud inference with edge preprocessing consumes nearly as much as cloud preprocessing for the images sizes $224^2/299^2$ and $2$MP, but starting to use less from $4$Mp images onwards.
The usages stay relatively consistently at around $8\%$ across all image sizes, as image have been preprocessed into the same shape before.

Cloud inference with cloud preprocessing uses more CPU than cloud inference with edge preprocessing, since the \emph{PredictRequest}, that is being sent to the TensorFlow Serving cloud-backend (see section \ref{chap:CloudInfImpl}) and contains the preprocessed or un-preprocessed images depending on deployment option, is larger in the case of cloud preprocessing.
Therefore the gRPC client, which our implementation of TensorFlow Serving uses, has to sent requests with larger payloads to the server, resulting in higher CPU usages than smaller payloads like in the case of edge preprocessing would.


Therefore it can be stated that cloud inference consumes way less CPU resources for large batch and image sizes.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_CPU_Usage.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $CPU_{inference}$ - lower is better}
\label{fig:BatchSizeInferenceCPU}
\end{figure}

\paragraph{$\mathbf{Memory_{inference}}$}
The memory consumption of inference with larger batches is very similar to the behavior of CPU usage.


Edge inference consumes substantially more than the cloud inference counterparts with InceptionV4 consuming $1289$MB for a batch size of $16$ and $2132.3$MB for batch size $32$.
The value for a batch size of $32$ is therefore more than $3$ times higher than the highest cloud inference result.
%Larger batch sizes affect the memory results of edge inferences significantly more than the cloud inference results.
%The worst cloud inference results for batch sizes $16$ and $32$ are still $x/s$ times better than the edge inference results.

Cloud preprocessing again uses more than cloud inference with edge preprocessing, since the preprocessed images are smaller in sizes (except for $224^2/299^2$) images than the large un-preprocessed \emph{PNG} images.




\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Memory.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Memory_{inference}$ - lower is better}
\label{fig:BatchSizeInferenceMemory}
\end{figure}


\paragraph{$\mathbf{Latency_{inference}}$}
Figure \ref{fig:BatchSizeInferenceLatency} reports the $Latency_{inference}$ results for the difference inference modes, batch and image sizes.

For a detailed view with more readability of the cloud inference results for the $224^2/299^2$ images please refer to figure \ref{fig:BatchSizeLatenciesCloud}.

Inference modes with edge preprocessing again have the same performance across all image sizes, hence only cloud inference with cloud preprocessing is affected by the various image sizes.

The larger the image size get, the closer the performance of the cloud preprocessing options gets to the edge inference performance

MobileNetV2 cloud inference with cloud preprocessing starts to perform worse than InceptionV4 in case of cloud preprocessing starting from $8$MP images for large batch sizes and even worse than edge inference of InceptionV4 for $16$MP images.
Since images for MobileNetV2 have to be resized to $224\times224$ and for InceptionV4 to $299\times299$, we believe the that the increased cost of resizing the images to $224\times224$ instead of $299\times299$ outweighs the performance advantages gained by the smaller architecture of MobileNetV2.

Plots showing $Latency_{network}$ and $Latency_{server}$ divided can be seen in figures \ref{fig:BatchSizeServer} and \ref{fig:BatchSizeNetwork}.
The variance of $Latency_{network}$ across all images size and batch sizes is relatively high, but even in case of cloud preprocessing with batch size $32$ and a $16$MP images, where in total more than $300$MB are transmitted to the cloud backend (see figure \ref{fig:BatchSizeTransmittedData}), the mean $Latency_{network}$ is below $200$ms. 
Therefore for example in the case of InceptionV4,batch size $32$, $16MP$ images, cloud inference with cloud preprocessing the latency of the network is accountable for less than $1\%$ of the total inference latency, confirming the speed of our network.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Latencies.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Latency_{inference}$ - lower is better}
\label{fig:BatchSizeInferenceLatency}
\end{figure}


\paragraph{$\mathbf{Throughput_{inference}}$}
\ref{fig:BatchSizeInferenceThroughput}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Throughput.pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Throughput_{inference}$ - higher is better - Only cloud inference with cloud preprocessing benefits from larger batch sizes.}
\label{fig:BatchSizeInferenceThroughput}
\end{figure}

\paragraph{$\mathbf{Data_{transmitted}}$ \& $\mathbf{Data_{received}}$}
Figures \ref{fig:BatchSizeReceivedData} and \ref{fig:BatchSizeTransmittedData} shows the data transmitted by the client to TensorFlow Serving on the cloud-backend, and the received data sent back.

As expected, the $Data_{transmitted}$ increases along with increasing batch and images sizes (in case of cloud preprocessing), especially for cloud preprocessing, as the un-preprocessed images are larger in sizes.
The increase of edge preprocessing option across the batch sizes is rather small, therefore not having much impact on network latencies.

$Data_{received}$ also increases with increasing batch sizes and images sizes (in case of cloud preprocessing), since communication between client and server is done via TCP, therefore if more packages are sent to the server, more acknowledgements are being sent back from the server to the client.


\paragraph{$\mathbf{Throughput_{total}}$}
\ref{fig:BatchSizeTotalThroughput}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Total_Throughput_(Preprocessing_+_Inference).pdf}
\caption{Edge vs. Cloud Inference for larger Batch Sizes -  $Throughput_{total}$ - higher is better}
\label{fig:BatchSizeTotalThroughput}
\end{figure}


\FloatBarrier
%%throughput can be increase 

After comparing the results edge and cloud inference of batch sizes larger than one it can be stated that even though edge inference can outperform cloud inference for small model sizes for batch sizes of one, the edge inference performance of large batches significantly drops off as edge devices do currently not have enough computational power for these large batch sizes.

But since computational power is available at the cloud-backend, large batch size increase performance of cloud inference in the form of throughput under the right circumstances, specifically if the inefficient preprocessing steps are minimized as well as I/O costs.
Although this increased throughput comes at the cost of higher latencies, hence a balance between throughput and latency needs to be found that satisfies the requirements of a real-time AI application.

\paragraph{Effect of larger Batch Sizes - Key Takeaways}
\emph{
\subparagraph*{Preprocessing}
\begin{itemize}
    \item B
    \begin{itemize}
        \item  x
\end{itemize}
    \item x
\end{itemize}
\subparagraph*{Inference}
\begin{itemize}
    \item x
    \begin{itemize}
        \item  x
    \end{itemize}
    \item x
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Cloud inference throughput can be increased by increasing batch sizes, but not edge inference throughput
 \item Cloud inference outperforms edge inference for all model types for batch sizes larger than one
 \item Current edge devices not enough computational power for large batch sizes.
\end{itemize}
}


\section{Methodology Evaluation}
%darlegen wie experimente zeigen dass influencing factors wirklich performance beinflussen
In section x we presented a performance model modeling the influencing factors onto the for inference essential performance metrics.
In this section we discuss, if the results presented in the previous section can validate the relations by the proposed performance model.

Recall that the three big influencing factors on inference performance are architecture of the deep learning model, the hardware specifications and the inference framework, that performs the operation defined in the deep learning model architecture on the hardware.
In the following subsections we discuss how each of these factors influencing the inference performance based on the results of ours study

\subsection{Deep Learning Model}
%3 models -Y effect on throughput
We evaluated three difference models, InceptionV4, MobileNetV2 and the quantized version of MobileNetV2.
While the architecture of InceptionV4 has more than $12$ times more parameters than MobileNetV2, the quantized MobileNetV2 has the same amount as the quantized version, but uses $8$-bit floats instead of $32$-bit floats, thus reducing computational complexity.
When looking at the results, the different architectures show significant differences in performance across all tested hardware and framework configurations.

\subsection{Hardware}
While we only evaluated one device for each edge devices and cloud-backends, specifically OnePlus 6T as the edge device and a Nvidia Tesla P100 as the cloud-backend, 

\subsection{Inference Framework}
We studied the performance of two inference frameworks TensorFlow Lite for edge inference and TensorFlow Serving for cloud inference.

For edge inference we additionally evaluated the performance of the Android Neural Network API (NNAPI), which TensorFlow Lite can use as an underlying framework.
%TF serving
%TF Lite
%%NNAPI


%%%%%%%%%%%%%%%%%%%%%%%%%
%Überleitung

\endinput 
