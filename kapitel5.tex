\chapter{Methodology Instantiation}
\label{chap:experiments}
In this section, we instantiate the methodology defined in the previous section step by step on an image classification use case.

In section \ref{chap:problemSpace} the problem space will be defined, based on the fundamentals of deep learning deployment presented in chapter \ref{chap:fundamentels}, followed by the performance metrics in section \ref{chap:metrics}.


The used infrastructure is presented in section \ref{chap:infrastructure} and includes the used hardware components, at the edge and the cloud, as well as the used inference frameworks for both edge and cloud inference.

Section \ref{chap:parameters} will cover the three deep learning models as well as the different types of input data used for the benchmarks.

Afterwards in section \ref{chap:androidApp} we design a Android benchmark system, which simulates a real-time AI application using the previously defined workloads and infrastructure. Additionally, the Android application also handles instrumentation of the performance metrics and their collection for later evaluation.

Using this benchmark system section \ref{chap:benchmarkExec} describes the execution of the benchmark, which results then get evaluated in \ref{chap:Evaluation}.

The benchmarks then get used to train a performance model in section \ref{chap:perfModel}, predicting the runtime latencies of the different deployment options and thus enable the performance model to give recommendations for the deployment.

Finally in chapter \ref{chap:DecisionModel}, the system understanding gained from all the previous steps of the methodology can be used to generate a decision model helping in the decision of optimal deployment option selection for deep learning inference.


\section{Problem Space}
\label{chap:problemSpace}
%AI applications ins spiel bringen
For real-time AI applications to be viable, real-time predictions are essential. Hence the inference performance of deep learning models is a key aspect.
If they performance cannot be achieved at the edge device itself, an alternative solution is the deployment of the deep learning model to a cloud-backend.
To be able to help in this decision, the factors influencing this performance have to be examined in the first place to get a general understanding of deep learning inference.
This section utilizes the in section \ref{chap:fundamentels} presented fundamentals of deep learning in general and specifically the aspects of deep learning deployment for inference.

The first factor on performance is the architecture of the deep learning model itself, as the architecture dictates how many operations are required to perform the inference, how much memory is needed by the model, how the input data has to be preprocessed.
Note that a large architecture is often needed for high accuracies, which are essential for the AI applications, but these large architectures result in high computational costs to perform the inference.

The second aspect is the hardware available in the deployment environment. 
Memory constraints can prevent that the deep learning model can be loaded into memory, network latencies can slow down inference too much or inference can take up too much CPU usages at the edge, resulting in the throttling of other running applications.
Hardware accelerators like GPUs or TPUs can significantly speed up inference performance.

The third influencing factor on inference performance is the inference framework. 
The framework is responsible to load the deep learning model, perform the operations of the deep learning model on the provided architecture and handle inference requests.
Optimized operators by the framework can optimize performance severely as well as the support of hardware accelerators.
Additionally, for cloud inference low latency network protocols are a key aspect for low-latency AI applications.

The fourth factor is the inference input itself, as different inputs have a different impact on both preprocessing and inference. 
Preprocessing larger inputs uses more preprocessing resources, while larger batch sizes can slow down inference performance.

All aspects of the problem space, hardware, model architecture, inference input and the used inference framework, have an impact on the performance, thus are the influencing factors on the inference performance.
In figure \ref{fig:perfmodel} the problem space is visualized depicting all performance influencing factors as its inputs and maps these factors to performance metrics, which are essential for assessing inference performance for real-time AI applications.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/PerformanceModel.pdf}
\caption{Problem Space with factors influencing the inference performance and the for the inference performance essential performance metrics}
\label{fig:perfmodel}
\end{figure}

To get the model output for a given inference input two steps are needed. The first one is the preprocessing step and transforms a given input into the format that is required by the deep learning model. Only then the actual inference operation can be performed to obtain the model output. Therefore the preprocessing takes a vital part in the general inference process and should be included in the performance model.

These inputs affect various performance metrics, of which inference latency and throughput are the most important ones for real-time AI applications on the edge. However, as the hardware on edge devices is limited and most of the times more than one application need to run in parallel, low usages of the other depicted metrics are vital as well.
In the following, we present performance metrics essential for the performance of real-time AI applications and the rationale behind them.







\section{Performance Metrics}
\label{chap:metrics}
Several metrics are important to measure the performance of the preprocessing and the inference steps. For most of the following metrics, we break down each metric into two sub-measurements, one for the preprocessing and the other for the inference part.

Except for data consumption, which is the only metric exclusive for cloud inference, because data is sent to a remote server, all metrics are relevant for both edge and cloud inference.
While accuracy is one of the most critical metrics for neural networks, it is only affected by the characteristics of the model and not by the deployment environment. Therefore we do not focus on accuracy in this thesis.

Note that we only measure the impact on inference/preprocessing on the resources on the edge devices, not the usage on the cloud-backend itself, as this thesis focuses on optimal selection for real-time AI applications on edge devices.
\subsubsection{Latency}
\label{chap:latency}
Latency is essential to determine the performance, since AI application often need predictions in real-time, resulting in need of low latencies.
The time needed to transform the original input to a shape fit for feeding into the deep learning model is called $Latency_{preprocessing}$.
%%WALL CLOCK TIME ADD HERE
$Latency_{inference}$ describes the time needed from requesting a prediction from a deep learning model given a specific input until getting the prediction.
The latency needed to perform both preprocessing and inference for a given input is called $Latency_{total}$.

For cloud inference, where network communication has to be done between a client (edge) and a server (cloud-backend), the network latency is an additional factor to the inference performance.
Therefore we split $Latency_{inference}$ in two sub metrics, $Latency_{network}$ and $Latency_{server}$, which together sum up to $Latency_{inference}$.
While $Latency_{server}$ denotes the time consumed by the cloud-backend to perform the inference on the given input, $Latency_{network}$ describes the time needed to send the inference input to the server and receive the prediction from the server.
\subsubsection{Throughput}
In order to accomplish real-time AI a certain level of throughput is essential. Therefore the number of predictions per second is a valuable metric. 
Since preprocessing is a vital part of the inference process, the throughput impact caused by preprocessed is also of interest.

Therefore we differentiate between three types of throughput: $Throughput_{preprocessing}$, $Throughput_{inference}$ and $Throughput_{total}$.
$Throughput_{total}$ is the throughput of the preprocessing and inference latencies combined.



\subsubsection{Energy Consumption}
$Energy_{preprocessing}$ and $Energy_{inference}$, which describe the amount of energy consumed during preprocessing and inference, are particularly important for mobile edge devices, since they often are powered by batteries with a limited lifespan. If the preprocessing or inference process consumes too much energy, the application using the model is not viable.


\subsubsection{CPU/GPU Usage}
Since the preprocessing/inference operations are most of the time not the only processes running on a system and other processes need to run simultaneously, the usages of CPU ($CPU_{inference}$, $CPU_{preprocessing}$) and GPU ($GPU_{inference}$, $GPU_{preprocessing}$) or other available accelerators are an important metric.
High usages would also indicate that performance would eventually be slowed down on devices with less CPU/GPU power.


\subsubsection{Memory Usage}
Similar to CPU usage, preprocessing and inference should not occupy the whole memory of the system or even demand more memory than the available memory. Therefore $Memory_{inference}$ and $Memory_{preprocessing}$ are of interest.
%\subsubsection{GPU Usage}
%If a GPU or an another accelerator is available their usage is of interest.

\subsubsection{Data Consumption}
$Data_{transmitted}$ and $Data_{received}$ are only relevant for cloud inference, as a request has to be sent to a remote server and the according response with prediction has to be sent back to the client. A high data consumption could slow down the inference latency significantly if the up- and downstream of the network connection is too slow. 

This is in particular important for the case where the input data for the model is preprocessed on the cloud, because the input data is often resized to a smaller shape during preprocessing. Therefore cloud preprocessing increases the amount of data that needs to be transmitted to the server. For slow network connections, this could result in higher inference latencies.

%%Add table here?




\section{Infrastructure}
\label{chap:infrastructure}
This section presents the infrastructure used for our benchmark system, specifically used hardware components and inference frameworks both at the edge and cloud.
An overview of the used infrastructure can be seen in figure \ref{fig:expDesign}.

\begin{figure}[H]
\centering
\resizebox{.95\linewidth}{!}{\input{./Bilder/Exp_design.tex}}
\caption{Infrastructure Overview}
\label{fig:expDesign}
\end{figure}


\subsection{Hardware}
In this thesis, we will benchmark one edge devices as well as one server, which will serve as the cloud-backend.
More devices would help illustrate the effect of different hardware environments better, but due to limited time, benchmarks for other devices are not possible.
\subsubsection{Edge}
\label{chap:hardwareEdge}
As the edge device we will use the OnePlus 6T (ONEPLUS A6013). This state of the art smartphone is powered by a Qualcomm Snapdragon 845 CPU(Octa-core, up to $2.8$ GHz), Adreno $630$ GPU, $8$ GB of memory and runs on OxygenOS $9.0.11$, which is based on Android $9$.
%%Cite from AI bechmark paper
\subsubsection{Cloud}
%The Nvidia DGX-1 will serve as the cloud-backend for the experiments. This server consists of 8$\times$Tesla V100 providing 1000 TFLOPS as well as 256 GB GPU memory and 512 GB system memory.
We use a virtual server hosted at the LRZ, which has 32 cores (16 real cores with hyperthreading), 240 GB of memory, a Tesla P100 16 GB PCIe GPGPU and a 800 PCIe SSD.
The server runs on Ubuntu 16.04 CUDA 9.1 PGI 17.9 nvidia-docker 2.0.3+docker18.03.1-1.
\subsection{Inference Framework}
We use two open source machine learning frameworks, both based on TensorFlow, for the experiments, TensorFlow Lite and TensorFlow Serving. We decided to use these frameworks, because they are open source and support, at the point of this theses, most of the operations needed for the popular image classification models as well as an increasing number of hardware accelerators and operating systems for both edge and cloud inference.
TensorFlow provides official releases of deep learning models, that are well maintained and tested, including the ones we use in this thesis.

This section only gives a brief overview of the most important aspects of the frameworks, for detailed information consider the TensorFlow Lite\cite{tfLite}  and TensorFlow Serving\cite{tfServing} websites, on which this section is partly based on, or the corresponding GitHub repositories.
\subsubsection{TensorFlow Lite}
\label{chap:TFLite}
TensorFlow Lite (Release 1.12.0) was developed for mobile and embedded devices and is a lightweight solution of TensorFlow and thus will be used for the edge inference experiments.

%%Mention NNAPI support?
At the moment only model inference can be made by TensorFlow Lite, not model training.
It supports acceleration with GPU or other accelerators as well as portability to Android, iOS and other IoT devices.

\paragraph{Android NNAPI}
\label{chap:NNAPI}
TensorFlow Lite is also compatible with the Android Neural Networks API\footnote{https://developer.android.com/ndk/guides/neuralnetworks/} (NNAPI). This API
is designed to speed up computationally intensive machine learning operations and can be used by TensorFlow Lite to improve inference performance. During inference NNAPI "can
efficiently distribute the computation workload across available on-device processors, including dedicated neural network chips, GPUs and DSPs"\cite{DBLP:journals/corr/abs-1810-01109}.

Figure \ref{fig:NNAPIarchitecture} shows the architecture of the NNAPI. In our use case the application is our android benchmark application presented in section \ref{chap:androidApp} and the machine learning framework we use is TensorFlow Lite.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./Bilder/nnapi_architecture.png}
\caption{System architecture for NNAPI framework \cite{NNAPI}}
\label{fig:NNAPIarchitecture}
\end{figure}


\paragraph{Hosting Models}
TensorFlow Lite expects models in their own \emph{FlatBuffer} file  format(\emph{.tflite}). Therefore models need to be converted to this format before TensorFlow Lite can load them. This conversion can be done using the TensorFlow Lite Converter, which supports various formats of trained TensorFlow models.
After conversion an object of the Interpreter class can load the \emph{.tflite} file.
\paragraph{Run Prediction}

To then run the inference process in TensorFlow Lite the run method of Interpreter object with a loaded model needs to be called. To call this function two objects need to be passed, first the input for the given model and second the output object, where the prediction response from the inference operation get stored. 
\begin{figure}[H]
\centering
\input{./Bilder/edge.tex}
\caption{Functionality of TensorFlow Lite}
\label{fig:edge}
\end{figure}

\subsubsection{TensorFlow Serving}
\label{chap:TFServing}
TensorFlow Serving (Release 1.11.1) will be used as the cloud inference framework since it provides a framework to serve machine learning models in production environments at the cloud. 



\paragraph{Hosting Models}
In order to host a model as a Servable in TensorFlow Serving, first a TensorFlow model needs to exported using TensorFlow's \emph{SavedModelBuilder}, resulting in a \emph{SavedModel protocol} buffer file along with the model’s variables and assets (Although TensorFlow Serving is optimized for TensorFlow models, the framework can be extended to serve other types of models).
%%Wieviel schreiben über signature, predict function etc?

Now the exported model can be loaded by an instance of Tensorflow Serving.
We use docker to start that instance, specifically nvidia-docker that allows us to run the inference operations on a GPU. For that TensorFlow Serving provides two docker images of their framework, one with CPU and the other with GPU support.

\paragraph{Run Prediction}
TensorFlow Serving supports two APIs for clients to create predictions requests: gRPC and REST. Since the gRPC protocol is supposed to deliver a better performance in the form of lower latencies and smaller payloads, we will use the gRPC API in this thesis.
Before a client can make a request to the server, a gRPC stub needs to be created in the first place, that allows us to call all methods implemented on the server. In our case, we need to call TensorFlow Serving's \emph{Predict} method to start the inference process. The method needs to be passed a \emph{PredictRequest} object, which contains among other things the input data for the model, the shape of the input and the requested model.%model signature?

After the request is sent and handled the server response by sending back a \emph{PredictResponse} object. This object holds the predictions for the given input data in the form specified by the exported model.
This request and response process can also be seen in figure \ref{fig:cloud}.

\begin{figure}[H]
\centering
\input{./Bilder/cloud.tex}
\caption{Basic Functionality of TensorFlow Serving}
\label{fig:cloud}
\end{figure}
 
%\section{Experimental Design}
%This section deals the specification of the Android benchmark application, used hardware, frameworks, models and how the measurements are conducted. 
%Figure \ref{fig:expDesign} depicts a brief overview of the design of the experiments, more precisely the used models, frameworks and hardware components, which are presented in greater detail in this section.
%The role of the Android benchmark application is to simulate a AI application that delegates the inputs, which in our use case are images, to the edge or cloud inference framework for the inference process and gets the predictions as a response from the frameworks.




\section{Parameters}
\label{chap:parameters}
There are two types of parameters, workload and factors.
Factors are system parameters with various levels affecting system performance, for example usage of an accelerator.
Workloads are requests to the system by a user, in our case request with a given image to a specific deep learning model, that is hosted by the system.
This requests can come in different shapes, thus having a different impact
on performance.
\subsection{Workload}
\label{chap:workload}
The methodology of this thesis will be illustrated on the use case of image classification, as state of the art image classification deep learning models are demanding in computational power as well as of high interest for real-time AI applications using these models for inference purposes.

There are two types of workload, first being the image classification model itself, as each architecture has different computational requirements.
This thesis will benchmark three different image classification models with significant differences in their architecture to illustrate their impact on inference performance.

The second type is the input data, in our use images, on which the inference should be performed.
This images come in various shapes and sizes and have to be preprocessed to the shape defined in the architecture of the image classification model.
To study the impact of these various shapes on the preprocessing step, we present different example images in this section.

\subsubsection{Image Classification Models}
\label{chap:models}
We will benchmark three different image classification models in this thesis, MobileNetV2, InceptionV4 and quantized MobileNetV2. The first is optimized for mobile deployment, the second towards high accuracy and the last one is a further optimized version of the first model.
All models are trained on the ImageNet dataset consisting of 1001 image classes (1000 image classes + 1 class for other image classes).
An overview of the models can be seen in table \ref{table:modelOverview} in the form of top-5 accuracy (a prediction is classified as accurate if the five labels with the highest confidence contain the real class of the input image), the input size of the model, the number of parameters in millions and the model size in the TensorFlow Lite format.
InceptionV4 has $4.5\%$ more accuracy than MobileNetV2, but also more than $12$ times more parameters and $7.5$ times larger TensorFlow Lite model size.
MobileNetV2 also uses a smaller input sizes of $224\times224$ than InceptionV4's $299\times299$ input size.
\begin{table}[H]
%CITE inception params ned vergessen
%http://dgschwend.github.io/netscope/#/preset/inceptionv4
\caption{Overview of used models}
\label{table:modelOverview}
\begin{tabular}{@{}lllll@{}}
\toprule
Model & Parameters & Top-5 Accuracy\cite{modelspecs} & Input Size & \emph{.tflite} Model Size \\
\midrule
InceptionV4 & $42.68$M\cite{InceptionV4params} & $95.1\%$ & $299\times299$ & $107.7$MB \\
MobileNetV2 1.0 & $3.47$M\cite{DBLP:journals/corr/abs-1801-04381} & $90.6\%$ & $224\times224$ & $14$MB \\
\begin{tabular}[c]{@{}l@{}}MobileNetV2 1.0\\quantized\end{tabular}  & $3.47$M\cite{DBLP:journals/corr/abs-1801-04381} & $89.9\%$ & $224\times224$ & $3.4$MB\\
\bottomrule
\end{tabular}
\end{table}
For TensorFlow Lite we use the models provided on the TensorFlow Lite website \cite{tfLiteModels}.
To convert model suitable for TensorFlow Serving, we use the TensorFlow-Slim library \cite{tfSlim}, where maintained and tested implementations of popular image classification models are being published. For both Serving and Lite we use the same training checkpoint to get the same weights for the graphs.

In the following, we give a brief overview of the models, their unique building blocks and the intuition behind them.
For full details please refer to \cite{DBLP:journals/corr/abs-1801-04381} and \cite{InceptionV4}.

\paragraph{MobileNetV2}
MobileNetV2 (version 1.0) is a successor of MobileNetV1 and is "specifically tailored for mobile and resource-constrained environments" \cite{DBLP:journals/corr/abs-1801-04381}. The authors achieve this by "significantly decreasing the number of operations and the memory needed while retaining the same accuracy"  \cite{DBLP:journals/corr/abs-1801-04381} and introducing a new layer module called "the
inverted residual with linear bottleneck".

This module is visualized in figure \ref{fig:bottleneckBlock} and consists of two parts: The inverted residual block and a shortcut from the input to the output.
In a first step of the inverted residual block, the channel dimensions of the input are expanded with the use of a pointwise $1\times1$ convolution layer. 
Afterwards depthwise $3\times3$ convolution is applied to the expanded input. Then again a pointwise $1\times1$ convolution gets used, but this time the dimensions are decreased instead of increased.

The intuition behind this module is that the expansion decodes information ensuring that the features can be extracted during the depthwise convolution. The extracted features then get encoded again by reducing the dimensions.
To improve the backpropagation of the gradient across multiple layers during the training the authors add the shortcut to the module, resulting in faster training and better accuracy.
The bottleneck module can be implemented very memory efficient, thus particularly fit for edge inference.

Figure \ref{fig:MobileNetArchi} displays the overall architecture of the model, with the majority of the building blocks being the bottleneck modules.
All bottleneck modules in a sequence have the same number of output channels and use a stride of $1$, except for the first bottleneck block in a sequence. All spatial convolutions use $3\times3$ kernels. 
Although not depicted on the figure, the model uses dropout and batch normalization during training.
%%expanstionf actor?
%%$$shortcut!! expanded by a "expansion factor" varying for different seqences

\begin{figure}[!htb]
\centering
   \resizebox{.7\linewidth}{!}{\input{Bilder/MobileNet_bottleneck.tex}}
\caption{Inverted Residual Block (bottleneck)}
\label{fig:bottleneckBlock}
%%NO DROPOUT AT INFERENCE
\end{figure}


\begin{comment}


\begin{table}[]

\centering
\caption{MobilenetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
\label{table:mobilenetArchi}
\begin{tabular}{@{}llllll@{}}

\toprule
Input & Operator & t & c & n & s \\ \midrule
$224^2\times 3$ & conv2d & - & 32 & 1 & 2 \\
$112^2\times 32$ & bottleneck & 1 & 16 & 1 & 1 \\
$112^2\times 16$ & bottleneck & 6 & 24 & 2 & 2 \\
$56^2\times 24$ & bottleneck & 6 & 32 & 3 & 2 \\
$28^2\times 23$ & bottleneck & 6 & 64 & 4 & 2 \\
$14^2\times 64$ & bottleneck & 6 & 96 & 3 & 1 \\
$14^2\times 96$ & bottleneck & 6 & 160 & 3 & 2 \\
$7^2\times 160$ & bottleneck & 6 & 320 & 1 & 1 \\
$7^2\times 320$ & conv2d 1x1 & - & 1280 & 1 & 1 \\
$7^2\times 1280$ & avgpool 7x7 & - & - & 1 & - \\
$1\times 1\times 1280$ & conv2d 1x1 & - & k & - &  \\ \bottomrule
\end{tabular}
\end{table}
\end{comment}
\subparagraph{Quantization}
Quantization of MobileNetV2, using the techniques presented in section \ref{chap:quant}, results in a  0.7\% loss of top-5 accuracy, but therefore has lost 75\% of its model size (see table \ref{table:modelOverview}) and is supposed to deliver a better inference performance, which we will evaluate in the benchmarks.



\paragraph{Inception V4}
%%Add infos about stem and reduction
InceptionV4, published in \cite{InceptionV4}, is a large image classification network with high accuracy, but also with a high number of parameters leading to higher computational demands than MobileNetV2 and thus larger impact on inference performance.
In comparison to its previous versions, InceptionV4 is built with "a more uniform simplified architecture and more inception modules"\cite{InceptionV4}. 

The general architecture of the network can be seen in figure \ref{fig:inceptionv4Archi} with the fundamental building blocks being multiple Inception (A-C) and Reduction(A-B) modules. 
An example for both of these modules can be seen in figures \ref{fig:InceptionA} and \ref{fig:InceptionReduction}.
Inception modules are built of multiple convolutions with multiple filters and pooling layers in parallel within the same layer.
After the convolutions, the output of all convolutions is concatenated.
The intuition behind this parallelism is to give the model multiple convolution choices for a given input and let the model learn itself the best feature extractor. An additional benefit is that the model can extract both local and more complex feature from an input.

To reduce the dimensionality before computational expensive large convolutions, and thus speed up training, the authors introduce the Reduction block containing small $1\times1$ convolutions.   %genauer erklären




\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{.95\textwidth}
\centering
   \resizebox{.8\linewidth}{!}{\input{Bilder/InceptionAModule.tex}}
   \caption{Inception-A module}
   \label{fig:InceptionA} 
\end{subfigure}

\vspace{1em}
\begin{subfigure}[b]{.95\textwidth}
\centering
   %%Verify the architectures again
   \resizebox{.6\linewidth}{!}{\input{Bilder/InceptionReductionModule.tex}}
   \caption{Reduction module (convolutions with "V" are valid, rest is same padded)}
   \label{fig:InceptionReduction}
\end{subfigure}

\caption{Special modules used by InceptionV4}

\end{figure}




Besides dropout InceptionV4 also benefits from the use of batch normalization during training.
%\begin{figure}[]
%\centering
%\resizebox{.45\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
%\includegraphics[angle=90,width=0.94\textwidth]{./Bilder/inceptionV4_architecture.png}
%\caption{InceptionV4 architecture \cite{InceptionV4}}
%\label{fig:inceptionv4}
%\end{figure}

\begin{figure}[!htb]
\centering
\begin{subfigure}[t]{0.47\textwidth}
   \resizebox{.99\linewidth}{!}{\input{Bilder/InceptionV4_archi.tex}}
   \caption{InceptionV4 architecture \cite{InceptionV4}}
   \label{fig:inceptionv4Archi} 
\end{subfigure}%
\begin{subfigure}[t]{0.47\textwidth}
   %%Verify the architectures again
   \resizebox{.99\linewidth}{!}{\input{Bilder/MobileNetV2_archi.tex}}
   \caption{MobileNetV2 architecture \cite{DBLP:journals/corr/abs-1801-04381}}
   \label{fig:MobileNetArchi}
\end{subfigure}

\caption{Architectures of InceptionV4 and MobileNetV2}
%%NO DROPOUT AT INFERENCE
\end{figure}

\subsubsection{Input data}
We evaluate the performance of 2, 4, 8 and 16  megapixels (MP) PNG images, as state of the art edge devices are capable of taking pictures with such high amounts of megapixels. This way the effect of different image sizes on the performance of the preprocessing step can be assessed. We also evaluate an image where no resizing is needed ($224\times224$ and $299\times299$ depending on the model) to study the impact of image resizing. A picture of a cat (see figure \ref{fig:cat}) scaled to the different sizes will serve as the picture for the experiments.
The sizes of the images in kilobytes as well as the according resolutions can be seen in table \ref{table:imagesOverview}. In the following, when we speak of $224^2/299^2$ or $0.05$MP images, we mean both $224\times224$ and $299\times299$ images.

\begin{table}[!htb]
\centering
%CITE inception params ned vergessen
%http://dgschwend.github.io/netscope/#/preset/inceptionv4
\caption{Overview of used image sizes}
\label{table:imagesOverview}
\begin{tabular}{@{}lll@{}}
\toprule
Image   & Resolution       & PNG Size  \\ \midrule
$224^2$ & $224\times224$   & $83$KB    \\
$299^2$ & $299\times299$   & $141$KB   \\
$2$MP   & $1732\times1155$ & $2411$KB  \\
$4$MP   & $2449\times1633$ & $4309$KB  \\
$8$MP   & $3464\times2309$ & $7515$KB  \\
$16$MP  & $4899\times3266$ & $10077$KB \\ \bottomrule
\end{tabular}
\end{table}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.3\textwidth]{./Bilder/European_cat_compressed.jpg}
\caption{Picture used for the experiments \cite{cat}}
\label{fig:cat}
\end{figure}
\paragraph{Batch Size}
%%mathe defs of throughput?
Another way to increase workload, except different images sizes, is to increase the batch size of a request.
Batch size denotes the number of images fed into the deep learning model in a single inference operation. 
Feeding more than one image to the network can lead to higher throughput, if enough computational power is available. This increase in throughput often comes at the cost of increased latency and general resource consumption.

To study these trade-offs, we conduct experiments with the following batch sizes: 1, 2, 16, 32. Since at the point of these benchmarks a batch size greater than one is not supported for the TensorFlow Lite versions of MobileNetV2 (both float and quantized), measurements with these batch sizes cannot be performed for the case of edge inference at this point.

\subsection{Factors}
Over the course of the benchmark execution, we study the effect of multiple factors on performance with different levels.
%%%293,performance analysis buch

\paragraph{Preprocessing Mode}
For the case of cloud inference, the major parts of the needed preprocessing are either done on the edge before sending the image to the cloud or done on the cloud. Therefore we evaluate both options.
\paragraph{Inference Mode}
The inference can either be performed on the edge or an a cloud-backend.
We expect significant trade-offs between these two inference options, especially for large deep learning models.


\paragraph{NNAPI}
The Android Neural Network API (NNAPI), presented in section \ref{chap:NNAPI}, is supposed to enhance the inference performance of TensorFlow Lite. Therefore we take a look into the effect of this framework.
%%Mention GPU use here
\paragraph{GPU Usage}
Since January 16 an experimental release of TensorFlow Lite supporting GPU usage on Android using OpenGL ES 3.1 Compute Shaders \cite{tfLiteGPU}.
So far only four public models and their operators are supported, including MobileNetV2, but not InceptionV4. 
Therefore we omit the test of this new feature in our experimentation, but our initial testing indicated that the use of NNAPI provides faster inference latencies for MobileNetV2 than the GPU version.
%%https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7
%experimental
%only mobilenet supported
%worse performance than NNAPI so far


\begin{figure}[!htb]
\centering
 \scalebox{.7}{\input{./Bilder/tree.tex}}

\caption{Excerpt of the executed parameter configurations}
\label{fig:tree}
\end{figure}


\vspace{15pt}
Figure \ref{fig:tree} shows a part of our parameter tree, including both workload and factors. In theory, there would be $240$ different parameter configurations, but due to a number of reasons, the number of different configurations reduced to 113 in total.
First, the Usage of NNAPI is independent of the image size hence we only perform experiments without NNAPI for one image size. Also the usage of NNAPI leads to a way better performance for edge inference and we want to compare edge and cloud inference, so we only carry out experiments without NNAPI for a batch size of one.
We omit the cloud inference for the quantized MobileNetV2 because the model is so small the network overhead would be overwhelming.
Lastly, due to lack of support certain operations in TensorFlow Lite for the MobileNetV2 models, we can not perform inference experiment for MobileNetV2 with a batch size larger than one.

\section{Android Benchmark System}
\label{chap:androidApp}
To conduct the benchmarks for both edge and cloud inference we developed and implemented an Android benchmark System using Kotlin.
This system implements all functionality needed to simulate the defined workloads and parameters and perform the inference on either the Android device itself or send the image to a cloud-backend.
For both preprocessing and inference the application logs metrics such as inference latency and time of the experiments and stores them to a \emph{CSV} file, which then can be used to evaluation purposes.

Both edge and cloud inference implementations are based on the example implementations in the respective GitHub repositories to ensure optimal performance. 


\begin{figure}[!htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/FlowChart_App.png}
\caption{Flowchart of the Benchmark Application showing the whole inference process of selecting an image until getting a prediction}
\label{fig:app}
\end{figure}
In figure \ref{fig:app} the work-flow to perform the inference is seen. First, an image needs to be selected. Afterwards, the image classification model needs to be chosen. Now the user selects whether the inference should be performed on the cloud-backend or directly on the edge device, in this case, an Android phone. In the case of edge inference, it needs to be decided if the NNAPI should be used by TensorFlow Lite. For cloud inference, the preprocessing mode needs to be selected (edge/cloud). Now the preprocessing operations can be performed based on the previously selected options (Even for the case of cloud inference with cloud preprocessing some preprocessing needs to be done at the edge beforehand like building a \emph{PredictRequest} containing the request for the cloud-backend). Now, that the input is preprocessed, the actual inference is performed as the last step, resulting in a prediction. For both preprocessing and inference the measurements mentioned in section \ref{chap:insta_measurements} are logged.

\begin{figure}[htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/UML.pdf}
\caption{UML class diagram of the benchmark application}
\label{fig:UML}
\end{figure}
Figure \ref{fig:UML} depicts a UML class diagram of the application featuring the most important classes, functions and variables. 
The main class is called \emph{MainActivity} and implements all of the graphical aspects. 
The \emph{MainActivity} handles requests for both cloud and edge inference by delegating the requests to instances of the classes \emph{TFServingClient} and \emph{ImageClassifier}, respectively. Both of these classes also take care of the preprocessing steps.
The abstract class \emph{ImageClassifier} has two subclasses \emph{FloatClassifier} and \emph{QuantClassifier}. The first class runs the inference for floating point models and the second for quantized models.
This separation is needed since these different model types require different input and output types.

The \emph{MainActivity} writes the collected measurements and the parameter configurations of an experiment to an instance of \emph{SingleExperiment}. After the experiment is completed \emph{MainActivity} tells a \emph{Logger} instance to save the contents of the \emph{SingleExperiment} object. The Logger then saves all collected data to a \emph{CSV} file.

\subsection{Preprocessing}
%%mention png
For the case of image classification, the images need to have the correct size ($224\times224$ for MobileNetV2 and $299\times299$ for InceptionV4) and the RGB values need to be normalized to the interval $[-1,1]$. After preprocessing the image has been transformed into the shape $224\times224\times3$ with all values between $[-1,1]$, where the first two dimensions represent the image height and width, while the last dimension represents the number of channels (3 since the images are RGB).
\begin{figure}[H]
\centering
\input{Bilder/Preprocessing.tex}
\caption{Preprocessing steps for image classification: Resizing to the model input size and normalization of the pixel values to $[-1,1]$}
\label{fig:prepro}
\end{figure}
\subsubsection{Edge Preprocessing}
\label{chap:preproImpl}
In the case of edge preprocessing all preprocessing steps are done on the edge device, meaning that the input can be fed directly into the neural network afterwards, either on the edge device itself or on a cloud-backend.
We perform these steps the following way: After loading the PNG image into an \emph{InputStream} we create a scaled Bitmap of the image (scaled to either $224\times224$ or $299\times299$) by calling the \emph{createScaledBitmap} function. 
Afterwards, we normalize the RGB values to $[-1,1]$ during the conversion from the bitmap to a \emph{ByteBuffer}. 
For the case of float models, this buffer contains all the pixels in float format and for the quantized model in bytes since quantized models work with lower representations.
%%%mehr erklären?
We do this conversion since feeding \emph{ByteBuffers} to TensorFlow Lite is performance enhancing. %add cite here
For cloud inference we then construct \emph{PredictRequest} object containing this \emph{ByteBuffer}.

For batch size larger than one we parallelize the preprocessing to speed up the preprocessing latency at the cost of higher maximum memory consumption. We start $n$ threads, where each thread is preprocessing one image in the way described above. After each image is preprocessed we concatenate all \emph{ByteBuffers} into a single one, which then can be fed to the TensorFlow Lite interpreter. Note that $n$ is determined by both batch size and available CPU cores on the edge device. There are never more threads than available cores, but if the batch size is smaller than the number of cores, we only start $n$ threads, where $n$ is the batch size. 

\subsubsection{Cloud Preprocessing}

In the case of cloud inference, the images can also be preprocessed directly at the cloud, resulting in nearly no preprocessing done at the edge. While the resizing and scaling steps are no longer done on the edge, the image still needs to be converted into a \emph{PredictRequest} object that TensorFlow Serving can handle.
To achieve this we load the PNG image into an \emph{InputStream}, convert it to a \emph{ByteArray} which then can be feed to the \emph{PredictRequest} object as a \emph{ByteString}. 

%Add TensorFlow Serving preprocessing here


\subsection{Inference}
To get the predictions for our now preprocessed image, we need to run the inference operation of the inference framework with the deep learning model loaded, which is loaded either directly at the edge or the remote cloud-backend. 
The inference is done using the steps described in section \ref{chap:TFLite} (TensorFlow Lite) and \ref{chap:TFServing} (TensorFlow Serving).
\subsubsection{Edge Inference}
To perform the inference operation we need to pass two things to the \emph{run} function of an interpreter of TensorFlow Lite: The \emph{ByteBuffer} created in the preprocessing step and an array (float models: \emph{FloatArray}, quantized models: \emph{ByteArray}) with the length 1001 (number of classes). TensorFlow lite then writes the confidence levels of the different classes to this array. We then sort the array for the five classes with the highest confidence and print them to the screen.

\subsubsection{Cloud Inference}
\label{chap:CloudInfImpl}

For the cloud inference we send the \emph{PredictRequest} object created in the preprocessing process to the TensorFlow Serving server over a plaintext gRPC channel.
If the input has been preprocessed at the edge beforehand, Tensorflow Serving can perform the model inference directly.
If the input has not been preprocessed beforehand, the input arrives at the server in the \emph{PNG} format, therefore we first decode them with \emph{tf.image.decode\_jpeg}, then resize with \emph{tf.image.resize\_bilinear} and finally normalize the tensor values to $[-1,1]$ using the the \emph{subtract} and \emph{multiply} functions. Now the input has the same shape as if they would have been preprocessed at the edge can be fed to the actual model graphs.

\subsection{Instrumentation}
\label{chap:insta_measurements}
In the following section we describe how we measure the performance metrics defined in \ref{chap:metrics}.
We conduct the measurements either directly in the source code or by using Android Studio Profiler\footnote{https://developer.android.com/studio/profile/} (Version 3.3). Since Android Studio cannot collect all metrics the way we wanted, we wanted to use Trepn\footnote{https://developer.qualcomm.com/software/trepn-power-profiler} as a secondary profiling application. But since Trepn does not support our test device, we had to omit measurements of these metrics.

In the following definitions, $t_{start}$ denotes the starting point of either the preprocessing or inference, while $t_{end}$ denotes the end point of the respective process.
\subsubsection{Latency}
We measure $Latency_{preprocessing}$ by measuring the time difference between the start and end of the preprocessing process.
Note that all latency measurements are reported in milliseconds(ms).
\begin{equation*}
\begin{gathered}
Latency_{preprocessing} = t_{endPreprocessing} - t_{startPreprocessing}
\end{gathered}
\end{equation*}
To measure inference latency we need to distinguish between edge and cloud inference, since for the latter the network latency needs to be considered.

\paragraph{Edge Inference}To measure edge inference latency we measure the time the TensorFlow Lite interpreter needs to run the inference operation on the loaded model given the input image.
\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{endInference} - t_{startInference}
\end{gathered}
\end{equation*}
\paragraph{Cloud Inference}
To measure the cloud inference latency we need to measure two latencies, which combined yield $Latency_{inference}$. The first latency is  $Latency_{server}$. This server latency describes the time difference between the point where TensorFlow Serving receives the inference request and the point in time where TensorFlow Serving sends the response back to the client.
The second latency $Latency_{network}$ denotes the time the prediction request needs to reach the cloud-backend and the time the response need from the server back to the TensorFlow Serving client..
These latencies are illustrated in figure \ref{fig:serverLat}.
\begin{figure}[!htb]
\centering
\input{./Bilder/server_lat.tex}
\caption{Measurement of $Latency_{server}$ and $Latency_{network}$ for Cloud Inference}
\label{fig:serverLat}
\end{figure}


Similar to the edge inference, $Latency_{inference}$ is being measured by calculating the time difference between starting the inference process and receiving the prediction for the given inference request. This whole process is covered by the \emph{predict} function of TensorFlow Serving. Therefore we measure the wall clock time of this function.

Since TensorFlow Serving does not output the server latency, we needed to tweak the source code of gRPC, which is the underlying protocol of TensorFlow Serving. gRPC already logs this latency, so we adjusted the source code to output this latency when a call to TensorFlow Serving is finished, repackage the source code and changed to dependencies of TensorFlow Serving pointing to the adjusted packages.

We then calculate $Latency_{network}$ implicitly by subtracting $Latency_{server}$ from $Latency_{inference}$.

\begin{equation*}
\begin{gathered}
Latency_{inference} = t_{endInference} - t_{startInference}\\
Latency_{server}= t_{receive Request} - t_{send Response}\\
Latency_{network} = Latency_{inference} - Latency_{server}
\end{gathered}
\end{equation*}


The total latency $Latency_{total}$ for both edge and cloud inference is simply calculated by summing up the latencies of both preprocessing and inference.
\begin{equation*}
\begin{gathered}
Latency_{total} = Latency_{preprocessing} + Latency_{inference}
\end{gathered}
\end{equation*}
\subsubsection{Energy Consumption}
Since Android Studio Profiler only estimates the energy consumption in the form of low, medium and high, the tool is not fit to provide empiric measurements. The Trepn Power Profiler would provide such measurements, but does not support the device used for the benchmarks (OnePlus 6T).
\subsubsection{CPU Usage}
We measure the CPU usage(\%) of both preprocessing as the maximum CPU usage during the respective processes.
\begin{equation*}
\begin{gathered}
%%CPU_{preprocessing} = max_{[start_{preprocessing}, end_{preprocessing}]}(CPU)\\
%%CPU_{inference} = max_{[start_{inference}, end_{inference}]}(CPU)\\
CPU_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} CPU_{Usage}(t)\\
CPU_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} CPU_{Usage}(t)
\end{gathered}
\end{equation*}
Android Studios’ CPU Profiler allows us to record the maximum CPU usage for both preprocessing and inference. To minimize the impact on the Android Profiler on the performance of the application we disable allocation tracking.
\begin{figure}[H]
\centering  
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_CPU}
\caption{Android CPU Profiler}
\label{fig:prof_cpu}
\end{figure}
\subsubsection{Memory Usage}
We measure the memory usage by recording the maximum memory consumption during both processing and inference. Both of the following metrics are reported in megabytes(MB) in the result section.
\begin{equation*}
\begin{gathered}
Memory_{preprocessing} = \max\limits_{t_{startPreprocessing} \leq t \leq t_{endPreprocessing}} Memory(t)\\\\
Memory_{inference} = \max\limits_{t_{startInference} \leq t \leq t_{endInference}} Memory(t)\\
\end{gathered}
\end{equation*}
The Memory Profiler is part of Android Studio Profiler and shows the memory consumption of the app it is profiling. Memory allocations by the operating system or other apps are not recorded. Besides recording the total amount of memory allocated the Profiler also tracks the different categories, for example memory allocated by Java/Kotlin code. We always record the maximum consumed memory for each operation.
%%memory not accurate over 1GB
Note that for values greater than $1000$MB the profiler only reports gigabyte values with one decimal place. For example the profiler reports $1410$MB as $1.4$GB.

Figure \ref{fig:prof_mem} depicts an example of the Memory Profiler for a single experiment. The first peak in memory consumption is the preprocessing step, while the inference process causes the second peak.
The little trash can at the bottom of the figure shows that the garbage collection was called. The garbage collection was always manually called after the preprocessing, in case not all unneeded memory allocations are collected before running the inference operation. 
Note that we report total memory consumption of the application, including memory consumed for the graphical interface or the \emph{Logger} class.
%In the case of preprocessing only the preprocessed image is needed, so the 
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./Bilder/profiler_MEM}
\caption{Android Memory Profiler showing the memory consumption during preprocessing and inference}
\label{fig:prof_mem}
\end{figure}
\subsubsection{GPU Usage}
Neither Android Studio nor Trepn can provide GPU metrics for the OnePlus 6T. Hence no GPU measurements can be conducted.
\subsubsection{Throughput}
We differentiate three types of throughput: the inference throughput, preprocessing throughput and the total throughput, which also includes preprocessing besides inference.
We calculate throughput in operations per second by 

\begin{equation*}
\begin{gathered}
Throughput_{preprocessing} =\frac{1000}{(Latency_{preprocessing}) / batchsize}\\
Throughput_{inference} =\frac{1000}{(Latency_{inference}) / batchsize}\\
Throughput_{total}  =\frac{1000}{(Latency_{total}) / batchsize}
\end{gathered}
\end{equation*}
\subsubsection{Data Consumption}
We measure both transmitted and received data by using the Android TrafficStats package\footnote{https://developer.android.com/reference/android/net/TrafficStats}. We start measuring both transmitted and received bytes when the inference operation is started and stop when the response from the server is returned. We report both $Data_{transmitted}$ and $Data_{received}$ in kilobytes(KB).
\begin{equation*}
\begin{gathered}
Data_{transmitted} = \sum_{t_{startInference}}^{t_{endInference}} Data_{transmitted}(t)\\
Data_{received} = \sum_{t_{startInference}}^{t_{endInference}} Data_{received}(t)
\end{gathered}
\end{equation*}

\section{Benchmark Execution}
\label{chap:benchmarkExec}
Using the benchmark system presented in the previous section, we now describe how the benchmark measurements are executed.


Preprocessing is performed without any models loaded at the edge. But with the image loaded into memory before the start of preprocessing to prevent the I/O operations from influencing the preprocessing performance.
Edge inference is executed using a loaded deep learning model. 
We do not consider model loading latency in this thesis.

All benchmarks are done using the eduroam Wi‑Fi since the cloud-backend server hosted at the LRZ is only reachable within the Münchner Wissenschaftsnetz (MWN) and a VPN would have an impact on network latencies.
We conducted the benchmarks in multiple sessions, where each session consists of around  $100$ measurements distributed over two hours, thus preventing the results to be skewed by thermal throttling.
During the benchmarks, no other applications are running on either the edge or the cloud device.

We run each parameter configuration listed in \ref{fig:tree} $15$ times to reduce variance and stabilize our results. In total we generate $1696$ in total.
\section{Evaluation of the Benchmark Results}
\label{chap:Evaluation}
The previous sections covered the design of the benchmark system, including used models, hardware devices and frameworks, as well as the definition of performance metrics. Using these design specifications, we will now present and evaluate the results of the conducted experiments.
For precise mean values and standard deviations of all measured metrics refer to tables \ref{measurementsInception} and \ref{measurementsMobilenet}.

First, we take a look into the individual results of edge and cloud inference in sections \ref{chap:EdgeResults} and \ref{chap:CloudResults}.
Afterwards compare them against each other in section \ref{chap:EdgeCloudResults}. In each of these sections, the preprocessing results are presented first, followed by the inference results.
In the last subsection, we take a separate look at batch sizes larger than one.

To visualize the results we mostly use bar and line plot, which report the average values.
The black bar on top of each bar of the following bar charts and translucent areas around the lines of the line plots represent the standard deviation.
Stacked bar plots do not show the standard deviations, as the they already are represented in the single plots of the individual results.

%die 83mb aufschlüsseln in GUI etc 
Note that the Android application consumes around 83MB in memory and 0\% CPU during idle. The logger, which is used to save the measurements of the experiments consumes memory as well, has an additional impact on the memory consumption as well, hence the memory consumption shown in the results include both logger and other memory overhead may be caused by aspects like the graphical interface.


\subsection{Edge Inference}
\label{chap:EdgeResults}
This section presents the results of the edge inference experiments with OnePlus 6T device presented in section \ref{chap:hardwareEdge}.

Note that this section only covers the results for a batch size of one, for the results of larger batch sizes please refer to section \ref{chap:resultsBatchSize}.

\FloatBarrier
\subsubsection{Preprocessing}
\label{chap:edgePrepro}

\paragraph{$\mathbf{Latency_{preprocessing}}$ \& $\mathbf{Memory_{preprocessing}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.96\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Preprocessing.pdf}
\caption[Edge Inference: Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$]{Edge Inference: Preprocessing $Latency_{preprocessing}$, $Memory_{preprocessing}$ -  lower is better: Increasing image sizes have increasing impact on both memory and latency, independent of model type}
\label{fig:EdgePrepro}
\end{figure}
Figure \ref{fig:EdgePrepro} shows the $Latency_{preprocessing}$ and $Memory_{preprocessing}$ results for the different image sizes and models.
There is little difference in both latency and memory for the different models. 
InceptionV4 uses on average $7.5$MB more memory (or $5\%$) than the MobileNetV2 models, which is not significant since the standard deviation of both models is above $20$MB.
We believe this tendency is caused by the models higher input size (InceptionV4 $299\times299$, MobileNetV2 $224\times224$). 
These small differences are expected, since both all three models require roughly the same input specifications, with the exception of input size, but the difference between the two input sizes is relatively small.

While the different models have little difference in memory and latency, the image size has a significant impact on both of these metrics.
A $0.05$MP image consumes on average $114.5$MB memory and takes $38.2$ms to preprocess, whilst  $16$MP images consume $177.8$MB and last $430.8$ms.
Therefore $16$ megapixel images take on average more than $11$ times as long to preprocess across all models, as well as using $1.55$ times more memory in comparison to a $0.05$MP image.
Figure \ref{fig:EdgePrepro} shows a steady increase in $Latency_{preprocessing}$ and $Memory_{preprocessing}$ across the increasing image sizes.
Preprocessing large images is therefore mainly affected by the resizing process, not the pixel normalization or conversion to a \emph{ByteBuffer}, since these steps are performed after the resizing, thus having the same impact on performance as they would have on a $0.05$MP image.

The standard deviation of both memory and latency are both low, indicating a stable resource consumption.


%%factors reinbringen auch kb größe der bilder
%%mehr impact on latency than memory


%%Was über CPU hier sagen
%%CPU usage in obere grafik integrieren?
$\mathbf{CPU_{preprocessing}}$
Looking at the CPU usages during preprocessing (see figure \ref{fig:CloudEdgePreproCPU}) one can see that the usages are very similar across all models and image sizes. This is probably due to the fact that the preprocessing of a single image is done on a single core. Since the OnePlus 6T has $8$ cores, the maximal CPU usage caused by a single core is $12.5\%$, which also displays in the mentioned plot, where all usages are close to this number.
%%High variance
%%around?

\FloatBarrier
\subsubsection{Inference}
This section presents the results for edge inference, in particularly the effect of the Android Neural Network API.
Different image sizes have no effect on edge inference, as images are always preprocessed when they reach the inference step.
Hence we do not consider the different image sizes in the majority of this section.


\begin{comment}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Inference_Inference.pdf}
\caption{Edge Inference: Inference metrics - lower is better}
\label{fig:EdgeInference}
\end{figure}


\end{comment}
\subsubsection{Effect of NNAPI}
%%effcient
The Android Neural Network API (NNAPI) is supposed to speed up inference of neural network on Android by introducing optimized kernels/operators. 
The effect of this framework can be seen in figure \ref{fig:NNAPI} and show the significant performance improvement caused by the NNAPI, not only affecting $Latency_{inference}$, but also $Memory_{inference}$ and $CPU_{inference}$.
In general, the usage of NNAPI leads to $4$ times lower inference latencies, $1.2$ times less memory consumption as well as $3$ times lower CPU usages.

This effect can be observed across all tested models, but especially on the InceptionV4 network, as this large model especially benefits from the optimized operations.
Note that the NNAPI uses the GPU of the OnePlus 6T for a part of its inference, therefore while reducing the CPU usage during inference, the NNAPI probably causes higher GPU usages.

Since the NNAPI leads to performance improvements in all measured metrics, it will be used for all further edge inference results.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.96\textwidth]{./Bilder/single_plots/edge_inference_plots/NNAPI_behavior.pdf}
\caption[Edge Inference - Effect of NNAPI on Inference]{Edge Inference - Effect of NNAPI on Inference - lower is better: NNAPI has a significant positive effect on the inference metrics $Latency_{inference}$, $Memory_{inference}$ and $CPU_{inference}$}
\label{fig:NNAPI}
\end{figure}

%, InceptionV4, MobileNetV2 and the quantized version of MobileNetV2,
When comparing the different models with NNAPI enabled it can be stated that there a significant performance differences between these models.
%%MobileNet und Inception vergleichen
%begründen: effect of quant mit paper vergleichen
%unterschiede mobileNet inception vergleichen
%%danahc MobileNet mit quantized
\paragraph{$\mathbf{Latency_{inference}}$ \& $\mathbf{Memory_{inference}}$}
On average, InceptionV4 inference causes higher inference latencies of factor $7$ ($33.1$ms vs. $237$ms) and consumes more than twice as much memory as MobileNetV2 ($126.2$MB vs. $276.3$MB).
This contrast in performance is as expected, since MobileNetV2 architecture only contains $8\%$ of InceptionV4's parameter number (see table \ref{table:modelOverview}).
This larger architecture, as well as bigger input size, lead to higher memory demands and latencies, thus explaining the performance differences.

A similar difference in inference latency can be seen when comparing MobileNetV2 against its quantized version, where the quantized version is more than $2.8$ times faster ($33.1$ms vs. $11.7$ms), confirming the study results of \cite{Quantizing} presented in section \ref{chap:quant}.
While there is a difference in latency between the quantized and non-quantized versions of MobileNetV2, there is no significant discrepancy in memory consumption ($126.2$MB vs. $119.5$MB), when considering the standard deviation of the quantized MobileNetV2, which is $10.8$MB.


\paragraph{$\mathbf{CPU_{inference}}$}
While having a big impact on both latency and memory, the different models have 
negligible CPU usage differences (see figure \ref{fig:EdgeVsCloudInferenceCPU}), but probably not on GPU usage, which we can not report in this thesis.
The $CPU_{inference}$ of all three models are around $8\%$, which no model using significantly less or more than the other models.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%Edge Inference Prepro Vs Inference%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mathbf{Latency_{total}}$}
Figure \ref{fig:EdgeInferenceRatio} shows $Latency_{total}$ results of edge inference, specifically the latency ratio between preprocessing and inference.
As image sizes increases, the preprocessing becomes more and more the bottleneck, especially for the MobileNetV2 models.
For the MobileNetV2 even $224\times224$ images take longer to preprocess than to perform the inference on them. The larger $Latency_{inference}$ result for MobileNetV2 $16$MP is caused by a single outlier.
%The Inception models are far more computational intensive, hence the longer inference latencies, but still 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_inference_plots/Edge_Preprocessing_+_Inference.pdf}
\caption[Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$]{Edge Inference - Ratio of Preprocessing and Inference in $Latency_{total}$ - lower is better: Preprocessing becomes the decisive factor for $Latency_{total}$ of both models for larger image sizes}
\label{fig:EdgeInferenceRatio}
\end{figure}


\paragraph{Edge Inference - Key Takeaways}
\emph{
\begin{itemize}
    \item Preprocessing becomes bottleneck for larger image sizes.\\
          $16$ MP image in comparison to a $0.05$MP image needs
    \begin{itemize}
        \item $10\times$ slower $Latency_{preprocessing}$
        \item $1.5\times$ more $Memory_{preprocessing}$
    \end{itemize}
    \item NNAPI leads to way better performance across all models:
    \begin{itemize}
        \item $4\times$ faster $Latency_{inference}$
        \item $1.2\times$ less $Memory_{inference}$
        \item $3\times$ less $CPU_{inference}$
    \end{itemize}
    \item In comparison to InceptionV4 MobileNetV2 leads to:
    \begin{itemize}
        \item $8.6\times$ faster $Latency_{inference}$
        \item $2.2\times$ more $Memory_{inference}$
    \end{itemize}
    \item Quantization of MobileNetV2 results in $2.3$x faster $Latency_{inference}$
    \item Preprocessing affects latency more than inference across all images sizes for small networks.
\end{itemize}}

\FloatBarrier
\subsection{Cloud Inference}
\label{chap:CloudResults}
This section deals with the results of the cloud inference benchmarks and their evaluation, divided into preprocessing and inference.
Note that the first subsections only cover the results for a batch size of one, for the results of larger batch sizes please refer to subsection \ref{chap:resultsBatchSize}.
We are evaluating $Latency_{inference}$ for the most cloud inference results, thus including the network component $Latency_{network}$, because this factor would also be present in real-world AI applications. In our case we have a high-speed connection with low network latency, thus the network latency represents a lower bound for real-time AI applications.
Listing \ref{lst:network} shows a network bandwidth test done with iperf3\footnote{https://github.com/esnet/iperf} between the Android phone and the cloud-backend. This test confirms the assumption about the network speed, as we achieved up to $5.77 Gbits/sec$.
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[label = lst:network, caption = iperf3 network connection test between cloud and edge, escapeinside={(*}{*)}]
iperf3 -c 10.155.47.236 -P 70 -w 90000000 -t 2
Connecting to host 10.155.47.236, port 8500
[  4] local 10.181.61.49 port 43574 connected to ...
[  6] local 10.181.61.49 port 43576 connected to ...
[  8] local 10.181.61.49 port 43578 connected to ...
 ...
[142] local 10.181.61.49 port 43712 connected to ...
[ ID] Interval           Transfer     Bandwidth
[  4]   0.00-1.00   sec  9.95 MBytes  83.5 Mbits/sec                  
[  6]   0.00-1.00   sec  9.90 MBytes  83.0 Mbits/sec                  
[  8]   0.00-1.00   sec  9.92 MBytes  83.2 Mbits/sec                  
 ...     
[138]   0.00-1.00   sec  9.74 MBytes  81.7 Mbits/sec                  
[140]   0.00-1.00   sec  9.72 MBytes  81.5 Mbits/sec                  
[142]   0.00-1.00   sec  9.71 MBytes  81.4 Mbits/sec                  
(*\bfseries[SUM]   0.00-1.00   sec   688 MBytes  5.77 Gbits/sec  *)  

\end{lstlisting}
\end{tabular}
\end{center}
%%add network proof (ping and upload)
\subsubsection{Preprocessing}
Cloud Inference allows two preprocessing methods, either on the edge beforehand or directly on the cloud (edge preprocessing and cloud preprocessing).
This section presents the results of these two methods, especially their impact on the resource consumption of edge devices.

\paragraph{$\mathbf{Latency_{preprocessing}}$ \& $\mathbf{Memory_{preprocessing}}$}
Figures \ref{fig:cloudInferencePreproLat} and \ref{fig:cloudInferencePreproMemory} display the effect of preprocessing on either edge or cloud on the preprocessing latencies and memory consumption at the edge in respect to the different deep learning models and image sizes.

For Edge preprocessing, $Memory_{preprocessing}$ and $Latency_{preprocessing}$ are heavily affected by rising image sizes, but not by the different models and their different image input sizes.
Since preprocessing at the edge is very similar as in the edge inference case, please refer to section \ref{chap:edgePrepro} for full details on the edge preprocessing results.

Preprocessing on the cloud leads to a significant decrease in $Latency_{preprocessing}$, which is expected since nearly no preprocessing steps are done at the edge except building a \emph{PredictRequest} object for TensorFlow Serving.
Preprocessing an $0.05$MP image on the edge instead of the cloud causes $20$ times ($1.9/39.8$ms) higher $Latency_{preprocessing}$ and an increase of factor $11$ ($37.9/431.1$ms) for an $16$MP image.
The impact of larger image sizes on memory in the case of cloud preprocessing is marginal, especially in comparison for the edge preprocessing counterparts. This is expected, since only compressed \emph{PNG} images are loaded into memory, in contrary to edge preprocessing, where in addition to the decoded \emph{PNG} images the resized images are also loaded to memory simultaneously.
While $Memory_{preprocessing}$ is lower for cloud preprocessing, the difference is nowhere as substantial as the latency differences. $16$MP images need $1.3$ times ($137.7/179.6$MB) more memory if preprocessing is done at  the edge instead of at the cloud.
%%Mention CPU

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Latency.pdf}
\caption[Cloud Inference:  $Latency_{preprocessing}$ - lower is better]{Cloud Inference:  $Latency_{preprocessing}$ - lower is better - Cloud preprocessing is significantly faster than edge preprocessing for all image sizes.}
\label{fig:cloudInferencePreproLat}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Preprocessing_Memory.pdf}
\caption[Cloud Inference:  $Memory_{preprocessing}$ - lower is better]{Cloud Inference:  $Memory_{preprocessing}$ - lower is better - The larger the image, the larger the difference between the preprocessing methods for cloud inference}
\label{fig:cloudInferencePreproMemory}
\end{figure}

\FloatBarrier
\subsubsection{Inference}
This section focuses on performance differences between the models and the impact of cloud inference with cloud preprocessing on cloud inference.
The inference metrics for cloud preprocessing include inference and the preprocessing steps, which are done at the cloud.

%%Also includes preprocessing!!!!
\paragraph{$\mathbf{Memory_{inference}}$}
Looking at the memory consumption during inference in figure \ref{fig:cloudInferenceInferenceMemory} one can see that $Memory_{inference}$ is very stable across all image sizes for edge preprocessing, which logical, since all image have been preprocessed to the same shape ($0.05$MP), therefore all requests sent to TensorFlow Serving have the same size.
Although MobileNetV2 uses $8.4$MB less memory on average than InceptionV4, this difference is not significant since the standard deviations of both are $3.7$ and $5.6$MB.
However, this tendency of MobileNetV2 consuming less memory could be due to its smaller model input size.

For cloud preprocessing the memory consumption increases for increasing image sizes, as larger images are being sent to the server, thus larger images are loaded into the \emph{PredictRequest} object, that is being sent to the  TensorFlow Serving instance at the cloud-backend.
$299^2/224^2$ images consume $123.8/112.34$MB, while $16$MP images $140.1/133.76$, therefore $Memory_{inference}$ increases by $13/19\%$ for InceptionV4/MobileNetV2 respectively.
%%add factor here
There are no significant $Memory_{inference}$  differences between the two models for cloud preprocessing, since un-preprocessed images are being sent, except for the $0.05$MP images, where the smaller $224^2$ image consumes $11.5$MB less memory on average (standard deviations $3$MB and $5.2$MB).
This makes little sense since the actual file size difference between the $299^2$ and $224^2$ \emph{PNG} files is only $58$KB ($141$KB vs. $83$KB).
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Memory.pdf}
\caption[Cloud Inference:  $Memory_{inference}$ - lower is better]{Cloud Inference:  $Memory_{inference}$ - lower is better - Cloud preprocessing causes higher memory consumption during inference during larger payloads being sent the TensorFlow Serving cloud-backend.}
\label{fig:cloudInferenceInferenceMemory}
\end{figure}
\paragraph{$\mathbf{Latency_{inference}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Latency.pdf}
\caption[Cloud Inference:  $Latency_{inference}$ - lower is better]{Cloud Inference:  $Latency_{inference}$ - lower is better - Cloud preprocessing is faster for $0.05$MP images, but slower for all other image sizes due to cost of resizing}
\label{fig:cloudInferenceInferenceLatency}
\end{figure}
\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_cloudprepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInferenceratioCloudtotal} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Server_+_NetworkLatencies_edgeprepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInferenceRatioEdgetotal}
\end{subfigure}

\caption[Cloud Inference:  Share of $Latency_{network}$ and $Latency_{server}$ - lower is better]{Cloud Inference:  $Latency_{inference}$ including $Latency_{network}$ and $Latency_{server}$ - lower is better - 
Share of $Latency_{network}$ in $Latency_{inference}$ does not significantly increase across the images sizes, thus being not the reason for performance drop off of cloud inference with cloud preprocessing. For Edge preprocessing the network accounts up to $30\%$ of the inference latency in case of MobileNetV2}
\end{figure}

The $Latency_{inference}$ results can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
Similar to the memory results, the latency for edge preprocessing stays the same across all image sizes, since all image sizes are preprocessed on the edge beforehand. 
Mean latencies are $181.2$ms and $108.46$ms for InceptionV4 and MobileNetV2 respectively, therefore MobileNetV2 is $1.7$ times faster.
Figure \ref{fig:CloudInferenceRatioEdgetotal} displays the share of $Latency_{network}$ and $Latency_{server}$ in $Latency_{inference}$ and one can see that the network is accountable for about $20\%$ of the  inference latency for InceptionV4 and $30\%$ for MobileNetV2, independent of image size.

If the images are preprocessed on the cloud, the preprocessing step on the cloud has a significant impact on $Latency_{inference}$, as can be seen in figure \ref{fig:cloudInferenceInferenceLatency}.
While an $0.05$MP image has an latency of $95.8/64.5$ms for InceptionV4/MobileNetV2, a $16$MP image needs $21.9/17.8$ times longer with a $1702.3/1414.8$ms mean latency.
This increase can be observed across other images sizes as well for the same extent. 
Looking at the share of $Latency_{network}$ in $Latency_{inference}$ in figure \ref{fig:CloudInferenceratioCloudtotal}, the network takes up to $50\%$ of the inference latency for $0.05$MP images, but shrinks to less than $15\%$ for all remaining image sizes.
Therefore it can be stated that the network is not the reason for the increase in latency for the larger image sizes, but rather the preprocessing done on the cloud-backend by TensorFlow Serving.
We think Tensorflow's \emph{resize\_bilinear} function, which we use to resize the images, causes the bottleneck in the preprocessing step on the cloud.
%%add GPU kernel optimisations issues
The use of an other resize approach like nearest-neighbor could speed up preprocessing, but would probably have an impact on the accuracy of the predictions.
Note that while the network connection in our experimentation environment is fast enough to prevent any bottlenecks caused by the network, a slower network connection could slow down inference for large image sizes.
%warum preprocessing s langsam bei tensorflow serving?

%%cloud preprocessing bigger images higher variance
Comparing the two cloud inference options, edge and cloud preprocessing,  $Latency_{inference}$ values for edge preprocessing of both models are faster for all image sizes except the $0.05$MP images.
We believe the $0.05$MP images are faster because of the lower I/O overhead in comparison to the larger preprocessed counterparts, even if the un-preprocessed images still have the decoded and normalized.

\paragraph{$\mathbf{Latency_{total}}$}

\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_cloud_prepro.pdf}
   \caption{Cloud Preprocessing}
   \label{fig:CloudInference+PreproCloud} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Preprocessing_Inference_Comb_edge_prepro.pdf}
   \caption{Edge Preprocessing}
   \label{fig:CloudInference+PreproEdge}
\end{subfigure}

\caption[Cloud Inference:  $Latency_{total}$  - lower is better]{Cloud Inference:  $Latency_{total}$ ($Latency_{preprocessing}+Latency_{inference}$) - lower is better - 
Cloud preprocessing performs only better than cloud inference with edge preprocessing for $0.05$MP images, all other image sizes perform worse}
\end{figure}

This trend oberserved in $Latency_{inference}$ stays also true for $Latency_{total}$ (see figures \ref{fig:CloudInference+PreproCloud} and \ref{fig:CloudInference+PreproEdge}), which includes both $Latency_{preprocessing}$ and $Latency_{inference}$. 
$0.05$MP images need $223.1/147.2$ms $Latency_{total}$ for InceptionV4/MobileNetV2 edge preprocessing, while cloud preprocessing needs only $96.9/64.3$ms, therefore cloud preprocessing is $2.3$ times faster.
In contrast cloud inference with edge preprocessing is faster by a factor of $2.7$ for $16$MP images, with the latencies of InceptionV4/MobileNetV2 being $626.7/532$ms for edge preprocessing and $1673.4/1453.9$ms for cloud preprocessing. $2$MP images are $1.7/1.8$ times faster when using edge preprocessing, $4$MP $2.1/2.2$ and $8$MP $2.6/3$ times faster.
%latency edge vs cloud prepro inference:

While MobileNetV2 is $1.75$ times faster in $Latency_{inference}$ for edge preprocessing than InceptionV4, this latency difference shrinks for cloud preprocessing for larger images, started by a difference of factor $1.52$ for $0.05$MP images and shrinking to $1.25/1.21/1.09/1.15$ for the respective image sizes $2/4/8/16$MP.
We believe this decrease is due to the fact as images for MobileNetV2 need to be resized to a smaller input size than InceptionV4, hence increasing the already high resize overhead and thus narrowing the latency difference between both models.
When comparing memory consumption of cloud inference with ether cloud preprocessing or edge preprocessing, the difference is only $3\%$ for  $0.05$MP images ($121.5$MB for edge preprocessing and $118.1$Mb for cloud preprocessing), but $14\%$ for $16$MP images (edge preprocessing $120.0$Mb, cloud preprocessing $136.9$MB).




\paragraph{$\mathbf{Data_{transmitted}}$ \& $\mathbf{Data_{received}}$}
\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Received_Data.pdf}
   \caption{$Data_{received}$}
   \label{fig:CloudInferenceReceivedData} 
\end{subfigure}

\begin{subfigure}[b]{0.95\textwidth}
   \includegraphics[width=1\linewidth]{./Bilder/single_plots/cloud_inference_plots/Cloud_Inference_Transmitted_Data.pdf}
   \caption{$Data_{transmitted}$}
   \label{fig:CloudInferenceTransmittedData}
\end{subfigure}

\caption[Cloud Inference:  $Data_{received}$ vs. $Data_{transmitted}$ - lower is better]{Cloud Inference:  $Data_{received}$ vs. $Data_{transmitted}$ - lower is better - 
With rising image size the data transmitted for cloud preprocessing rises as un-preprocessed images get larger.
For edge preprocessing the amount of data transmitted is constant across images sizes, as all preprocessed images are the same shape.
$Data_{received}$ is increasing along with $Data_{transmitted}$, since communication is done via \emph{TCP}, thus more sent packages result in more acknowledgements from the server
}
\end{figure}

Figures \ref{fig:CloudInferenceReceivedData} and \ref{fig:CloudInferenceTransmittedData} show the data transmitted and received from the edge client to the cloud-backend server for the different image sizes and preprocessing options.
For edge processed image the mean transmitted data is $1061.9/608.7$KB for InceptionV4/MobileNetV2 and $16.7/10.3$KB data received by the respective models.
Therefore $299^2$ images require $453.2$ kilobytes more than $224^2$ images.
For cloud preprocessing $Data_{transmitted}$ the amount of data sent is at maximum $100$KB larger than the sizes of the \emph{PNG} images ($224^2$: $83$KB, $299^2$: $141$KB, $2$MP: $2411$KB, $4$MP: $4309$KB, $8$MP: $7515$KB,  $16$MP: $10077$KB).
For both edge and cloud preprocessing the $Data_{received}$ rises for rising $Data_{transmitted}$ values, since the communication between server and client is done via \emph{TCP}, thus more sent data results in more packages resulting in more \emph{ACK} signals getting sent back to the client.
%%NR:[114.5] transmitted
%%NR:[3.] received
%%2MP:[2446.2] transmitted
%%2MP:[38.2] received
%%4MP:[4365.5] transmitted
%%4MP:[64.8] received
%%8MP:[7603.6] transmitted
%%8MP:[114.9] received
%%16MP:[10176.8] transmitted
%%16MP:[141.4] received
%299 png kleiner als 299 preporcessing
%2MP($1732\times1155$, $2411$KB), 4MP($2449\times1633$, $4309$KB), 8MP($3464\times2309$, $7515$KB) and 16MP($4899\times3266$, $10077$KB) ($224\times224$, $83$KB or $299\times299$, $141$KB



\FloatBarrier
\paragraph{Cloud Inference - Key Takeaways}
%cloud prepro less preprocessing memory and latency
%cloud prepro more inference memory
%cloud preprocessing very slow for large image sizes
%edge prepro faster for all image sizes except 224/299
%network not a bottlneck
%resize binlinear bad implementation?
\emph{
\begin{itemize}
    \item Preprocessing on the cloud in comparison to edge preprocessing causes:
    \begin{itemize}
        \item $20\times$ faster $Latency_{preprocessing}$ for $0.05$MP images
        \item $10\times$ faster $Latency_{preprocessing}$ for $16$MP images
        \item $1.28\times$ less $Memory_{preprocessing}$ for $16$MP images
    \end{itemize}
    \item Inference on the cloud including preprocessing in comparison to cloud inference without preprocessing causes:
    \begin{itemize}
        \item $2.3\times$ faster InceptionV4/MobileNetV2 $Latency_{total}$ for $0.05$MP images
        \item $2.7\times$ slower InceptionV4/MobileNetV2 $Latency_{total}$ for $16$MP images
        \item $0.97\times$ less $Memory_{inference}$ for $0.05$MP images
        \item $1.14\times$ more $Memory_{inference}$ for $16$MP images
    \end{itemize}
    \item MobileNetV2 is up to $1.75\times$ faster than InceptionV4 for smaller images, but shrinking to around $10\%$ faster for large image in case of cloud preprocessing.
    \item Network is accountable for up to $50\%$ of $Latency_{inference}$ for $0.05$MP images and networks, but less than $30\%$ for larger images and network for both cloud and edge preprocessing.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Edge Preprocessing has faster $Latency_{total}$ latencies for all image sizes except $0.05$MP.
\end{itemize}
}%emph end


\subsection{Edge vs. Cloud Inference}
\label{chap:EdgeCloudResults}
Previous two sections presented the results for both edge and cloud inference.
Now, the results of the cloud and the edge inference with batch size one, including preprocessing, are compared against each other.
First preprocessing and inference are evaluated separately and afterwards both of the steps combined. 
Each plot in this section contains five subplots, one for each image sizes, the x-axis contains the deep learning model, while the y-axis displays the various performance metrics. 
The legend used in all those plots in explained in table \ref{table:legendPlots}.
\begin{table}[!htb]
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}
\centering
\caption{Explanation of the plot legends}
\label{table:legendPlots}
\begin{tabular}{@{}lll@{}}
\toprule
Inference on & Description & Color \\ \midrule
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:CLOUD\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ cloud-backend.\end{tabular} &  \crule[gruen]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:CLOUD;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference is done on cloud-backend, but images are \\ preprocessed on edge beforehand.\end{tabular} &  \crule[orangedunkel]{0.8cm}{0.8cm}\\
\begin{tabular}[c]{@{}l@{}}Inf on:EDGE;\\ Prepro on:EDGE\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference as well as preprocessing is done on the\\ edge.\end{tabular} & \crule[lila]{0.8cm}{0.8cm} \\ \bottomrule
\end{tabular}
\end{table}

Note that for the quantized MobileNetV2 we only conduct edge inference experiments. Thus no cloud inference results for this model can be seen in the plots of this section.
\subsubsection{Preprocessing}

Figures \ref{fig:EdgeVsCloudPreproMemory}, \ref{fig:EdgeVsCloudPreproLat} and \ref{fig:CloudEdgePreproCPU} report the results of the preprocessing metrics $Memory_{preprocessing}$, $Latency_{preprocessing}$ and $CPU_{preprocessing}$.

%Memory
\paragraph{$\mathbf{Memory_{preprocessing}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Memory.pdf}
\caption[Edge vs. Cloud Inference:  $Memory_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference:  $Memory_{preprocessing}$ - lower is better -
While for small images sizes difference between preoprocessing options are small, the larger the difference becomes in factor of cloud preprocessing}
\label{fig:EdgeVsCloudPreproMemory}
\end{figure}
Both edge preprocessing options, for either edge inference or cloud inference, have very similar memory consumption results with the cloud inference mean values being at most $5.5\%$ higher from the edge inference values for all image sizes.
This due to the fact that both options share most of the preprocessing steps, the only difference is that in case of cloud inference with edge preprocessing the \emph{ByteBuffer}, containing the preprocessed image, gets used to build the \emph{PredictRequest} for TensorFlow Serving. 
For edge inference no such step is needed as the \emph{ByteBuffer} can be directly be used to call the run function of the TensorFlow Lite interpreter, which starts the inference process at the edge.
Thus the $5.5\%$ overhead in memory can be explained by the additional \emph{PredictRequest} object that has to loaded into memory for cloud inference, but this difference is not significant.

In case of small images the memory consumption of cloud preprocessing is very close to its edge preprocessing counterpart, the larger the image, the larger the memory difference in favor of cloud preprocessing.
While the difference is only $1.9\%$ for an $0.05$MP image ($114.53/116.69$MB), the difference grows to an $22.5\%$ decrease ($177.8/137.8$MB) in memory for $16$MP images ($2$MP: $1.1\%$, $4$MP: $7.9\%$, $8$MP: $9.9\%$).
This difference is caused by the fact that for cloud preprocessing only the compressed \emph{PNG} gets loaded into memory, which is considerably smaller than the decoded image, especially for large images such as the $16$MP image.

When comparing the different models, InceptionV4 ($140.1$MB) consumed in general about $7.9$MB more $Memory_{preprocessing}$ than MobileNetV2 ($132.2$MB), or about $6\%$. This is not significant, since the standard deviations for both model are higher than the difference.
Edge inference of the quantized version of MobileNetV2 causes the same memory consumption as the non-quantized version.
When differentiating between the different image sizes and inference modes the difference is always between $4-8\%$, except for cloud preprocessing of a $0.05$MP image, where the difference rises to $10\%$ ($11.35$MB). This tendencies can be caused by the fact that for MobileNetV2 $224^2$ images are preprocessed and for InceptionV4 $299^2$ images, but the standard deviations do not allow definitive conclusions.

%Latency
\paragraph{$\mathbf{Latency_{preprocessing}}$}
The preprocessing latency results (see figure \ref{fig:EdgeVsCloudPreproLat}) are similar to the memory results. Both edge preprocessing options are very similar in performance, while cloud preprocessing performs better, but in case of latency the difference is larger.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Preprocessing_Latencies.pdf}
\caption[Edge vs. Cloud Inference:  $Latency_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference:  $Latency_{preprocessing}$ - lower is better - Cloud preprocessing is significantly faster than edge preprocessing, as preprocessing step is outsourced to the cloud, minimizing $Latency_{preprocessing}$ at the edge}
\label{fig:EdgeVsCloudPreproLat}
\end{figure}

The difference between the edge preprocessing options (cloud inference with edge preprocessing and edge preprocessing) is at most $4.9\%$ ($5.1$ms) across all images sizes and therefore not significant (not differentiating between the different models).

When comparing edge preprocessing to cloud preprocessing the latency gap for $0.05$MP images is by a factor of $20$ in favor of cloud preprocessing ($1.9/38.2$ms). For larger image sizes this gap shrinks, but still amounts up to  $11\times$ for $16$MP images ($37.9/430.78$ms).

The different models have little to no impact on the preprocessing when comparing same images sizes.
For cloud preprocessing the MobileNetV2 and InceptionV4 differ at most $4.9$ms (for $8$MP, all other image sizes are below that for both relative and total difference). This difference is not significant since the standard deviation for the mean values is more than twice as high for both models.
%%difference between models for edge preporcessing
%%%model compare
%cloud prepro no diff for models
%%diff for NR large for edge prepro -> normalizing more work
This is also true for edge preprocessing for all image sizes, regardless of edge or cloud inference.
Although the difference between the models is up to $29\%$ or $10.3$ms ($0.05$MP image), the speedup is not significant as the standard deviation for both models is above $9$ms.

%CPU
\paragraph{$\mathbf{CPU_{preprocessing}}$}
Looking at the CPU usages in figure \ref{fig:CloudEdgePreproCPU} it can be seen that the variance of all inference modes, models and images sizes is too large to make conclusions, although there is an indication that cloud preprocessing causes lower usages.
Overall it can be stated that all usages are around $12.5\%$. This makes sense since the preprocessing of a single image is done on a single thread, thus running on one core and since the OnePlus 6T has 8 cores, the full utilization of a single core causes an overall $CPU_{preprocessing}$  usage of $12.5\%$. 
This infers that the preprocessing uses $100\%$ of a single core, especially for large images.





\FloatBarrier
\subsubsection{Inference}
\paragraph{$\mathbf{Memory_{inference}}$}
Figure \ref{fig:EdgeVsCloudInferenceMemory} reports $Memory_{inference}$ for all inference modes, models and image sizes.
Image size has no impact on the memory consumption during inference, where images have been previously preprocessed at the edge, which is logical since the preprocessed images have the same memory constraints regardless of the size before preprocessing.
In contrary cloud inference with cloud preprocessing memory consumption is impacted by different image sizes, since the images are not preprocessed and thus larger images are being sent to the server.
$16$MP images use $19\%$ more memory for InceptionV4 in comparison to a $0.05$MP image and $13\%$ more for MobileNetV2.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Memory.pdf}
\caption[{Edge vs. Cloud Inference:  $Memory_{inference}$ - lower is better}]{Edge vs. Cloud Inference:  $Memory_{inference}$ - lower is better - InceptionV4 has significant impact on memory consumption in case of edge inference, while MobileNetV2 small size has very little impact}
\label{fig:EdgeVsCloudInferenceMemory}
\end{figure}

While the different images sizes only have a small impact on $Memory_{inference}$ different inference modes (edge inference, cloud inference with either edge or cloud preprocessing) do have considerable performance differences, especially in the case of the large InceptionV4 network.
Edge Inference with InceptionV4 consumes more than twice ($2.2\times$) as much memory than both cloud inference options.

While cloud inference for InceptionV4 can half memory consumption, MobileNetV2 does not profit nearly as much from cloud inference, as edge inference with MobileNetV2 only uses $10.5\%$ ($13.6$MB) more an average (independent of image size) than cloud inference with edge preprocessing due to its small model size.
Edge inference with MobileNetV2 uses less memory than cloud inference with cloud preprocessing for $16$MP images, as the un-preprocessed image consumes more memory than the loaded model. 

The quantization of MobileNetV2 has no significant difference on memory consumption compared to its non-quantized version.


%inception on egde much more, mobilenet nearly equal (more for small images, less forlarge images than cloud prepro)
\paragraph{$\mathbf{Latency_{inference}}$}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies_onlyNR.pdf}
\caption[Edge vs. Cloud Inference:  $Latency_{inference}$ of $0.05$MP images - lower is better]{Edge vs. Cloud Inference:  $Latency_{inference}$ of $0.05$MP images - lower is better - For InceptionV4 cloud inference is the fastest option, while for MobileNetV2 edge inference is the preferred option.}
\label{fig:EdgeVsCloudInferenceLatNR}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_Latencies.pdf}
\caption[Edge vs. Cloud Inference:  $Latency_{inference}$ - lower is better]{Edge vs. Cloud Inference:  $Latency_{inference}$ - lower is better -
While for $0.05$MP images cloud inference with cloud preprocessing is the fastest options, for larger images the impact of preprocessing makes it the slowest option}
\label{fig:EdgeVsCloudInferenceLat}
\end{figure}

Results for $Latency_{inference}$ can be seen in figures \ref{fig:EdgeVsCloudInferenceLat} and \ref{fig:EdgeVsCloudInferenceLatNR}. The latter only contains the latency results for the $0.05$MP size for better legibility and the first one for all images sizes.
These figures show the substantial difference in latency between the different deployment options, models and image sizes.
Recall that the cloud inference results include the network latency $Latency_{network}$, because this factor would also be present in real-world AI applications. 
In our case we have a high-speed connection with low network latency, thus being a lower bound for real-time AI applications. 
However, the same plot without $Latency_{network}$ can be seen in figure \ref{fig:CloudEdgeInfLatWONetwork}.

For InceptionV4 and $0.05$MP images cloud inference with cloud preprocessing is the fastest inference method with a mean of $94.9$ms, followed by cloud inference with edge inference ($180.4$ms) and edge inference with edge preprocessing ($235.9$ms).
Therefore cloud inference with cloud preprocessing is $1.9$ times faster than cloud inference with edge preprocessing and $2.5$ times faster than edge inference, in case of InceptionV4.
We believe this difference between the two cloud inference options is due to lower I/O overhead as well as smaller network latency in case of cloud preprocessing, since the compressed $0.05$MP \emph{PNG} images are smaller in size than the decoded counterparts. 

In contrary in the case of MobileNetV2 edge inference is the fastest option for $0.05$MP images with a mean of $27.7$ms, followed by cloud inference with cloud preprocessing ($62.5$ms) and lastly cloud inference with edge preprocessing ($110.2$ms).
Hence edge inference of MobileNetV2 is $2.3$ times faster than cloud inference with cloud preprocessing and $4$ times faster than cloud inference with edge preprocessing.
When neglecting the $Latency_{network}$ part in $Latency_{inference}$, cloud inference with cloud preprocessing is nearly as fast as edge inference of MobileNetV2 with a difference of $0.26$ms.

Therefore the fastest $0.05$MP image option for MobileNetV2 is the edge inference option, while for InceptionV4 cloud inference is the optimal choice.

The quantized version of MobileNetV2 has a mean inference latency of $11.7$ms, thus being $1.9$ times faster than edge inference of non-quantized MobileNetV2 and $8.1$ times faster than the best InceptionV4 result.

While the various images sizes have no impact on inference methods, where images have been previously preprocessing at the edge, they have a severe impact on cloud inference with cloud preprocessing.
With the average latencies for InceptionV4/MobileNetV2 for an $16$MP images being $1633.9/1417.6$ms, the difference to a $0.05$MP image is more than factor $17/22$.
%%standard deviation larger for cloud preprocessing of large images

\paragraph{$\mathbf{CPU_{inference}}$}
%not interesting
Figure \ref{fig:EdgeVsCloudInferenceCPU} shows that there are no severe differences in CPU usage between edge and cloud inference, regardless of model and image size, at least for the case of batch size of one.
All usages are below $12.5\%$, therefore no core is utilized fully since the OnePlus 6T used in our experiments has $8$ cores.


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Inference_CPU.pdf}
\caption[Edge vs. Cloud Inference:  $CPU_{inference}$ - lower is better]{Edge vs. Cloud Inference:  $CPU_{inference}$ - lower is better -
CPU usages across all inference modes, image sizes and models are below $10\%$}
\label{fig:EdgeVsCloudInferenceCPU}
\end{figure}


\paragraph{$\mathbf{Throughput_{inference}}$}
Since $Throughput_{inference}$ is directly computed from $Latency_{inference}$, the results for this metric in figure \ref{fig:EdgeVsCloudinferneceThroughput} have the same tendencies as the latencies.
Hence cloud inference with cloud preprocessing is the best inference option for InceptionV4, at least in the case of $0.05$MP images with a mean throughput of $10.8$ predictions per second.
Meanwhile for MobileNetV2 reaches up to $36.6$ predictions per second, when inference is done at the edge.
The quantized version of MobileNetV2 can make up to $83.7$ predictions per second.

While $Throughput_{inference}$ stays constant across all images sizes for edge preprocessing options, cloud inference with cloud preprocessing is impacted severely with a drop to $0.6$ predictions per second for the case of InceptionV4, $16$MP.


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Throughput_without_Preprocessing.pdf}
\caption[Edge vs. Cloud Inference:  $Throughput_{inference}$ - higher is better]{Edge vs. Cloud Inference:  $Throughput_{inference}$ - higher is better -
The quantized version of MobileNetV2 achieves $2.3$ times higher throughput than the highest MobileNetV2 option and $7.75$ times more than the highest InceptionV4 option}
\label{fig:EdgeVsCloudinferneceThroughput}
\end{figure}

\paragraph{$\mathbf{Throughput_{total}}$}
So far we only evaluated the results of preprocessing and inference separately, but especially for the metrics throughput and latency the sum of both preprocessing and inference is of interest.%,since 
When looking at the $Throughput_{total}$, which is derived from the sum of $Latency_{preprocessing}$ and $Latency_{inference}$, in figure \ref{fig:EdgeVsCloudTotalThroughput} the impact of preprocessing on the throughput can be seen.
While this impact is small for cloud inference with cloud preprocessing, because $Latency_{preprocessing}$ values for this option are minimal, the impact is significant for inference methods with edge preprocessing.
However, even when accounting for both preprocessing and inference, cloud inference with cloud preprocessing performs way worse for large image sizes than both edge preprocessing methods.

First, we evaluate the results for the $0.05$MP images.
When including preprocessing the throughput of MobileNetV2 on the edge drops from $36.6$ to $16.1$, since preprocessing takes longer than the actual inference, even for an image where no resizing has to be done.
which is very close to the best cloud inference option for this model ($15.7$).
For the quantized version of MobileNetV2 the preprocessing latency is the same as for the non-quantized counterpart, but the lower inference latency leads to a total throughput of $23.1$.
The highest $Throughput_{total}$ for InceptionV4 is $10.4$, which is achieved by cloud inference with cloud preprocessing.

When analyzing the larger images sizes preprocessing takes a significant toll on all deployment options, as throughput of MobileNetV2/InceptionV4 drops to $7.8/3.6$, $5.3/2.8$, $3.3/2.2$, $2.1/1.6$ for the image sizes $2$MP, $4$MP, $8$MP and $16$MP respectively (reporting the values for the best inference option for each image size).
Even though cloud inference with preprocessing has the lowest preprocessing latency, this options remains the worst inference option for all except the $0.05$MP images.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/edge_vs_cloud_plots/Edge_vs_Cloud_Inference_Throughput_with_Preprocessing.pdf}
\caption[Edge vs. Cloud Inference:  $Throughput_{total}$ - higher is better]{Edge vs. Cloud Inference:  $Throughput_{total}$ (Inference + Preprocessing) - higher is better - 
When looking at the $Throughput_{total}$ results compared to the $Throughput_{inference}$ results, it can be seen that preprocessing has severe impact on edge preprocessing options, especially for large images, as the $Latency_{preprocessing}$ increases}
\label{fig:EdgeVsCloudTotalThroughput}
\end{figure}





Concluding it can be stated that the edge is the preferred deployment option for MobileNetV2, both quantized and non-quantized versions, in respect to latency/throughput as well as memory consumption.
The impact of I/O and network latency added by cloud inference is not feasible for these small models.

While the edge is preferred for small models like MobileNetV2, InceptionV4's architecture is too demanding for the current edge devices, thus cloud inference is the better option for these large models.
Cloud preprocessing is only viable for the small $0.05$MP images, as it lowers I/O costs and data consumption. For all other images sizes it increases $Latency_{total}$, $Data_{transmitted}$ and $Memory_{inference}$, while only having little positive effect on $Memory_{preprocessing}$ compared to the other inference options.

In general, it can be stated that the impact of preprocessing is significant, even for small images sizes, thus the preprocessing should be minimized as much as possible.

\FloatBarrier
\paragraph{Edge vs. Cloud Inference - Key Takeaways}
\emph{
\subparagraph*{Preprocessing}
\begin{itemize}
    \item Both edge preprocessing methods for either edge or cloud inference are very similar in preprocessing performance.
    \item No major preprocessing performances difference between different model types.
    \item Cloud inference with cloud preprocessing compared to edge preprocessing options causes
    \begin{itemize}
        \item  $1.9\%$ for NR, $22.5\%$ for 16MP less $Memory_{preprocessing}$ 
        \item $20\times$ for NR, $11\times$ for 16MP faster $Latency_{preprocessing}$ 
        \item No significant differences in $CPU_{preprocessing}$
    \end{itemize}
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Preprocessing cost should be minimized as much as possible.
\end{itemize}
\subparagraph*{Inference}
\begin{itemize}
    \item Different images sizes only impact the inference performance of cloud inference with cloud preprocessing.
    \item Edge inferences compared to cloud inference causes:
    \begin{itemize}
        \item  $2.2\times$ higher $Memory_{inference}$ for InceptionV4, for MobileNetV2 only $10.5\%$ increase (also quantized).
        \item $1.9\times$ higher $latency_{inference}$ for Inception, $2.3\times$ faster for MobileNetV2.
        \item No significant difference in $CPU_{inference}$.
    \end{itemize}
    \item Best inference option for InceptionV4 can achieve a $Throughput_{inference}$ of $10.8$ (cloud inference with cloud preprocessing), MobileNetV2 $36.6$ (edge inference) and MobileNetV2 quantized $83.7$ (edge inference) for $0.05$MP images.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Edge inference performs better for small models, cloud inference performs better for large models.
 \item Highest $Throughput_{total}$ for InceptionV4, including both preprocessing and inference, is $10.4$ (cloud inference with cloud preprocessing), for MobileNetV2 $16.1$ (edge inference) and for MobileNetV2 quantized $23.1$ (edge inference).
\end{itemize}
}

\subsubsection{Effect of larger Batch Sizes}
\label{chap:resultsBatchSize}
So far we only reported the results of the of the experiments with batch size of one, but in this section, we present the results for the batch sizes $1$, $2$, $16$, $32$.
Increasing the batch size is supposed to increase throughput at the expense of low latencies.
Since the TensorFlow Lite of the used MobileNetV2 version does not allow batch sizes larger than one, we can not report edge inference results for the two MobileNetV2 models.

Since the batch size is added as a new dimension in this section, the plots visualizing the results need to be adjusted accordingly.
Now the x-axis represents the various batch sizes and the models are moved to the legend. 
The legend is similar to table \ref{table:legendPlots}, but the model is added as an additional feature to the inference mode.
\subsubsection{Preprocessing}

\paragraph{$\mathbf{CPU_{preprocessing}}$}
While the CPU usages for batch sizes of one were not different for the different deployment options, for larger batch sizes this changes since in case of edge preprocessing the parallelism comes into play.
Recall that for batch sizes larger than one, images get resized and normalized in parallel in the case of edge preprocessing to increase throughput (for implementation details refer to section \ref{chap:preproImpl}).

For rising images size and batch size the CPU usages rise up to $92\%$, as can be seen in figure \ref{fig:BatchSizePreproCPU}, hence all core are utilized close to the fullest.


While CPU usages of the cloud preprocessing options also rise for increasing image/batch sizes, they stagnate at below $20\%$.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_CPU_Usage.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $CPU_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $CPU_{preprocessing}$ - lower is better - CPU usage for edge preprocessing significantly increases for larger batch and image sizes as parallelism takes effect }
\label{fig:BatchSizePreproCPU}
\end{figure}

\paragraph{$\mathbf{Memory_{preprocessing}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Memory.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Memory_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Memory_{preprocessing}$ - lower is better - The larger the image and batch size becomes, the larger the gap between the preprocessing options grows}
\label{fig:BatchSizePreproMemory}
\end{figure}

While the memory consumption results pictured in figure \ref{fig:BatchSizePreproMemory} stays close together for all deployment options for the small images sizes over the different batch sizes, the larger the image gets, the further the gap between the cloud and edge preprocessing methods becomes.
This growing gap can be explained by the fact that for cloud preprocessing only encoded \emph{PNG} images need to be loaded, but for edge preprocessing these images needed to be encoded, hence more memory is needed.
As the batch size and images size increases, the gap between decoded and encoded images grows, hence explaining the gap between the preprocessing methods.

The gap for a batch size of $32$ is $76.6$MB ($1.56\times$ increase) for $0.05$MP images, $133.8$MB ($0.74\times$ increase) for $4$MP images and $842.2$MB ($2.88\times$ increase) for $16$MP images. 
A similar progression can be observed for the other batch sizes (e.\,g. batch size $16$: $36.5$MB ($0.05$MP), $134.4$MB ($4$MP), $674.6$MB ($16$MP)).

For cloud preprocessing there is no difference in memory consumption between the different models, but for edge preprocessing InceptionV4 tends to use more memory, because of the models larger input size (InceptionV4 $299^2$, MobileNetV2 $224^2$).


\paragraph{$\mathbf{Latency_{preprocessing}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Preprocessing_Latencies.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Latency_{preprocessing}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Latency_{preprocessing}$ - lower is better - Latency is constantly increasing for increasing image and batch sizes. The gap between the preprocessing options grows with larger images sizes}
\label{fig:BatchSizePreproLatency}
\end{figure}

Figure \ref{fig:BatchSizePreproLatency} shows the progression of preprocessing latency results across the various image and batch sizes.
Similar to the $Memory_{preprocessing}$ results, there is no significant difference between the different models, but large differences between the preprocessing methods for increasing image and batch sizes.

If preprocessing $16$ megapixel images at the edge, it takes $1.1$ times longer to preprocess $2$ images instead of $1$, $2.9$ times longer to preprocess $16$ images instead of two and $1.96$ times longer to preprocess $32$ images instead of $16$.
This illustrates the effect of parallel preprocessing.

$Latency_{preprocessing}$ for cloud preprocessing behaves nearly linear, as $2$ $16$MP images take $1.85$ times longer than $1$ image does, $16$ images $17.1$ times longer and $32$ images $34.7$ times longer.
This is explained by the fact that in the case of cloud preprocessing all images only need to be concatenated consecutively before getting used to build the \emph{PredictRequest} object for TensorFlow Serving. 


While Cloud preprocessing takes $12.8$ms to preprocess $32$ $0.05$MP images, edge preprocessing takes $197.1$ms, hence an increase of $184.3$ms or factor $15.4$. For $4$MP images using the same batch size this gap grows to $556.9$ms (factor $2$), and for $16$MP images to $1375.3$ms (factor $2$).
For batch size $16$ the gap grows as follows: $92.3$ms ($0.05$MP), $297.6$ms ($4$MP), $724$ms ($16$MP).






\FloatBarrier
\subsubsection{Inference}

\paragraph{$\mathbf{CPU_{inference}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_CPU_Usage.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $CPU_{inference}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $CPU_{inference}$ - lower is better - Edge inference uses up to $92\%$ CPU, while cloud inference stagnates below $20\%$}
\label{fig:BatchSizeInferenceCPU}
\end{figure}

Looking at the $CPU_{inference}$ usages in figure \ref{fig:BatchSizeInferenceCPU}, the three different deployment options can be clearly identified for the larger image sizes.
While for a batch size of one the CPU usages was of little interest since there wasn't any major difference between the different deployment options, this changes for larger batch sizes.

The option with the highest usage is edge inference $CPU_{inference}$ does not grow for a batch size of $2$, but it grows to $92\%$ for batch size $16$ and stagnates at this value for the larger $32$ batch.
%%CPU NNAPI hier nennen?

The two other options only consume a fraction of the edge inference usage for the batch sizes $16$ and $32$.
CPU usage of Cloud inference with cloud preprocessing continuously grow over the course of larger images up to $18\%$ usages for the larger batch sizes starting at the $4$MP images.

Cloud inference with edge preprocessing consumes nearly as much as cloud preprocessing for the images sizes $0.05$MP and $2$MP, but starting to use less from $4$Mp images onwards.
The usages stay relatively consistently at around $8\%$ across all image sizes, as image have been preprocessed into the same shape before.

Cloud inference with cloud preprocessing uses more CPU than cloud inference with edge preprocessing, since the \emph{PredictRequest}, that is being sent to the TensorFlow Serving cloud-backend (see section \ref{chap:CloudInfImpl}) and contains the preprocessed or un-preprocessed images depending on deployment option, is larger in the case of cloud preprocessing.
Therefore the gRPC client, which our implementation of TensorFlow Serving uses, has to send requests with larger payloads to the server, resulting in higher CPU usages than smaller payloads like in the case of edge preprocessing would.


Therefore it can be stated that cloud inference consumes way less CPU resources for large batch and image sizes.



\paragraph{$\mathbf{Memory_{inference}}$}


\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Memory.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Memory_{inference}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Memory_{inference}$ - lower is better - Edge inference uses significantly more memory than cloud inference. Cloud inference with cloud preprocessing consumes more memory than cloud inference with edge preprocessing, especially for large images and batch sizes}
\label{fig:BatchSizeInferenceMemory}
\end{figure}

The memory consumption of inference with larger batches is very similar to the behavior of CPU usage.


Edge inference consumes substantially more than the cloud inference counterparts with InceptionV4 consuming $1289$MB for a batch size of $16$ and $2132.3$MB for batch size $32$.
The value for a batch size of $32$ is therefore more than $3$ times higher than the highest cloud inference result.
%Larger batch sizes affect the memory results of edge inferences significantly more than the cloud inference results.
%The worst cloud inference results for batch sizes $16$ and $32$ are still $x/s$ times better than the edge inference results.

Cloud preprocessing again uses more than cloud inference with edge preprocessing, since the preprocessed images are smaller in  images sizes than the large un-preprocessed \emph{PNG} images (except for $0.05$MP images).
For a batch size of $32$ and $16$ megapixel images, the difference between the two cloud inference options is $446.6$MB for Inception and $527.16$MB for MobileNetV2.
Hence cloud preprocessing uses $4.2$ times more memory for MobileNetV2 and $3$ times more for InceptionV4.




\paragraph{$\mathbf{Latency_{inference}}$}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Latencies.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Latency_{inference}$ - lower is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Latency_{inference}$ - lower is better - Edge inference takes significantly longer than cloud inference options, except for large images sizes, where cloud inference with cloud preprocessing starts to perform worse}
\label{fig:BatchSizeInferenceLatency}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Latencies_only_CLOUD_NR.pdf}
\caption{Edge vs.  Cloud Inference for larger Batch Sizes: $Latency_{inference}$ result of cloud inference and $224^2/299^2$ images - lower is better}
\label{fig:BatchSizeLatenciesCloud}
\end{figure}

Figure \ref{fig:BatchSizeInferenceLatency} reports the $Latency_{inference}$ results for the difference inference modes, batch and image sizes.

For a detailed view of the cloud inference results for the $0.05$MP images with more legibility please refer to figure \ref{fig:BatchSizeLatenciesCloud}.

Inference modes with edge preprocessing again have the same performance across all image sizes, hence only cloud inference with cloud preprocessing is affected by the various image sizes.
The larger the image sizes get, the closer the performance of the cloud preprocessing options get to the edge inference performance.

When comparing the results of the $0.05$MP images, $Latency_{inference}$ for InceptionV4 edge inference  increases by $11843.4$ms (factor $1.84$), when doubling the batch size from $16$ to $32$.
Meanwhile, in case of cloud inference the latency only increases by $383.1$ms (factor $1.88$) for cloud preprocessing and $1898.7$ms (factor $2.3$) for edge preprocessing.
This shows the superiority of cloud inference in terms of computational power.

Like for the batch size one results, cloud inference with cloud preprocessing is superior to cloud inference with edge preprocessing for $0.05$MP images but worse for all other image sizes.
We believe the reason behind this is the reduced I/O cost in case of the smaller $0.05$MP image, but for the larger images, this advantage vanishes as the un-preprocessed images causes higher I/O costs as well as higher preprocessing costs than the preprocessed counterparts.

MobileNetV2 cloud inference with cloud preprocessing starts to perform worse than InceptionV4 in case of cloud preprocessing starting from $8$MP images for large batch sizes and even worse than edge inference of InceptionV4 for $16$MP images.
Since images for MobileNetV2 have to be resized to $224\times224$ and for InceptionV4 to $299\times299$, we believe the that the increased cost of resizing the images to $224\times224$ instead of $299\times299$ outweighs the performance advantages gained by the smaller architecture of MobileNetV2.

Plots showing $Latency_{network}$ and $Latency_{server}$ divided can be seen in figures \ref{fig:BatchSizeServer} and \ref{fig:BatchSizeNetwork}.
The variance of $Latency_{network}$ across all images size and batch sizes is relatively high, but even in case of cloud preprocessing with batch size $32$ and a $16$MP images, where in total more than $300$MB are transmitted to the cloud backend (see figure \ref{fig:BatchSizeTransmittedData}), the mean $Latency_{network}$ is below $200$ms. 
Therefore for example in the case of InceptionV4, batch size $32$, $16MP$ images, cloud inference with cloud preprocessing the latency of the network is accountable for less than $1\%$ of the total inference latency, supporting the claim that our network can be seen as lower bound for cloud inference, at least for current network speeds.




\paragraph{$\mathbf{Throughput_{inference}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Inference_Throughput.pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Throughput_{inference}$ - higher is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Throughput_{inference}$ - higher is better - Only cloud inference with cloud preprocessing benefits from larger batch sizes}
\label{fig:BatchSizeInferenceThroughput}
\end{figure}

When looking at the $Throughput_{inference}$ results in figure \ref{fig:BatchSizeInferenceThroughput} it gets clear that the cloud preprocessing options benefit significantly from higher batch sizes, but only for the $0.05$MP images, where no resizing is needed.
The throughput of MobileNetV2 for batch size $32$ is $93.2$, which is $5.8$ times higher than the throughput achieved by batch size $1$. 
InceptionV4's throughput for the same batch sizes increases by a factor $3.8$ to a inference throughput of $39.8$.

While cloud inference with edge preprocessing also benefits from higher batch sizes, the throughput increase is not nearly as high as in the cloud preprocessing case.
When comparing the throughput of batch sizes $32$ to the throughput of batch size $1$, the increase is by a factor of $2$ for MobileNetV2 (to $18.9$) and $1.7$ for InceptionV4 (to $9.6$).

The throughput of the edge inference results show that the edge devices does not have the computational power to handle these large batch sizes efficiently, hence the throughput drops from $4.2$ (batch size $1$) to $1.2$ (batch size $32$).


\paragraph{$\mathbf{Data_{transmitted}}$ \& $\mathbf{Data_{received}}$}
Figures \ref{fig:BatchSizeReceivedData} and \ref{fig:BatchSizeTransmittedData} shows the data transmitted by the client to TensorFlow Serving on the cloud-backend, and the received data sent back.

As expected, the $Data_{transmitted}$ increases along with increasing batch and images sizes (in case of cloud preprocessing), especially for cloud preprocessing, as the un-preprocessed images are larger in sizes.
The increase of edge preprocessing option across the batch sizes is rather small, therefore not having much impact on network latencies.

$Data_{received}$ also increases with increasing batch sizes and images sizes (in case of cloud preprocessing), since communication between client and server is done via \emph{TCP}, therefore if more packages are sent to the server, more acknowledgements are being sent back from the server to the client.


\paragraph{$\mathbf{Throughput_{total}}$}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{./Bilder/single_plots/batch_size_plots/Effects_of_Batch_size_Total_Throughput_(Preprocessing_+_Inference).pdf}
\caption[Edge vs. Cloud Inference for larger Batch Sizes:  $Throughput_{total}$ - higher is better]{Edge vs. Cloud Inference for larger Batch Sizes:  $Throughput_{total}$ - higher is better - Compared to the $Throughput_{inference}$ results the impact of preprocessing can be seen particular on the edge preprocessing option for larger images sizes}
\label{fig:BatchSizeTotalThroughput}
\end{figure}

When combining preprocessing and inference to $Throughput_{total}$, as can be seen in figure \ref{fig:BatchSizeTotalThroughput}, the throughput for $0.05$MP images is similar to the $throughput_{inference}$ results, as the throughput reduces by $2$ at most.
However, when looking at the larger image sizes, the impact of preprocessing can be observed, especially for the edge preprocessing option, where preprocessing has a larger impact on overall latency, thus lowering throughput.

For $16$ megapixel images the highest total throughput across all inference modes is $7.4$, achieved by cloud inference with edge preprocessing of MobileNetV2 for the batch sizes $16$ and $32$.
This is only $36\%$ of the highest $Throughput_{inference}$  results achieved for the same image and batch size.



\FloatBarrier
%%throughput can be increase 

After comparing the results edge and cloud inference of batch sizes larger than one it can be stated that even though edge inference can outperform cloud inference for small model sizes for batch sizes of one, the edge inference performance of large batches significantly drops off as edge devices do currently not have enough computational power for these large batch sizes.

However, since computational power is available at the cloud-backend, large batch size increase performance of cloud inference in the form of throughput under the right circumstances, specifically if the inefficient preprocessing steps are minimized as well as I/O costs.
Although this increased throughput comes at the cost of higher latencies, hence a balance between throughput and latency needs to be found that satisfies the requirements of a real-time AI application.


\paragraph{Edge vs. Cloud - Effect of larger Batch Sizes - Key Takeaways}
\emph{
\subparagraph*{Preprocessing}
\begin{itemize}
    \item $CPU_{preprocessing}$ of edge preprocessing options increases up to $92\%$ for large batch and images sizes, as parallel preprocessing kicks in.
    \item $CPU_{preprocessing}$ of cloud preprocessing stays below $20\%$.
    \item As image and batch sizes grow, the $Memory_{preprocessing}$ gap between the two preprocessing options grows up to $842.2$MB, where edge preprocessing is $2.88\times$ faster than cloud preprocessing. 
    \item As images and batch sizes grow, the $Latency_{preprocessing}$ gap between the two preprocessing options grows up to $1375.3$ms, where edge preprocessing uses $2\times$ more memory than cloud preprocessing.
    \item There are no significant performance difference between the different models for large batch sizes.
\end{itemize}
\subparagraph*{Inference}
\begin{itemize}
    \item Edge inference with large batch sizes causes up to $92\%$ $CPU_{inference}$ usages, while cloud inference causes at maximum $18\%$.
    \item Edge inference with large batch sizes causes up to $2132.3$MB $Memory_{inference}$ consumption, $3\times$ more than the highest cloud inference result.
    \item Cloud inference with cloud preprocessing uses up to $4.2\times$ more $Memory_{inference}$ for large image and batch sizes.
    \item By increasing batch size, the cloud inference $Throughput_{inference}$ of MobileNetV2/InceptionV4 can be increased from $16.2/10.6$ to $93.24/39.8$.
    \item Edge inference $Throughput_{inference}$ decreases for larger batch sizes.
\end{itemize}
\begin{itemize}[leftmargin=4em]
 \renewcommand{\labelitemi}{$\Rightarrow$}
 \item Cloud inference throughput can be increased by increasing batch sizes, but not edge inference throughput.
 \item Cloud inference outperforms edge inference for all model types for batch sizes larger than one.
 \item Current edge devices do not have enough computational power for large batch sizes.
\end{itemize}
}



\section{Performance Model}
\label{chap:perfModel}
In this section, we use the benchmarks generated in the previous steps of the methodology to create a performance model predicting the optimal deployment selection for a given system configuration.
Therefore, we first define regression models predicting the latency of a specific deployment solution and afterwards fit these regression models using the data from the benchmarks.
Finally, we evaluate these regression models and the resulting predictions.



\subsection{Multiple Linear Regression}
To define a suitable regression model we need to recall the performance model presented in figure \ref{fig:perfmodel}.
The inference performance is mainly affected by four factors, the hardware, the architecture of the deep learning model, the inference framework and the inference-input, on which preprocessing and inference has to be performed.

These factors have an impact on a variety of performance metrics, with latency and throughput being the most important ones for real-time AI applications on edge devices.
Due to limited time and simplicity, we will focus on the prediction of the $Latency_{total}$ in this regression model.
As presented in section \ref{chap:latency}, $Latency_{total}$ is defined as follows
\begin{equation*}
\begin{gathered}
Latency_{total} = Latency_{preprocessing}  + Latency_{inference}
\end{gathered}
\end{equation*}
where in case of cloud inference
\begin{equation*}
\begin{gathered}
Latency_{inference} = Latency_{network}  + Latency_{server}
\end{gathered}
\end{equation*}

When looking at how the previous mentioned performance influencing factors have impact on the different parts of the total latency, the following relations can be defined for edge inference:
\begin{equation*}
\begin{gathered}
 Latency_{preprocessing}\sim ImageSize, BatchSize, Hardware\\
 Latency_{inference} \sim DLModel, BatchSize, Hardware, Inference Framework\\
\end{gathered}
\end{equation*}

For cloud inference these relations look as follows:
\begin{equation*}
\begin{gathered}
 Latency_{preprocessing}\sim ImageSize, BatchSize, Hardware\\
 Latency_{network}\sim ImageSize, BatchSize, Network, Inference Framework\\
 Latency_{server} \sim DLModel, BatchSize, Hardware, Inference Framework\\
\end{gathered}
\end{equation*}
Batch size has impact on all of these metrics, as it determines how many images have to be preprocessed and have to be fed to the deep learning model for inference.
Preprocessing performance is influenced by the image size of the image and the hardware components, on which the preprocessing step is performed on since for example faster processors can speed up preprocessing of images.
The Inference is also influenced by the hardware components, as available accelerators can speed up inference significantly. 
The deep learning model, specifically its architecture, determine which operations have to be performed in the inference, thus having a decisive part in the inference/server latency.
In case of cloud inference, the used network plays parts in the total latency, as the size of the payload ($ImageSize$, $BatchSize$) decides how much data has to be transmitted and the network itself decides the speed at which this payload is transmitted. The inference framework also has an impact, as low latency protocols can speed up network latencies.

Note that in case of cloud inference with cloud preprocessing, the $ImageSize$ also has an impact on the server latency, as preprocessing is done at the server.


These factors now can be used to generate a multiple linear regression model, which predicts the $Latency_{total}$.


To predict the total latency ($Latency_{total}^{pred}$), we use the following multiple linear regression formula:
\begin{equation} \label{eq:regression}
\begin{gathered}
\hat{Y}(X,\beta) = Latency_{total}^{pred}(X,\beta) = X \beta = \begin{bmatrix} 1 &x_{1} & x_{2}& ...& x_{p} \end{bmatrix} \begin{bmatrix}
           \beta_{0} \\
           \beta_{1} \\
           \beta_{2} \\
           ... \\
           \beta_{p}
         \end{bmatrix} 
\end{gathered}
\end{equation}
where $\beta_0$ is the intercept, $(\beta_1,...,\beta_p)$ are the coefficients and $X$ is the vector with all influencing factors.

We decided to use multiple linear regression with ordinary least squares for this task, since we think, based on the previously evaluated benchmarks, that there is a linear relationship between the latency and the above-mentioned factors.
Another advantage of multiple linear regression is its interpretability, as the coefficients of a fitted model tell us something about the impact of the factor in inference performance.

%quantized omit ,batch size = 1
%Since in we benchmark only a single network, hardware components for edge and cloud, we can simplify the regression
Using the generated benchmarks, not all of the above-mentioned factors apply, hence we can apply several simplifications.
First, we only benchmarked one hardware component for each edge and cloud, rendering the hardware factor irrelevant.
The same reasoning can be applied to the factor of inference framework, as we only benchmarked TensorFlow Lite as the edge framework and TensorFlow Serving as the cloud inference framework.
We executed all benchmarks with the same network connection, thus the network factor can be omitted as well.
Lastly, we omit batch size as a factor, as we only have MobileNetV2 edge inference benchmarks for a batch size of one, since TensorFlow Lite does not support larger batch sizes of this model at the moment.

Applying these simplification to the equation \ref{eq:regression}, the regression model looks as follows:

\begin{equation}\label{eq:regression2}
\begin{gathered}
\hat{Y}(X,\beta) = \beta_0 + \beta_1 ImageSize + \beta_2 MobileNetV2 + \beta_3 InceptionV4 \\
\end{gathered}
\end{equation}
where $ImageSize$ is the total resolution of the image (e.\,g. $224\times224=50176$) and $MobileNetV2/InceptionV4$ are the one hot encoded models. The quantized version of MobileNetV2 is not part of this equation, as no cloud inference benchmarks with this models have been executed.


We use the equation \ref{eq:regression2} to fit three multiple linear regression models, one for each deployment option ($Edge$, $Cloud_{EdgePrepro}$, $Cloud_{Cloud Prepro}$).
\begin{equation*}
\begin{gathered}
\hat{Y}_{\{Edge, Cloud_{EdgePrepro}, Cloud_{CloudPrepro}\}}(X,\beta) \\
\end{gathered}
\end{equation*}

Given the three $Latency_{total}$ predictions, we can predict the optimal deployment solution $OptimalChoice_{pred}$.
If the predicted edge latency is smaller or equal to the cloud options, we predict edge deployment, as edge inference does no rely on a stable network connection as well as raising fewer security concerns.
If $Cloud_{EdgePrepro}$ and $Cloud_{CloudPrepro}$ have the same prediction, we choose $Cloud_{EdgePrepro}$ as the prediction, as preprocessed payloads are usually smaller, thus rely less on the strength of the network connection. Choosing $Cloud_{CloudPrepro}$, in this case, would also be an option if the user prefers to move as much computational cost to the cloud.
\begin{equation*}
\begin{gathered}
OptimalChoice_{pred}(X) = \left\{\begin{array}{ll}
Edge  & \hat{Y}_{Edge} \leq \hat{Y}_{Cloud_{EdgePrepro}} \And \hat{Y}_{Cloud_{CloudPrepro}}\\
Cloud_{EdgePrepro}  &\hat{Y}_{Cloud_{EdgePrepro}} <   \hat{Y}_{Edge} \And \leq \hat{Y}_{Cloud_{CloudPrepro}}\\
Cloud_{CloudPrepro} & \hat{Y}_{Cloud_{CloudPrepro}} < \hat{Y}_{Edge} \And \hat{Y}_{Cloud_{EdgePrepro}}
\end{array}\right.
\end{gathered}
\end{equation*}

\subsection{Evaluation}
Based on the previously defined regression formulas, we can now use the generated benchmark data to fit those models using the ordinary least squares approach.
We split the benchmark data into a training and test set, as we fit the model using the training set and evaluate it using the test set.
The data set is split $70\%$ training and $30\%$ test.
%%sum of sqaured
%Length edge train data:106Length edge test data:44
%Length cloud edge prepro train data:107
%Length cloud edge prepro test data:44
%Length cloud cloud prepro train data:118
%Length cloud cloud prepro test data:32
%Summary of model_edge
The summaries of the three models can be seen in listings \ref{lst:edgeModelSummary}, \ref{lst:CloudEdgePreproModelSummary} and \ref{lst:CloudCloudPreproModelSummary}.
After fitting, the coefficients and intercept of the model look as follows:
\begin{equation*}
\begin{split}
\hat{Y}_{Edge} =& 124.1744      -39.9870\cdot MobileNetV2 + 164.1614\cdot InceptionV4\\
&+2.554\mathrm{e}{-05}\cdot ImageSize
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
\hat{Y}_{Cloud_{EdgePrepro}} = &
135.7102 + 26.8673\cdot MobileNetV2+ 108.8429\cdot InceptionV4\\
&+2.436\mathrm{e}{-05}\cdot ImageSize
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
\hat{Y}_{Cloud_{CloudPrepro}} = &
158.6698 + 22.3658\cdot MobileNetV2+ 136.3039\cdot InceptionV4\\
&+9.35\mathrm{e}{-05}\cdot ImageSize
\end{split}
\end{equation*}

The coefficient of the $ImageSize$ predictor for $\hat{Y}_{Cloud_{CloudPrepro}}$ is more than $3$ times higher than for the other models, confirming the high impact of cloud preprocessing on latency we observed in the evaluation of the benchmarks.
The same coefficient is relatively similar between both edge preprocessing options $\hat{Y}_{Edge}$ and $\hat{Y}_{Cloud_{EdgePrepro}}$, as both use the same preprocessing method.
The coefficient of the InceptionV4 model is significantly higher than the MobileNetV2 coefficient for all three models.
This illustrates the effect of InceptionV4 larger architecture on performance in comparison to the smaller MobileNetV2.

\paragraph{Coefficient of Determination}
A metric for the goodness of a regression is the coefficient of determination, also called $R^2$.
If the regression model predicts all observed value perfectly, this $R^2$ values is $1$, else it is below $1$.

The following equations show the $R^2$ values for all three regression models.

\begin{equation*}
\begin{split}
R^2(\hat{Y}_{Cloud_{EdgePrepro}}) = 0.964\\
R^2(\hat{Y}_{Edge}) = 0.948\\
R^2(\hat{Y}_{Cloud_{CloudPrepro}}) = 0.897
\end{split}
\end{equation*}
The best model is $\hat{Y}_{Cloud_{EdgePrepro}}$ with $0.964$, meaning that the fitted regression model explains $96.4\%$ of the variation.
The worst model is the $Cloud_{CloudPrepro}$ with a $R^2$ of $0.897$.
While this model performs worse than the other models, it still has a relatively high $R^2$, indicating that image sizes have a linear impact on the total latency.
The worse fit of the $Cloud_{CloudPrepro}$ model could be due to the fact the bigger the images sizes get, the bigger the uncertainty of the factor shared network becomes, as more data has to be transmitted from the server to the client.
\paragraph{Confidence Intervals}
%%confidence intervals here
\begin{table}[!htb]
\centering
\caption{$95\%$ Confidence Intervals for all three regression models}
\label{table:confidence}
\begin{center}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{c}{$\mathbf{\hat{Y}_{Edge}}$} & \multicolumn{2}{c}{Conf. Interval} \\ 
\cmidrule(l){2-3}
Coefficient & $[0.025$ & $0.975]$ \\\midrule
$Intercept$ & $116.227$ & $132.122$ \\
$MobileNetV2$ & $-48.846$ & $-31.128$ \\
$InceptionV$4 & $154.731$ & $173.592$ \\
$ImageSize$ & $2.41\mathrm{e}{-05}$ & $2.7\mathrm{e}{-05}$ \\\bottomrule
\end{tabular}
\end{center}
\vspace*{0.3cm}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{c}{$\mathbf{\hat{Y}_{Cloud_{EdgePrepro}}}$} & \multicolumn{2}{c}{Conf. Interval} \\ 
\cmidrule(l){2-3}
Coefficient & $[0.025$ & $0.975]$ \\\midrule
$Intercept$ & $130.284$ & $141.136$ \\
$MobileNetV2$ & $21.102$ & $32.632$ \\
$InceptionV4$ & $102.487$ & $115.199$ \\
$ImageSize$ & $2.34\mathrm{e}{-05}$ & $2.54\mathrm{e}{-05}$ \\\bottomrule
\end{tabular}
\quad
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{c}{$\mathbf{\hat{Y}_{Cloud_{CloudPrepro}}}$} & \multicolumn{2}{c}{Conf. Interval} \\ 
\cmidrule(l){2-3}
Coefficient & $[0.025$ & $0.975]$ \\\midrule
$Intercept$ & $125.807$ & $191.533$ \\
$MobileNetV2$ & $-13.700$ & $58.431$ \\
$InceptionV4$ & $97.585$ & $175.023$ \\
$ImageSize $& $8.72\mathrm{e}{-05}$ & $9.98\mathrm{e}{-05}$ \\\bottomrule
\end{tabular}

\end{table}
Table \ref{table:confidence} shows the $95\%$ confidence intervals across all regression models and their coefficients.
If a confidence interval of a coefficient includes zero, the coefficient is not significantly different from zero at the confidence level \cite{books/daglib/0076234}.

Out of the three models, there is only one model with one confidence interval that includes zero.
This model is $\hat{Y}_{Cloud_{CloudPrepro}}$ and the coefficient concerned is MobileNetV2.
Except for this coefficient of this single model, all other parameters of all models are significant at the $95\%$ confidence level.




\begin{figure}[!htb]
\centering
\includegraphics[width=0.99\textwidth]{./Bilder/regression.pdf}
\caption{Fitted regression line and scattered true measurements of the test set}
\label{fig:regression}
\end{figure}

Figure \ref{fig:regression} shows the regression lines for the three different inference modes.
For better legibility, the different deep learning models are shown on side by side plots.

This plot confirms the $R^2$ values of the models, as the lines of the $\hat{Y}_{Edge}$ and $\hat{Y}_{Cloud_{EdgePrepro}}$ model are fitted very close to the points of the test data set, while the $\hat{Y}_{Cloud_{CloudPrepro}}$ line has relatively large distances to the data points.
This distance is especially visible for $224^2/299^2$ images, where $\hat{Y}_{Cloud_{CloudPrepro}}$ is the best deployment solution, but the fitted line does not reflect this at all.


\begin{table}[!htb]
\centering
\caption{Predictions for optimal deployment choice and respective ground truth.}
\label{table:perfmofelpred}
\begin{tabular}{@{}llll@{}}
\toprule
$Model$ & $ImageSize$ & $OptimalChoice$ & $OptimalChoice_{Pred}$ \\ \midrule
InceptionV4 & $299\times299$ & $Cloud_{CloudPrepro}$ & $\mathbf{Cloud_{EdgePrepro}}$ \\
 & $2$MP & $Cloud_{EdgePrepro}$ & $Cloud_{EdgePrepro}$ \\
 & $4$MP & $Cloud_{EdgePrepro}$ & $Cloud_{EdgePrepro}$ \\
 & $8$MP & $Cloud_{EdgePrepro}$ & $Cloud_{EdgePrepro}$ \\
 & $16$MP & $Cloud_{EdgePrepro}$ & $Cloud_{EdgePrepro}$ \\
MobileNetV2 & $224\times224$ & $Edge$ & $Edge$ \\
 & $2$MP & $Edge$ & $Edge$ \\
 & $4$MP & $Edge$ & $Edge$ \\
 & $8$MP & $Edge$ & $Edge$ \\ 
 & $16$MP & $Edge$ & $Edge$ \\ \bottomrule
\end{tabular}
\end{table}


Table \ref{table:perfmofelpred} presents the actual optimal choices for the given model and image size and the choice predicted by the performance model.
The only wrong prediction is for InceptionV4 and $299\times299$ images, where the optimal choice is cloud inference with cloud preprocessing instead of cloud inference with edge inference.
This wrong prediction is not surprising since the regression model for $Cloud_{CloudPrepro}$ does not represent the results of the benchmarks well.
This still results in an overall accuracy of $90\%$.


While the performance model introduced in this section as simplified due to the limitations for the benchmarks, it still helped to understand the impact of the different factors on inference performance and can be extended to more factors easily.

At the last step of the proposed methodology, the next section utilizes the knowledge from the previous sections to present a decision model for the deployment of deep learning models.


\FloatBarrier
\section{Decision Model}\label{chap:DecisionModel}

Based on the results of the benchmark system and the gained system understanding we can now create a generic decision model, that models the decision process regarding the optimal selection for edge and cloud inference.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{./Bilder/DecisionModel.pdf}
\caption{Generic decision model for optimal deployment of deep learning models}
\label{fig:DecisionModel}
\end{figure}


This decision model, visualized in figure \ref{fig:DecisionModel}, can be applied to all types of deep learning models, not just image classification models.
Note that this model is based on the assumption that edge deployment should be preferred deployment option if possible and cloud deployment should only be considered if the required inference performance cannot be achieved at the edge.


In case  the deep learning model achieves the desired performance at the edge device out of the box, edge deployment is the optimal choice as no network connection is needed as well as fewer security issues need to be solved.

If the desired performance can not be achieved with the current deep learning model architecture, the next step is to think about model optimization techniques like quantization.
Note as these optimization techniques often lower the accuracy of the deep learning models, it is important to ensure that the accuracy does not drop lower than the accuracy threshold required by the AI application.
If the desired inference performance still cannot be achieved by model optimization, cloud deployment has to be considered in the next step.

For cloud deployment, a network connection is needed. Depending on the input data to be transmitted over this network as well as latency requirements by the AI application, sufficient bandwidth threshold and network latency need to ensured by the provider. 
If that is the case cloud deployment can be performed.

If the network is not strong or fast enough the last step is to minimize the payload transmitted over the network.
One method for optimization is to preprocess the input data at the edge, which often reduces input data size. 
Another way to reduce input data size is to make use of compression algorithms and for example compress an image to the \emph{JPG} format.
In the case of lossy compression it is essential to ensure that the compression does not infer with the accuracy of the deep learning model.

If inference performance still cannot satisfy the needs of the real-time AI application, no deployment optional is feasible, hence the deep learning model architecture, the hardware at the edge or cloud or the inference framework has to be adjusted.



%%%%%%%%%%%%%%%%%%%%%%%%%
%Überleitung


\vspace{0.5cm}
This chapter presented a instantiation of the proposed methodology in chapter \ref{chap:methodology}.
In the next chapter, we conclude this thesis with a summary of the findings produced by the instantiation of the methodology in this chapter as well as an outlook of future work topics.

\endinput 
